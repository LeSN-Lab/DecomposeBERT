{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = None\n",
    "\n",
    "device = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print (device)\n",
    "else:\n",
    "    print (\"cuda device not found.\")\n",
    "\n",
    "\n",
    "# 모델 및 토크나이저 로드\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "# model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.sentences = df.iloc[:, 1:].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).values\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        self.tokenizer = tokenizer\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        inputs = self.tokenizer(sentence, truncation=True, max_length=512, padding='max_length', return_tensors=\"pt\")\n",
    "        label = torch.tensor(self.labels[idx]) # label은 1부터 시작하기 때문에, 나중에 inference할 때에 예측값에 1을 더해줘야 합니다.\n",
    "        \n",
    "        return inputs, label\n",
    "\n",
    "test_data = \"b.csv\"\n",
    "test_df = pd.read_csv(test_data, header=None)\n",
    "\n",
    "test_dataset = TestDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_1_data = np.array([\n",
    "    [-0.07, 0.30, 0.07, -0.10, 0.03, -0.30, 0.13, 0.03, -0.27, -0.53, -0.20, -0.13],\n",
    "    [-0.20, -0.10, 0.00, 0.00, 0.03, 0.00, -0.20, -0.37, 0.03, 0.27, -0.47, -0.10],\n",
    "    [0.03, 0.17, 0.07, -0.23, 0.00, 0.10, -0.40, -0.13, -0.17, 0.10, -0.47, -0.03],\n",
    "    [-1.03, 0.13, 0.13, -0.23, -0.07, 0.07, 0.03, 0.03, -0.20, 0.13, -0.23, -0.27],\n",
    "    [-1.03, -0.33, -0.20, -0.40, 0.17, -0.10, 0.10, 0.10, -0.13, 0.03, -0.13, -0.13],\n",
    "    [-0.03, -0.13, 0.07, -0.03, 0.33, 0.00, 0.10, 0.03, -0.27, -0.23, -0.23, -0.17],\n",
    "    [-0.27, 0.03, -0.10, 0.00, -0.13, -0.33, -0.10, 0.40, 1.03, -0.10, 0.13, 0.23],\n",
    "    [-0.57, 0.07, 0.10, 0.03, 0.17, 0.70, 0.07, -0.07, -0.57, 0.07, -0.03, 0.10],\n",
    "    [-0.13, -0.37, -0.03, -0.17, -0.30, 0.03, -0.33, 0.20, 0.13, -0.43, -0.40, -0.50],\n",
    "    [-0.17, -0.17, 0.13, -0.27, -0.40, 1.77, -0.07, 0.03, 0.40, 0.13, 0.17, -0.30],\n",
    "    [0.60, -0.30, -0.03, 0.07, -0.03, 0.00, 0.53, -0.03, 0.43, -0.03, 0.13, 0.80],\n",
    "    [0.50, 0.17, -0.47, -0.10, -0.77, 0.10, 0.60, -0.73, -0.50, -0.03, -0.10, 1.10]\n",
    "])\n",
    "class_2_data = np.array([\n",
    "    [-0.10, -0.07, -0.27, -0.27, 0.27, -0.07, -0.17, -0.33, -0.50, -0.03, -0.17, 0.07],\n",
    "    [0.07, 0.03, -0.17, 0.10, -0.17, -0.23, -0.17, -0.23, -0.07, -0.33, -0.10, -0.27],\n",
    "    [-0.17, -0.13, 0.03, 0.10, 0.03, 0.00, -0.13, -0.20, 0.03, -0.07, -0.10, 0.00],\n",
    "    [0.20, 0.10, 0.07, -0.07, -0.20, -0.43, 0.17, -0.10, -0.37, -0.13, -0.27, -0.07],\n",
    "    [-0.17, -0.07, 0.10, -0.27, -0.20, 0.00, -0.03, -0.20, 0.00, 0.23, 0.00, 0.10],\n",
    "    [-0.10, 0.03, 0.07, 0.03, 0.10, -0.10, 0.17, -0.13, -0.23, 0.17, 0.17, -0.03],\n",
    "    [-0.17, -0.03, 0.13, 0.00, -0.03, -0.13, -0.03, 0.00, -0.47, -0.23, -0.03, 0.30],\n",
    "    [0.17, 0.03, 0.13, -0.07, -0.23, -0.13, 0.00, 0.00, 0.00, -0.13, 0.03, 0.07],\n",
    "    [-0.03, -0.13, -0.03, 0.03, -0.03, -0.10, -0.13, -0.27, -0.07, 0.00, 0.07, -0.03],\n",
    "    [-0.03, -0.70, 0.27, 0.23, 0.10, -1.30, -0.13, -0.10, -0.47, -0.17, 0.03, -0.23],\n",
    "    [0.00, -0.23, 0.13, 0.17, -0.07, 0.03, -0.57, -0.43, -0.43, 0.00, -0.03, -0.93],\n",
    "    [-1.53, 0.07, 0.17, 0.03, 0.03, -0.20, -0.60, -0.07, 0.37, 0.37, -0.23, -0.37]\n",
    "])\n",
    "class_3_data = np.array([\n",
    "    [0.23, 0.33, 0.03, -0.10, 0.07, -0.67, 0.17, 0.30, 0.27, -0.43, 0.00, 0.00],\n",
    "    [0.17, -0.03, 0.20, 0.07, 0.00, 0.00, 0.07, 0.07, 0.20, 0.10, -0.07, 0.07],\n",
    "    [-0.10, -0.10, 0.20, 0.03, -0.03, -0.07, 0.13, -0.03, -0.03, -0.17, 0.00, 0.10],\n",
    "    [-0.20, 0.00, -0.13, 0.00, 0.20, 0.10, -0.07, -0.13, -0.03, 0.07, -0.07, -0.20],\n",
    "    [-0.03, 0.10, -0.20, 0.20, 0.10, -0.27, -0.13, -0.07, -0.07, -0.07, -0.03, 0.03],\n",
    "    [-0.03, -0.13, -0.07, -0.10, 0.10, 0.03, -0.07, 0.00, 0.13, 0.00, -0.17, 0.00],\n",
    "    [0.03, -0.17, -0.07, -0.03, -0.03, -0.07, -0.07, -0.10, 0.00, -0.07, 0.03, 0.03],\n",
    "    [-0.23, -0.13, 0.00, 0.03, 0.07, -0.17, -0.07, 0.00, -0.03, -0.10, -0.20, 0.03],\n",
    "    [-0.03, -0.10, 0.13, -0.07, 0.03, -0.13, -0.10, 0.07, -0.10, 0.00, -0.33, -0.07],\n",
    "    [0.30, -0.03, -0.10, 0.13, 0.07, -0.20, -0.07, -0.40, -0.50, -0.03, -0.13, 0.13],\n",
    "    [0.20, 0.10, -0.20, 0.10, -0.23, 0.07, -1.07, 0.33, 0.27, 0.07, -0.03, -0.20],\n",
    "    [-0.67, 0.07, 0.00, 0.23, -0.23, 0.23, 0.33, -0.07, -0.17, 0.27, 0.07, -0.83]\n",
    "])\n",
    "class_4_data = np.array([\n",
    "    [0.33, 0.20, -0.07, -0.13, -0.10, 0.10, -0.03, 0.23, -0.17, -0.17, -0.30, 0.00],\n",
    "    [-0.07, -0.10, 0.33, 0.07, -0.10, -0.13, 0.03, 0.13, 0.03, 0.03, -0.27, 0.23],\n",
    "    [0.07, -0.43, 0.13, -0.07, 0.10, 0.17, 0.03, -0.13, -0.17, -0.10, 0.07, 0.23],\n",
    "    [0.50, -0.03, -0.03, 0.20, 0.00, 0.10, -0.17, -0.03, -0.17, -0.03, -0.20, 0.07],\n",
    "    [-0.17, -0.17, 0.23, -0.07, -0.13, 0.07, -0.23, -0.03, 0.10, 0.13, -0.17, -0.13],\n",
    "    [-0.07, -0.07, -0.17, 0.00, -0.17, -0.13, -0.07, 0.03, -0.17, 0.10, -0.03, 0.10],\n",
    "    [0.13, -0.03, -0.17, 0.00, -0.07, -0.07, 0.00, -0.27, 0.30, 0.10, -0.03, 0.07],\n",
    "    [0.10, -0.10, -0.10, -0.10, 0.03, -0.50, -0.03, 0.00, 0.10, -0.20, -0.23, 0.03],\n",
    "    [0.07, 0.33, -0.17, 0.10, -0.03, 0.13, 0.03, -0.07, -0.43, -0.13, 0.10, -0.10],\n",
    "    [0.20, 0.33, -0.17, 0.10, -0.23, -0.87, 0.13, -0.60, -0.07, 0.10, 0.50, -0.10],\n",
    "    [-0.70, 0.20, -0.03, 0.67, 0.27, 0.03, 1.47, 0.97, -0.07, -0.03, -0.07, 0.27],\n",
    "    [1.80, 0.30, -0.13, -0.13, 0.57, -0.20, 0.47, 0.03, 0.37, -0.40, 0.03, 0.93]\n",
    "])\n",
    "class_5_data = np.array([\n",
    "    [-0.20, -0.37, -0.10, 0.20, -0.07, 0.33, 0.17, -0.10, 0.03, 0.20, -0.03, 0.03],\n",
    "    [0.03, 0.03, 0.30, -0.10, -0.23, -0.30, 0.00, -0.27, -0.17, -0.10, 0.17, -0.13],\n",
    "    [-0.33, 0.03, 0.03, -0.13, -0.10, -0.33, -0.10, -0.17, -0.17, -0.27, 0.07, 0.07],\n",
    "    [-0.43, -0.10, -0.10, -0.17, -0.13, 0.03, 0.00, -0.10, 0.10, -0.03, 0.13, -0.23],\n",
    "    [-0.33, 0.13, 0.13, -0.07, 0.10, 0.07, -0.07, -0.07, -0.07, 0.20, -0.03, 0.10],\n",
    "    [0.10, 0.00, -0.17, -0.13, 0.03, -0.10, -0.03, 0.00, 0.07, 0.03, -0.10, -0.03],\n",
    "    [0.07, 0.07, -0.17, 0.00, -0.13, 0.17, 0.13, -0.07, 0.20, 0.03, -0.10, 0.00],\n",
    "    [-0.03, -0.23, 0.13, -0.03, -0.13, -0.37, -0.10, 0.13, -0.07, -0.10, 0.20, -0.20],\n",
    "    [-0.03, 0.07, 0.00, 0.03, 0.03, -0.03, -0.20, 0.00, 0.10, 0.03, 0.23, 0.27],\n",
    "    [0.07, 0.57, 0.00, -0.20, -0.13, -1.30, -0.13, 0.20, 0.30, 0.00, -0.53, -0.33],\n",
    "    [0.63, 0.13, -0.30, -0.63, -0.53, 0.03, -0.67, -0.90, 0.20, 0.17, 0.00, -0.07],\n",
    "    [-0.57, -0.23, -0.20, 0.30, 0.03, 0.03, -0.73, -0.70, -0.37, 0.60, -0.53, 1.37]\n",
    "])\n",
    "class_6_data = np.array([\n",
    "    [0.10, -0.17, 0.03, 0.00, 0.10, 0.27, -0.10, -0.07, 0.27, 0.23, 0.07, -0.03],\n",
    "    [0.00, 0.00, 0.03, -0.03, 0.00, -0.03, -0.03, -0.07, 0.13, 0.03, -0.10, -0.23],\n",
    "    [0.10, 0.07, 0.07, 0.03, 0.07, -0.03, -0.10, 0.07, 0.00, 0.03, 0.00, -0.17],\n",
    "    [-0.07, 0.00, -0.03, -0.07, 0.03, 0.07, 0.00, -0.10, -0.13, -0.07, -0.07, 0.00],\n",
    "    [0.03, 0.03, 0.13, 0.03, 0.07, 0.03, -0.07, -0.07, 0.00, 0.00, -0.10, 0.03],\n",
    "    [-0.03, 0.00, 0.00, 0.00, 0.00, -0.07, -0.03, -0.07, -0.07, -0.10, 0.03, -0.03],\n",
    "    [0.00, -0.10, 0.00, 0.00, 0.00, -0.03, -0.03, -0.03, -0.17, 0.00, 0.00, 0.03],\n",
    "    [0.07, -0.07, -0.07, -0.03, -0.10, -0.20, -0.03, 0.03, -0.03, 0.03, 0.03, 0.00],\n",
    "    [-0.07, 0.03, -0.03, -0.03, -0.07, -0.03, -0.03, -0.03, -0.03, -0.03, -0.03, 0.07],\n",
    "    [-0.10, -0.27, 0.03, 0.03, 0.00, -0.60, -0.07, 0.13, -0.10, 0.00, -0.03, 0.13],\n",
    "    [0.00, -0.07, 0.13, 0.03, 0.13, -0.03, -1.00, -0.13, -0.10, 0.00, 0.00, 0.03],\n",
    "    [-0.10, -0.13, 0.03, -0.07, -0.03, -0.03, 0.13, 0.03, 0.07, 0.03, 0.07, -1.00]\n",
    "])\n",
    "class_7_data = np.array([\n",
    "    [0.20, -0.23, -0.17, -0.27, 0.00, -0.33, 0.07, -0.30, -0.30, -0.17, 0.13, -0.50],\n",
    "    [-0.13, -0.13, -0.30, 0.17, 0.17, -0.03, -0.07, 0.13, 0.00, -0.10, -0.13, 0.23],\n",
    "    [-0.30, 0.03, -0.13, 0.13, -0.07, -0.13, -0.10, 0.23, -0.07, -0.20, -0.30, 0.07],\n",
    "    [-0.13, 0.30, -0.20, 0.03, 0.23, -0.27, -0.17, 0.07, -0.53, -0.07, -0.17, -0.03],\n",
    "    [-0.43, -0.07, -0.03, -0.13, -0.20, -0.03, -0.03, -0.10, 0.07, 0.27, -0.23, 0.07],\n",
    "    [0.03, 0.03, -0.10, -0.10, -0.17, -0.10, -0.03, -0.30, 0.10, 0.00, -0.23, -0.20],\n",
    "    [-0.07, -0.23, -0.10, -0.10, 0.07, -0.27, -0.10, 0.03, -0.30, 0.10, 0.13, 0.07],\n",
    "    [-0.47, -0.07, -0.07, -0.13, -0.13, -0.13, -0.13, -0.20, 0.13, -0.07, 0.03, -0.13],\n",
    "    [-0.07, -0.03, -0.03, -0.03, -0.07, 0.00, -0.07, 0.23, -0.17, -0.57, -0.20, -0.67],\n",
    "    [0.00, 0.07, -0.20, -0.03, 0.03, 0.60, 0.17, -0.17, -0.77, -0.03, -0.07, 0.53],\n",
    "    [0.03, -0.37, 0.03, -0.30, 0.60, -0.30, 0.53, 0.77, -0.03, -0.20, 0.03, -0.10],\n",
    "    [0.17, 0.13, 0.13, 0.50, -0.27, -0.27, 0.97, 0.90, 0.03, -0.13, 0.40, -2.33]\n",
    "])\n",
    "class_8_data = np.array([\n",
    "    [0.00, -0.20, -0.23, -0.03, -0.17, 0.27, -0.37, -0.03, -0.17, 0.03, -0.40, -0.03],\n",
    "    [-0.53, -0.07, 0.03, -0.07, -0.23, -0.23, -0.20, 0.03, -0.10, 0.10, -0.10, 0.33],\n",
    "    [-0.17, -0.07, -0.10, -0.07, -0.03, -0.07, 0.33, 0.13, 0.20, -0.07, 0.17, -0.13],\n",
    "    [0.33, -0.10, 0.03, 0.03, -0.17, -0.33, 0.03, 0.00, 0.30, -0.23, -0.17, -0.17],\n",
    "    [0.27, 0.00, 0.43, -0.30, -0.33, 0.07, -0.13, -0.23, 0.00, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.23, -0.20, 0.13, -0.20, -0.13, 0.03, -0.10, 0.07, 0.00, -0.23, 0.03],\n",
    "    [-0.20, -0.03, 0.20, 0.00, 0.03, 0.07, -0.13, -0.13, 0.13, -0.17, 0.00, 0.03],\n",
    "    [-0.10, -0.20, -0.17, 0.13, -0.13, 0.20, -0.07, -0.13, 0.17, 0.03, 0.20, -0.07],\n",
    "    [-0.03, 0.10, 0.10, 0.00, -0.03, -0.03, 0.13, 0.07, -0.17, 0.10, 0.23, 0.10],\n",
    "    [0.03, 0.13, 0.00, 0.03, 0.20, 0.60, -0.10, 0.37, 0.57, 0.13, -0.40, 0.03],\n",
    "    [-0.23, 0.27, -0.30, -0.27, 0.93, 0.10, -0.60, 0.07, 0.00, 0.03, -0.13, 0.00],\n",
    "    [-0.50, -0.07, -0.13, 0.00, 0.40, 0.20, -0.60, 0.27, -0.13, -0.30, 0.00, 1.37]\n",
    "])\n",
    "class_9_data = np.array([\n",
    "    [-0.40, -0.27, -0.13, -0.07, 0.00, -0.73, -0.37, -0.27, 0.20, -0.53, -0.10, -0.20],\n",
    "    [0.07, 0.00, 0.07, -0.30, -0.10, -0.13, -0.03, 0.17, -0.30, -0.03, -0.27, -0.07],\n",
    "    [-0.13, 0.13, -0.10, 0.00, -0.03, -0.23, 0.00, -0.33, -0.10, -0.33, -0.07, -0.07],\n",
    "    [0.00, -0.17, -0.10, 0.03, -0.03, -0.07, 0.07, -0.20, 0.13, -0.13, -0.20, -0.07],\n",
    "    [0.37, 0.07, -0.13, 0.10, -0.20, -0.17, 0.07, 0.00, -0.10, -0.20, -0.03, -0.10],\n",
    "    [-0.13, -0.07, -0.13, -0.03, -0.17, -0.03, -0.17, 0.00, -0.17, -0.07, -0.13, 0.00],\n",
    "    [0.20, -0.27, -0.27, -0.07, -0.07, -0.17, -0.10, -0.27, -0.43, 0.07, -0.10, -0.07],\n",
    "    [0.13, -0.07, -0.17, -0.07, -0.20, -0.27, 0.00, 0.03, -0.03, -0.20, -0.13, -0.27],\n",
    "    [-0.07, 0.07, -0.13, -0.20, 0.03, -0.17, -0.20, -0.27, -0.27, 0.23, 0.37, -0.07],\n",
    "    [-0.10, -0.03, -0.27, -0.13, -0.07, -0.77, 0.03, 0.03, 0.00, -0.27, -0.13, -0.20],\n",
    "    [-0.10, -0.30, -0.23, -0.97, -0.93, 0.00, 0.50, -0.33, -0.03, -0.07, -0.10, 0.00],\n",
    "    [0.40, -0.20, 0.33, -0.43, -0.13, -0.83, -0.93, 0.40, -0.20, 0.07, 0.00, -1.13]\n",
    "])\n",
    "class_10_data = np.array([\n",
    "    [-0.07, 0.07, 0.07, 0.13, 0.00, 0.27, -0.07, 0.10, 0.23, 0.63, -0.07, 0.00],\n",
    "    [-0.07, -0.07, -0.37, -0.03, -0.13, -0.03, -0.03, -0.13, 0.00, -0.17, -0.07, 0.00],\n",
    "    [-0.13, 0.20, -0.10, -0.10, 0.10, 0.20, -0.10, 0.07, 0.00, -0.03, 0.20, -0.20],\n",
    "    [0.03, 0.07, -0.07, 0.10, 0.00, 0.07, -0.10, 0.03, 0.17, 0.00, -0.03, 0.17],\n",
    "    [-0.10, -0.07, -0.20, 0.13, 0.03, -0.10, 0.03, -0.03, -0.13, -0.13, 0.00, 0.00],\n",
    "    [0.10, -0.03, -0.07, -0.03, -0.07, 0.00, 0.00, 0.03, -0.03, 0.07, 0.13, -0.13],\n",
    "    [0.33, -0.10, 0.07, 0.07, -0.10, 0.17, 0.07, -0.03, -0.07, -0.03, 0.03, 0.07],\n",
    "    [0.63, 0.07, 0.00, -0.07, 0.20, -0.27, -0.07, -0.03, 0.07, 0.17, -0.10, -0.03],\n",
    "    [-0.07, 0.13, 0.10, 0.00, -0.03, 0.00, 0.03, -0.03, 0.40, 0.33, 0.33, 0.20],\n",
    "    [-0.20, -0.33, 0.30, 0.07, 0.23, -0.60, 0.00, 0.13, 0.37, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.03, 0.37, 0.20, -0.37, -0.07, 0.90, -1.03, -0.03, 0.00, 0.07, 0.03],\n",
    "    [-0.43, -0.17, -0.03, -0.07, -0.17, 0.23, -0.13, -0.37, 0.30, -0.07, 0.33, 0.33]\n",
    "])\n",
    "\n",
    "class_1_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, -0.09, -0.03, -0.06, -0.04, -0.02, -0.09, -0.06, 0.00, 0.00, -0.03, -0.03],\n",
    "    [-0.06, -0.05, -0.03, -0.06, -0.05, -0.09, -0.03, -0.04, -0.03, -0.02, -0.01, 0.05],\n",
    "    [-0.08, -0.06, -0.01, -0.02, 0.00, -0.06, 0.04, -0.04, -0.01, -0.06, 0.03, -0.06],\n",
    "    [0.13, -0.03, -0.07, -0.01, 0.02, -0.03, 0.01, -0.02, -0.02, -0.02, -0.03, -0.03],\n",
    "    [0.07, 0.06, 0.07, -0.04, -0.11, -0.03, -0.04, -0.02, -0.02, 0.00, 0.00, 0.00],\n",
    "    [-0.01, -0.02, -0.05, -0.02, -0.05, -0.08, -0.07, 0.01, 0.00, 0.01, -0.04, 0.01],\n",
    "    [-0.01, -0.05, -0.06, 0.00, 0.00, 0.01, -0.03, -0.12, -0.15, -0.03, -0.03, 0.04],\n",
    "    [0.06, -0.04, -0.05, -0.03, -0.07, -0.22, -0.01, -0.01, 0.13, -0.04, 0.00, -0.02],\n",
    "    [-0.02, 0.09, -0.05, -0.02, 0.03, -0.02, 0.05, -0.05, -0.12, 0.01, 0.10, 0.01],\n",
    "    [0.04, -0.06, -0.01, 0.05, 0.01, -0.64, -0.05, -0.03, 0.00, -0.05, -0.09, 0.00],\n",
    "    [-0.09, 0.05, -0.06, -0.06, 0.03, -0.01, -0.22, -0.02, -0.07, -0.03, -0.03, -0.22],\n",
    "    [-0.20, 0.02, 0.05, 0.02, 0.09, -0.02, -0.25, 0.05, 0.05, -0.03, 0.05, -0.30]\n",
    "]   \n",
    ")\n",
    "class_2_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, -0.02, -0.01, 0.05, -0.03, 0.02, -0.01, 0.03, 0.10, -0.04, 0.01, -0.04],\n",
    "    [-0.01, 0.02, 0.01, -0.02, 0.01, 0.00, 0.00, 0.08, 0.00, 0.04, -0.01, 0.05],\n",
    "    [0.00, -0.01, 0.01, 0.01, 0.00, -0.02, 0.01, 0.03, 0.00, -0.01, 0.02, 0.02],\n",
    "    [-0.04, -0.01, -0.02, 0.01, -0.01, 0.03, -0.02, 0.00, 0.04, -0.01, -0.01, 0.00],\n",
    "    [0.03, 0.00, -0.01, 0.03, 0.00, 0.03, 0.04, 0.02, 0.00, 0.00, -0.02, 0.01],\n",
    "    [0.00, -0.01, -0.02, -0.02, -0.02, -0.01, 0.00, 0.01, -0.02, -0.02, -0.01, 0.00],\n",
    "    [0.03, -0.01, 0.00, 0.00, -0.01, 0.01, 0.01, 0.00, 0.05, 0.00, 0.01, -0.01],\n",
    "    [-0.01, -0.01, -0.02, 0.02, -0.01, 0.00, 0.02, 0.00, -0.01, -0.01, 0.02, -0.01],\n",
    "    [0.01, 0.01, 0.02, 0.00, 0.02, 0.01, 0.01, 0.02, -0.02, 0.01, -0.01, 0.01],\n",
    "    [0.00, 0.04, -0.01, 0.01, -0.02, 0.12, 0.03, 0.00, 0.08, 0.03, 0.00, 0.04],\n",
    "    [0.01, 0.04, -0.02, -0.04, -0.01, 0.02, 0.06, 0.06, 0.04, 0.00, 0.00, 0.13],\n",
    "    [0.21, 0.03, -0.05, -0.01, 0.02, 0.02, 0.05, 0.02, -0.05, -0.04, 0.02, 0.05]\n",
    "]\n",
    ")\n",
    "class_3_neg_acc = np.array(\n",
    "[\n",
    "    [-0.05, -0.04, -0.02, 0.00, 0.04, 0.12, -0.01, -0.04, -0.02, 0.08, -0.02, -0.02],\n",
    "    [-0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02, 0.00, -0.01, -0.01, 0.02],\n",
    "    [0.02, 0.03, -0.02, 0.02, 0.01, 0.02, -0.01, 0.02, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.02, 0.03, 0.05, 0.02, -0.05, -0.04, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02],\n",
    "    [0.03, -0.02, 0.04, -0.04, -0.01, 0.02, 0.03, 0.03, 0.00, 0.00, 0.02, 0.03],\n",
    "    [0.00, 0.00, 0.00, 0.04, -0.03, 0.02, 0.02, -0.01, 0.01, 0.06, 0.02, -0.02],\n",
    "    [0.04, 0.06, 0.04, 0.01, 0.01, 0.07, -0.03, 0.02, 0.02, 0.05, 0.04, 0.05],\n",
    "    [0.02, 0.02, 0.02, 0.00, -0.01, 0.07, 0.00, -0.01, 0.02, -0.02, 0.06, -0.01],\n",
    "    [0.00, 0.03, 0.02, 0.02, 0.02, 0.01, -0.02, -0.01, 0.01, 0.06, 0.12, 0.00],\n",
    "    [-0.05, 0.03, 0.03, -0.05, 0.00, 0.11, 0.03, 0.05, 0.10, 0.04, 0.01, -0.03],\n",
    "    [-0.04, -0.01, 0.07, -0.03, 0.07, 0.00, 0.28, -0.10, 0.00, 0.00, 0.01, 0.06],\n",
    "    [0.19, -0.01, -0.01, -0.04, 0.07, -0.07, -0.10, 0.04, 0.05, -0.04, 0.00, 0.30]\n",
    "]\n",
    ")\n",
    "class_4_neg_acc = np.array(\n",
    "[\n",
    "    [-0.04, -0.03, 0.01, 0.02, 0.03, -0.09, -0.05, 0.00, 0.02, -0.03, -0.01, 0.02],\n",
    "    [0.03, 0.02, 0.02, -0.01, -0.01, 0.00, 0.00, -0.04, -0.02, 0.01, -0.01, -0.06],\n",
    "    [-0.04, 0.00, 0.03, 0.00, 0.00, -0.01, -0.01, 0.01, 0.00, -0.04, -0.04, 0.01],\n",
    "    [-0.09, 0.02, -0.03, -0.03, -0.02, -0.01, 0.02, 0.00, 0.01, -0.03, -0.04, -0.02],\n",
    "    [-0.09, -0.01, -0.01, -0.02, -0.01, -0.03, 0.02, -0.05, -0.02, -0.02, -0.02, -0.03],\n",
    "    [0.01, -0.01, 0.00, -0.01, 0.02, 0.00, 0.02, -0.01, 0.02, 0.01, -0.04, -0.02],\n",
    "    [-0.03, -0.04, -0.02, -0.01, 0.00, -0.03, 0.00, 0.05, -0.01, -0.04, 0.00, 0.01],\n",
    "    [0.00, 0.02, 0.02, 0.00, 0.00, 0.05, -0.02, -0.01, -0.03, 0.00, 0.00, -0.03],\n",
    "    [-0.01, -0.03, 0.03, -0.04, -0.03, -0.01, -0.03, 0.03, 0.08, 0.02, 0.00, 0.03],\n",
    "    [-0.03, -0.06, 0.03, -0.03, 0.04, 0.14, -0.02, 0.06, -0.05, -0.04, -0.06, 0.05],\n",
    "    [0.18, -0.08, 0.02, -0.16, -0.06, 0.00, -0.21, -0.20, 0.03, 0.01, 0.01, -0.02],\n",
    "    [-0.31, -0.06, 0.03, 0.05, -0.09, -0.02, -0.16, -0.01, -0.07, 0.08, 0.02, -0.19]\n",
    "]  \n",
    ")\n",
    "class_5_neg_acc = np.array(\n",
    "[\n",
    "    [0.03, 0.04, 0.03, -0.02, -0.01, -0.04, 0.00, 0.00, -0.03, 0.01, -0.01, -0.01],\n",
    "    [-0.01, 0.00, -0.02, 0.03, 0.00, 0.00, 0.00, 0.02, 0.01, 0.01, -0.02, 0.02],\n",
    "    [0.02, 0.01, -0.01, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, 0.00, 0.00],\n",
    "    [0.02, 0.01, 0.01, 0.02, -0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02],\n",
    "    [0.03, -0.01, 0.01, 0.00, 0.02, 0.02, -0.01, 0.01, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.00, 0.01, 0.00, 0.01, -0.01, -0.01, 0.02, 0.02, -0.01, 0.00, 0.01, 0.00],\n",
    "    [0.00, -0.01, 0.01, 0.00, 0.00, -0.02, 0.00, 0.01, 0.01, -0.01, 0.02, 0.01],\n",
    "    [-0.01, 0.00, 0.00, 0.00, 0.02, 0.03, 0.01, 0.01, -0.03, 0.00, -0.02, 0.02],\n",
    "    [0.00, 0.00, -0.01, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, -0.01, 0.00, -0.05],\n",
    "    [0.03, -0.06, -0.01, -0.01, 0.02, 0.15, 0.00, -0.01, -0.04, 0.00, 0.05, 0.03],\n",
    "    [-0.07, 0.01, 0.01, 0.05, 0.03, 0.00, 0.13, 0.09, 0.00, 0.00, -0.01, 0.01],\n",
    "    [0.03, 0.01, 0.01, -0.03, 0.00, 0.00, 0.07, 0.07, 0.02, -0.03, 0.04, -0.13]\n",
    "] \n",
    ")\n",
    "class_6_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, 0.02, -0.02, 0.00, 0.01, -0.04, 0.02, 0.02, -0.02, -0.02, -0.02, 0.01],\n",
    "    [0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, 0.00, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.01, 0.00, 0.00, 0.01, 0.00, -0.02, 0.00, 0.00, 0.01, 0.01],\n",
    "    [0.00, 0.01, 0.01, 0.00, 0.01, -0.01, 0.01, 0.01, 0.02, -0.01, 0.02, -0.01],\n",
    "    [-0.03, 0.00, -0.02, 0.03, 0.01, 0.01, 0.01, 0.00, 0.02, 0.01, 0.00, 0.02],\n",
    "    [0.01, 0.01, 0.00, 0.01, 0.00, 0.01, 0.01, 0.02, -0.02, 0.01, 0.00, 0.01],\n",
    "    [0.02, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02, 0.00, 0.06, 0.01, 0.01, 0.01],\n",
    "    [0.01, 0.01, 0.00, 0.02, 0.02, 0.02, 0.01, 0.00, -0.01, 0.01, -0.01, 0.01],\n",
    "    [0.01, 0.00, 0.01, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.03, -0.01],\n",
    "    [0.02, 0.03, 0.00, 0.01, -0.01, 0.08, 0.01, 0.00, 0.06, 0.01, 0.01, -0.02],\n",
    "    [0.02, 0.00, 0.00, 0.01, -0.04, 0.01, 0.16, 0.01, 0.03, 0.01, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.03, 0.00, -0.02, -0.01, 0.00, 0.02, 0.00, 0.02, 0.00, 0.14]\n",
    "]  \n",
    ")\n",
    "class_7_neg_acc = np.array(\n",
    "[\n",
    "    [-0.02, 0.03, 0.01, 0.01, 0.00, 0.04, 0.00, 0.04, 0.02, 0.06, -0.02, 0.01],\n",
    "    [-0.01, -0.03, 0.03, 0.01, -0.02, -0.01, -0.01, 0.01, 0.00, 0.00, -0.02, -0.03],\n",
    "    [0.01, 0.03, 0.02, -0.01, -0.01, -0.02, 0.01, 0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "    [0.03, -0.02, 0.01, -0.02, -0.01, -0.02, 0.00, 0.00, 0.01, 0.01, 0.02, -0.02],\n",
    "    [0.03, 0.00, 0.01, 0.00, -0.01, -0.01, -0.01, -0.01, -0.01, 0.00, 0.00, -0.03],\n",
    "    [-0.04, -0.02, 0.00, 0.00, 0.00, -0.03, 0.00, -0.02, -0.02, -0.02, -0.01, -0.01],\n",
    "    [0.02, 0.01, 0.00, 0.00, -0.03, 0.04, 0.01, -0.03, 0.00, 0.01, -0.01, 0.00],\n",
    "    [0.05, -0.02, 0.00, 0.00, 0.03, 0.00, -0.01, 0.01, -0.04, -0.01, -0.01, 0.00],\n",
    "    [-0.01, -0.02, -0.02, 0.00, -0.01, -0.01, -0.01, -0.07, 0.00, 0.03, 0.02, 0.04],\n",
    "    [-0.01, 0.00, 0.02, 0.00, -0.01, -0.11, -0.03, 0.07, 0.10, -0.01, -0.02, -0.09],\n",
    "    [0.00, 0.04, -0.02, -0.03, -0.05, 0.00, -0.03, -0.14, 0.02, 0.03, -0.01, -0.02],\n",
    "    [-0.08, -0.03, -0.01, -0.06, 0.01, 0.03, -0.15, -0.11, 0.00, 0.00, -0.06, 0.17]\n",
    "]\n",
    ")\n",
    "class_8_neg_acc = np.array(\n",
    "[\n",
    "    [0.02, -0.02, -0.02, -0.05, 0.01, -0.10, -0.02, -0.03, -0.01, -0.10, -0.01, 0.00],\n",
    "    [0.02, 0.00, -0.04, -0.02, 0.00, -0.02, 0.00, -0.08, 0.00, -0.06, -0.01, -0.04],\n",
    "    [-0.04, 0.00, 0.00, 0.01, 0.00, 0.03, -0.09, -0.06, -0.06, -0.06, -0.13, 0.00],\n",
    "    [-0.13, -0.03, -0.02, -0.02, 0.02, 0.02, -0.05, -0.05, -0.09, -0.02, -0.09, 0.02],\n",
    "    [-0.17, 0.00, -0.10, 0.04, 0.04, -0.03, -0.01, -0.03, 0.00, -0.01, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.02, 0.00, 0.01, 0.01, 0.00, -0.04, -0.03, -0.03, 0.03, -0.03],\n",
    "    [-0.02, -0.05, -0.03, -0.02, -0.05, -0.10, -0.01, -0.03, -0.03, -0.01, -0.03, -0.02],\n",
    "    [-0.01, -0.03, -0.02, -0.03, -0.02, -0.20, -0.04, 0.01, -0.04, 0.01, -0.05, -0.02],\n",
    "    [-0.01, -0.06, -0.03, 0.00, -0.05, -0.02, -0.07, 0.01, -0.02, -0.06, -0.05, -0.06],\n",
    "    [-0.02, -0.09, -0.03, -0.03, -0.05, -0.35, 0.03, -0.14, -0.15, -0.08, 0.04, -0.07],\n",
    "    [0.03, -0.12, 0.00, 0.04, -0.36, -0.02, 0.10, -0.05, -0.03, -0.02, 0.00, 0.00],\n",
    "    [0.03, -0.02, 0.04, 0.02, -0.15, -0.07, 0.14, -0.08, 0.01, 0.12, 0.01, -0.33]\n",
    "]\n",
    ")\n",
    "class_9_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, 0.03, 0.00, -0.04, 0.01, 0.10, 0.10, 0.02, -0.08, 0.06, -0.02, -0.01],\n",
    "    [-0.03, -0.02, 0.00, 0.04, -0.04, 0.00, -0.02, -0.02, 0.03, 0.01, -0.02, -0.02],\n",
    "    [0.00, -0.01, -0.02, -0.03, 0.02, 0.01, -0.02, 0.01, 0.00, 0.02, -0.01, -0.02],\n",
    "    [-0.03, 0.02, 0.00, 0.03, 0.00, 0.01, -0.01, 0.00, -0.03, 0.00, 0.01, -0.04],\n",
    "    [-0.06, -0.04, 0.01, -0.08, 0.02, -0.03, -0.06, -0.03, -0.01, 0.02, -0.05, -0.01],\n",
    "    [-0.02, -0.03, 0.01, -0.03, 0.02, -0.02, 0.00, -0.01, 0.02, -0.05, -0.03, 0.00],\n",
    "    [-0.04, 0.00, 0.00, -0.01, 0.00, 0.01, 0.01, 0.04, 0.05, -0.01, 0.00, 0.01],\n",
    "    [-0.07, -0.01, 0.01, -0.01, -0.02, 0.03, -0.02, -0.02, -0.01, 0.00, -0.02, 0.01],\n",
    "    [-0.01, -0.02, 0.00, 0.01, -0.03, 0.00, -0.02, 0.03, 0.06, -0.06, -0.16, 0.00],\n",
    "    [0.01, 0.00, 0.02, 0.04, -0.02, 0.11, -0.04, -0.01, -0.07, 0.04, 0.00, 0.03],\n",
    "    [-0.02, 0.02, 0.01, 0.15, 0.17, -0.01, -0.12, 0.06, 0.00, -0.01, 0.04, -0.02],\n",
    "    [-0.11, 0.01, -0.12, 0.07, 0.00, 0.12, 0.25, -0.10, 0.00, -0.04, -0.05, 0.31]\n",
    "]\n",
    ")\n",
    "class_10_neg_acc = np.array(\n",
    "[\n",
    "    [0.01, 0.05, -0.01, 0.03, 0.01, -0.06, 0.01, -0.02, 0.01, -0.09, 0.03, 0.00],\n",
    "    [0.03, 0.01, 0.08, 0.03, 0.04, -0.02, 0.02, 0.01, 0.03, 0.02, -0.02, 0.03],\n",
    "    [0.00, 0.00, 0.03, 0.01, 0.02, -0.02, 0.04, 0.01, 0.02, 0.00, 0.02, 0.05],\n",
    "    [0.02, 0.04, 0.03, 0.00, 0.05, 0.00, 0.05, 0.00, 0.01, 0.01, 0.03, -0.01],\n",
    "    [-0.02, 0.00, 0.04, 0.00, 0.01, 0.01, 0.00, 0.01, 0.01, 0.05, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.02, 0.01, 0.04, 0.04, 0.01, 0.00, 0.01, 0.03, 0.00, 0.02],\n",
    "    [0.00, 0.01, 0.00, 0.02, 0.04, -0.03, 0.00, 0.04, 0.05, 0.03, 0.01, 0.01],\n",
    "    [-0.06, 0.00, 0.02, 0.01, 0.00, 0.11, 0.02, 0.01, 0.00, 0.01, 0.02, 0.00],\n",
    "    [0.01, 0.04, 0.02, 0.01, 0.02, 0.01, 0.00, 0.03, -0.01, -0.06, 0.00, -0.04],\n",
    "    [0.04, 0.14, -0.03, 0.01, 0.02, 0.10, 0.02, -0.02, -0.02, 0.04, 0.00, 0.05],\n",
    "    [0.04, -0.02, -0.03, -0.01, 0.21, 0.00, -0.13, 0.23, 0.02, 0.01, 0.00, 0.03],\n",
    "    [0.12, 0.03, 0.01, 0.02, 0.02, -0.03, 0.10, 0.08, -0.03, 0.03, -0.01, -0.07]\n",
    "]\n",
    ")\n",
    "\n",
    "def get_sorted_indices(data):\n",
    "\n",
    "    data_np = np.array(data)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "\n",
    "    result = []\n",
    "\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "def get_sorted_indices_except_max(data):\n",
    "    data_np = np.array(data)\n",
    "    max_indices = np.argmin(data_np, axis=1)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)[::-1]\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        # 각 행의 최대값 인덱스를 제외\n",
    "        if col_indices[i] != max_indices[row_indices[i]]:\n",
    "            result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "ablating_head_num_in_CI = 100\n",
    "\n",
    "\n",
    "correct_num = []\n",
    "correct_num_neg = []\n",
    "for r in range(20, 20 + 1): # 20개 되살림\n",
    "    print('recovered head num: ', r)\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 1번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_1_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_1_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_1 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_1 = model_1.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_1.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 2번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_2_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_2_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_2 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_2 = model_2.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_2.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 3번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_3_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_3_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_3 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_3 = model_3.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_3.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 4번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_4_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_4_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_4 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_4 = model_4.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_4.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 5번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_5_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_5_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_5 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_5 = model_5.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_5.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #6번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_6_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_6_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_6 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_6 = model_6.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_6.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #7번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_7_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_7_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_7 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_7 = model_7.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_7.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #8번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_8_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_8_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_8 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_8 = model_8.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_8.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #9번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_9_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_9_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_9 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_9 = model_9.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_9.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #10번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_10_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_10_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_10 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_10 = model_10.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_10.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.squeeze(1).to(device) for k, v in inputs.items()} \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_1 = model_1(**inputs)\n",
    "            outputs_2 = model_2(**inputs)\n",
    "            outputs_3 = model_3(**inputs)\n",
    "            outputs_4 = model_4(**inputs)\n",
    "            outputs_5 = model_5(**inputs)\n",
    "\n",
    "            outputs_6 = model_6(**inputs)\n",
    "            outputs_7 = model_7(**inputs)\n",
    "            outputs_8 = model_8(**inputs)\n",
    "            outputs_9 = model_9(**inputs)\n",
    "            outputs_10 = model_10(**inputs)\n",
    "            \n",
    "        prediction = outputs_1.logits.argmax(dim=-1) + 1\n",
    "        \n",
    "        preds.extend(prediction.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "        \n",
    "    # print('classification report')\n",
    "    # print(classification_report(true_labels, preds, digits=5))\n",
    "\n",
    "    # print('confusion matrix')\n",
    "    # print(confusion_matrix(true_labels, preds))\n",
    "\n",
    "    con = confusion_matrix(true_labels, preds)\n",
    "    con_diag = np.diag(con) # numpy array\n",
    "    con_vertical = np.sum(con, axis=0) # numpy array\n",
    "\n",
    "    # print(con[0]) # TP ##############################\n",
    "    # print(con_vertical[0]) # TP+ FP ##############################\n",
    "\n",
    "    correct_num.append(con_diag[0].item()) ##############################\n",
    "    correct_num_neg.append(con_vertical[0].item() - con_diag[0].item()) ##############################\n",
    "    # for i in range(10):\n",
    "    #     print(con[i])\n",
    "\n",
    "\n",
    "    print(correct_num)\n",
    "    print(correct_num_neg)\n",
    "\n",
    "print('class 1') ##############################\n",
    "print(correct_num)\n",
    "print(correct_num_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
