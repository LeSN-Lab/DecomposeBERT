{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f1ee1c-0594-49b2-8b60-099cd6398872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d52d686-863f-4ef4-a172-1ae435fa8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5611dd8a-9d8c-42c6-8e2f-83d0cb321880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from utils.model_utils.evaluate import evaluate_model\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.decompose_utils.weight_remover import WeightRemoverBert\n",
    "from utils.decompose_utils.concern_identification import ConcernIdentificationBert\n",
    "from utils.decompose_utils.tangling_identification import TanglingIdentification\n",
    "from transformers import AutoConfig\n",
    "from utils.model_utils.save_module import save_module\n",
    "from datetime import datetime\n",
    "from utils.decompose_utils.concern_modularization import ConcernModularizationBert\n",
    "from utils.decompose_utils.sampling import sampling_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bec197-1db0-45eb-8ad2-eb0fe4cf425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils.evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82e28fa-64e5-4c4a-84e1-e1ba928d9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fabriceyhc/bert-base-uncased-yahoo_answers_topics\"\n",
    "task_type = \"classification\"\n",
    "architectures = \"bert\"\n",
    "dataset_name = \"Yahoo\"\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fda417-04ff-42a1-9ad8-c92a6a5e13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.model_utils.evaluate import get_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca83f3d1-f1a6-4639-a07c-9be575d4180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fcb878-31e0-41f3-8ff1-7220ca9451a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "model_config = ModelConfig(\n",
    "    model_name=model_name,\n",
    "    task_type=task_type,\n",
    "    dataset_name=dataset_name,\n",
    "    checkpoint=checkpoint,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc39d9e-bf4c-48c0-84e6-9d53cc98115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/Minwoo/LESN/Decompose/DecomposeTransformer/Models/Configs/classification/fabriceyhc/bert-base-uncased-yahoo_answers_topics exists.\n",
      "Loading the model.\n",
      "The model fabriceyhc/bert-base-uncased-yahoo_answers_topics is loaded.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, checkpoint = load_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030711ac-259e-4bbf-9b4d-47449daf22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset Yahoo\n",
      "Load cached dataset.\n",
      "The dataset Yahoo is loaded\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
    "        model_config, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6cf8e8-e578-494a-b5a0-b15faf349a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_indices(data):\n",
    "\n",
    "    data_np = np.array(data)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "\n",
    "    result = []\n",
    "\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "def get_sorted_indices_except_max(data):\n",
    "    data_np = np.array(data)\n",
    "    max_indices = np.argmin(data_np, axis=1)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)[::-1]\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        # 각 행의 최대값 인덱스를 제외\n",
    "        if col_indices[i] != max_indices[row_indices[i]]:\n",
    "            result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "ablating_head_num_in_CI = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e6ce30-935f-43da-b1aa-3f41067df563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_data = np.array([\n",
    "    [-0.07, 0.30, 0.07, -0.10, 0.03, -0.30, 0.13, 0.03, -0.27, -0.53, -0.20, -0.13],\n",
    "    [-0.20, -0.10, 0.00, 0.00, 0.03, 0.00, -0.20, -0.37, 0.03, 0.27, -0.47, -0.10],\n",
    "    [0.03, 0.17, 0.07, -0.23, 0.00, 0.10, -0.40, -0.13, -0.17, 0.10, -0.47, -0.03],\n",
    "    [-1.03, 0.13, 0.13, -0.23, -0.07, 0.07, 0.03, 0.03, -0.20, 0.13, -0.23, -0.27],\n",
    "    [-1.03, -0.33, -0.20, -0.40, 0.17, -0.10, 0.10, 0.10, -0.13, 0.03, -0.13, -0.13],\n",
    "    [-0.03, -0.13, 0.07, -0.03, 0.33, 0.00, 0.10, 0.03, -0.27, -0.23, -0.23, -0.17],\n",
    "    [-0.27, 0.03, -0.10, 0.00, -0.13, -0.33, -0.10, 0.40, 1.03, -0.10, 0.13, 0.23],\n",
    "    [-0.57, 0.07, 0.10, 0.03, 0.17, 0.70, 0.07, -0.07, -0.57, 0.07, -0.03, 0.10],\n",
    "    [-0.13, -0.37, -0.03, -0.17, -0.30, 0.03, -0.33, 0.20, 0.13, -0.43, -0.40, -0.50],\n",
    "    [-0.17, -0.17, 0.13, -0.27, -0.40, 1.77, -0.07, 0.03, 0.40, 0.13, 0.17, -0.30],\n",
    "    [0.60, -0.30, -0.03, 0.07, -0.03, 0.00, 0.53, -0.03, 0.43, -0.03, 0.13, 0.80],\n",
    "    [0.50, 0.17, -0.47, -0.10, -0.77, 0.10, 0.60, -0.73, -0.50, -0.03, -0.10, 1.10]\n",
    "])\n",
    "class_2_data = np.array([\n",
    "    [-0.10, -0.07, -0.27, -0.27, 0.27, -0.07, -0.17, -0.33, -0.50, -0.03, -0.17, 0.07],\n",
    "    [0.07, 0.03, -0.17, 0.10, -0.17, -0.23, -0.17, -0.23, -0.07, -0.33, -0.10, -0.27],\n",
    "    [-0.17, -0.13, 0.03, 0.10, 0.03, 0.00, -0.13, -0.20, 0.03, -0.07, -0.10, 0.00],\n",
    "    [0.20, 0.10, 0.07, -0.07, -0.20, -0.43, 0.17, -0.10, -0.37, -0.13, -0.27, -0.07],\n",
    "    [-0.17, -0.07, 0.10, -0.27, -0.20, 0.00, -0.03, -0.20, 0.00, 0.23, 0.00, 0.10],\n",
    "    [-0.10, 0.03, 0.07, 0.03, 0.10, -0.10, 0.17, -0.13, -0.23, 0.17, 0.17, -0.03],\n",
    "    [-0.17, -0.03, 0.13, 0.00, -0.03, -0.13, -0.03, 0.00, -0.47, -0.23, -0.03, 0.30],\n",
    "    [0.17, 0.03, 0.13, -0.07, -0.23, -0.13, 0.00, 0.00, 0.00, -0.13, 0.03, 0.07],\n",
    "    [-0.03, -0.13, -0.03, 0.03, -0.03, -0.10, -0.13, -0.27, -0.07, 0.00, 0.07, -0.03],\n",
    "    [-0.03, -0.70, 0.27, 0.23, 0.10, -1.30, -0.13, -0.10, -0.47, -0.17, 0.03, -0.23],\n",
    "    [0.00, -0.23, 0.13, 0.17, -0.07, 0.03, -0.57, -0.43, -0.43, 0.00, -0.03, -0.93],\n",
    "    [-1.53, 0.07, 0.17, 0.03, 0.03, -0.20, -0.60, -0.07, 0.37, 0.37, -0.23, -0.37]\n",
    "])\n",
    "class_3_data = np.array([\n",
    "    [0.23, 0.33, 0.03, -0.10, 0.07, -0.67, 0.17, 0.30, 0.27, -0.43, 0.00, 0.00],\n",
    "    [0.17, -0.03, 0.20, 0.07, 0.00, 0.00, 0.07, 0.07, 0.20, 0.10, -0.07, 0.07],\n",
    "    [-0.10, -0.10, 0.20, 0.03, -0.03, -0.07, 0.13, -0.03, -0.03, -0.17, 0.00, 0.10],\n",
    "    [-0.20, 0.00, -0.13, 0.00, 0.20, 0.10, -0.07, -0.13, -0.03, 0.07, -0.07, -0.20],\n",
    "    [-0.03, 0.10, -0.20, 0.20, 0.10, -0.27, -0.13, -0.07, -0.07, -0.07, -0.03, 0.03],\n",
    "    [-0.03, -0.13, -0.07, -0.10, 0.10, 0.03, -0.07, 0.00, 0.13, 0.00, -0.17, 0.00],\n",
    "    [0.03, -0.17, -0.07, -0.03, -0.03, -0.07, -0.07, -0.10, 0.00, -0.07, 0.03, 0.03],\n",
    "    [-0.23, -0.13, 0.00, 0.03, 0.07, -0.17, -0.07, 0.00, -0.03, -0.10, -0.20, 0.03],\n",
    "    [-0.03, -0.10, 0.13, -0.07, 0.03, -0.13, -0.10, 0.07, -0.10, 0.00, -0.33, -0.07],\n",
    "    [0.30, -0.03, -0.10, 0.13, 0.07, -0.20, -0.07, -0.40, -0.50, -0.03, -0.13, 0.13],\n",
    "    [0.20, 0.10, -0.20, 0.10, -0.23, 0.07, -1.07, 0.33, 0.27, 0.07, -0.03, -0.20],\n",
    "    [-0.67, 0.07, 0.00, 0.23, -0.23, 0.23, 0.33, -0.07, -0.17, 0.27, 0.07, -0.83]\n",
    "])\n",
    "class_4_data = np.array([\n",
    "    [0.33, 0.20, -0.07, -0.13, -0.10, 0.10, -0.03, 0.23, -0.17, -0.17, -0.30, 0.00],\n",
    "    [-0.07, -0.10, 0.33, 0.07, -0.10, -0.13, 0.03, 0.13, 0.03, 0.03, -0.27, 0.23],\n",
    "    [0.07, -0.43, 0.13, -0.07, 0.10, 0.17, 0.03, -0.13, -0.17, -0.10, 0.07, 0.23],\n",
    "    [0.50, -0.03, -0.03, 0.20, 0.00, 0.10, -0.17, -0.03, -0.17, -0.03, -0.20, 0.07],\n",
    "    [-0.17, -0.17, 0.23, -0.07, -0.13, 0.07, -0.23, -0.03, 0.10, 0.13, -0.17, -0.13],\n",
    "    [-0.07, -0.07, -0.17, 0.00, -0.17, -0.13, -0.07, 0.03, -0.17, 0.10, -0.03, 0.10],\n",
    "    [0.13, -0.03, -0.17, 0.00, -0.07, -0.07, 0.00, -0.27, 0.30, 0.10, -0.03, 0.07],\n",
    "    [0.10, -0.10, -0.10, -0.10, 0.03, -0.50, -0.03, 0.00, 0.10, -0.20, -0.23, 0.03],\n",
    "    [0.07, 0.33, -0.17, 0.10, -0.03, 0.13, 0.03, -0.07, -0.43, -0.13, 0.10, -0.10],\n",
    "    [0.20, 0.33, -0.17, 0.10, -0.23, -0.87, 0.13, -0.60, -0.07, 0.10, 0.50, -0.10],\n",
    "    [-0.70, 0.20, -0.03, 0.67, 0.27, 0.03, 1.47, 0.97, -0.07, -0.03, -0.07, 0.27],\n",
    "    [1.80, 0.30, -0.13, -0.13, 0.57, -0.20, 0.47, 0.03, 0.37, -0.40, 0.03, 0.93]\n",
    "])\n",
    "class_5_data = np.array([\n",
    "    [-0.20, -0.37, -0.10, 0.20, -0.07, 0.33, 0.17, -0.10, 0.03, 0.20, -0.03, 0.03],\n",
    "    [0.03, 0.03, 0.30, -0.10, -0.23, -0.30, 0.00, -0.27, -0.17, -0.10, 0.17, -0.13],\n",
    "    [-0.33, 0.03, 0.03, -0.13, -0.10, -0.33, -0.10, -0.17, -0.17, -0.27, 0.07, 0.07],\n",
    "    [-0.43, -0.10, -0.10, -0.17, -0.13, 0.03, 0.00, -0.10, 0.10, -0.03, 0.13, -0.23],\n",
    "    [-0.33, 0.13, 0.13, -0.07, 0.10, 0.07, -0.07, -0.07, -0.07, 0.20, -0.03, 0.10],\n",
    "    [0.10, 0.00, -0.17, -0.13, 0.03, -0.10, -0.03, 0.00, 0.07, 0.03, -0.10, -0.03],\n",
    "    [0.07, 0.07, -0.17, 0.00, -0.13, 0.17, 0.13, -0.07, 0.20, 0.03, -0.10, 0.00],\n",
    "    [-0.03, -0.23, 0.13, -0.03, -0.13, -0.37, -0.10, 0.13, -0.07, -0.10, 0.20, -0.20],\n",
    "    [-0.03, 0.07, 0.00, 0.03, 0.03, -0.03, -0.20, 0.00, 0.10, 0.03, 0.23, 0.27],\n",
    "    [0.07, 0.57, 0.00, -0.20, -0.13, -1.30, -0.13, 0.20, 0.30, 0.00, -0.53, -0.33],\n",
    "    [0.63, 0.13, -0.30, -0.63, -0.53, 0.03, -0.67, -0.90, 0.20, 0.17, 0.00, -0.07],\n",
    "    [-0.57, -0.23, -0.20, 0.30, 0.03, 0.03, -0.73, -0.70, -0.37, 0.60, -0.53, 1.37]\n",
    "])\n",
    "class_6_data = np.array([\n",
    "    [0.10, -0.17, 0.03, 0.00, 0.10, 0.27, -0.10, -0.07, 0.27, 0.23, 0.07, -0.03],\n",
    "    [0.00, 0.00, 0.03, -0.03, 0.00, -0.03, -0.03, -0.07, 0.13, 0.03, -0.10, -0.23],\n",
    "    [0.10, 0.07, 0.07, 0.03, 0.07, -0.03, -0.10, 0.07, 0.00, 0.03, 0.00, -0.17],\n",
    "    [-0.07, 0.00, -0.03, -0.07, 0.03, 0.07, 0.00, -0.10, -0.13, -0.07, -0.07, 0.00],\n",
    "    [0.03, 0.03, 0.13, 0.03, 0.07, 0.03, -0.07, -0.07, 0.00, 0.00, -0.10, 0.03],\n",
    "    [-0.03, 0.00, 0.00, 0.00, 0.00, -0.07, -0.03, -0.07, -0.07, -0.10, 0.03, -0.03],\n",
    "    [0.00, -0.10, 0.00, 0.00, 0.00, -0.03, -0.03, -0.03, -0.17, 0.00, 0.00, 0.03],\n",
    "    [0.07, -0.07, -0.07, -0.03, -0.10, -0.20, -0.03, 0.03, -0.03, 0.03, 0.03, 0.00],\n",
    "    [-0.07, 0.03, -0.03, -0.03, -0.07, -0.03, -0.03, -0.03, -0.03, -0.03, -0.03, 0.07],\n",
    "    [-0.10, -0.27, 0.03, 0.03, 0.00, -0.60, -0.07, 0.13, -0.10, 0.00, -0.03, 0.13],\n",
    "    [0.00, -0.07, 0.13, 0.03, 0.13, -0.03, -1.00, -0.13, -0.10, 0.00, 0.00, 0.03],\n",
    "    [-0.10, -0.13, 0.03, -0.07, -0.03, -0.03, 0.13, 0.03, 0.07, 0.03, 0.07, -1.00]\n",
    "])\n",
    "class_7_data = np.array([\n",
    "    [0.20, -0.23, -0.17, -0.27, 0.00, -0.33, 0.07, -0.30, -0.30, -0.17, 0.13, -0.50],\n",
    "    [-0.13, -0.13, -0.30, 0.17, 0.17, -0.03, -0.07, 0.13, 0.00, -0.10, -0.13, 0.23],\n",
    "    [-0.30, 0.03, -0.13, 0.13, -0.07, -0.13, -0.10, 0.23, -0.07, -0.20, -0.30, 0.07],\n",
    "    [-0.13, 0.30, -0.20, 0.03, 0.23, -0.27, -0.17, 0.07, -0.53, -0.07, -0.17, -0.03],\n",
    "    [-0.43, -0.07, -0.03, -0.13, -0.20, -0.03, -0.03, -0.10, 0.07, 0.27, -0.23, 0.07],\n",
    "    [0.03, 0.03, -0.10, -0.10, -0.17, -0.10, -0.03, -0.30, 0.10, 0.00, -0.23, -0.20],\n",
    "    [-0.07, -0.23, -0.10, -0.10, 0.07, -0.27, -0.10, 0.03, -0.30, 0.10, 0.13, 0.07],\n",
    "    [-0.47, -0.07, -0.07, -0.13, -0.13, -0.13, -0.13, -0.20, 0.13, -0.07, 0.03, -0.13],\n",
    "    [-0.07, -0.03, -0.03, -0.03, -0.07, 0.00, -0.07, 0.23, -0.17, -0.57, -0.20, -0.67],\n",
    "    [0.00, 0.07, -0.20, -0.03, 0.03, 0.60, 0.17, -0.17, -0.77, -0.03, -0.07, 0.53],\n",
    "    [0.03, -0.37, 0.03, -0.30, 0.60, -0.30, 0.53, 0.77, -0.03, -0.20, 0.03, -0.10],\n",
    "    [0.17, 0.13, 0.13, 0.50, -0.27, -0.27, 0.97, 0.90, 0.03, -0.13, 0.40, -2.33]\n",
    "])\n",
    "class_8_data = np.array([\n",
    "    [0.00, -0.20, -0.23, -0.03, -0.17, 0.27, -0.37, -0.03, -0.17, 0.03, -0.40, -0.03],\n",
    "    [-0.53, -0.07, 0.03, -0.07, -0.23, -0.23, -0.20, 0.03, -0.10, 0.10, -0.10, 0.33],\n",
    "    [-0.17, -0.07, -0.10, -0.07, -0.03, -0.07, 0.33, 0.13, 0.20, -0.07, 0.17, -0.13],\n",
    "    [0.33, -0.10, 0.03, 0.03, -0.17, -0.33, 0.03, 0.00, 0.30, -0.23, -0.17, -0.17],\n",
    "    [0.27, 0.00, 0.43, -0.30, -0.33, 0.07, -0.13, -0.23, 0.00, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.23, -0.20, 0.13, -0.20, -0.13, 0.03, -0.10, 0.07, 0.00, -0.23, 0.03],\n",
    "    [-0.20, -0.03, 0.20, 0.00, 0.03, 0.07, -0.13, -0.13, 0.13, -0.17, 0.00, 0.03],\n",
    "    [-0.10, -0.20, -0.17, 0.13, -0.13, 0.20, -0.07, -0.13, 0.17, 0.03, 0.20, -0.07],\n",
    "    [-0.03, 0.10, 0.10, 0.00, -0.03, -0.03, 0.13, 0.07, -0.17, 0.10, 0.23, 0.10],\n",
    "    [0.03, 0.13, 0.00, 0.03, 0.20, 0.60, -0.10, 0.37, 0.57, 0.13, -0.40, 0.03],\n",
    "    [-0.23, 0.27, -0.30, -0.27, 0.93, 0.10, -0.60, 0.07, 0.00, 0.03, -0.13, 0.00],\n",
    "    [-0.50, -0.07, -0.13, 0.00, 0.40, 0.20, -0.60, 0.27, -0.13, -0.30, 0.00, 1.37]\n",
    "])\n",
    "class_9_data = np.array([\n",
    "    [-0.40, -0.27, -0.13, -0.07, 0.00, -0.73, -0.37, -0.27, 0.20, -0.53, -0.10, -0.20],\n",
    "    [0.07, 0.00, 0.07, -0.30, -0.10, -0.13, -0.03, 0.17, -0.30, -0.03, -0.27, -0.07],\n",
    "    [-0.13, 0.13, -0.10, 0.00, -0.03, -0.23, 0.00, -0.33, -0.10, -0.33, -0.07, -0.07],\n",
    "    [0.00, -0.17, -0.10, 0.03, -0.03, -0.07, 0.07, -0.20, 0.13, -0.13, -0.20, -0.07],\n",
    "    [0.37, 0.07, -0.13, 0.10, -0.20, -0.17, 0.07, 0.00, -0.10, -0.20, -0.03, -0.10],\n",
    "    [-0.13, -0.07, -0.13, -0.03, -0.17, -0.03, -0.17, 0.00, -0.17, -0.07, -0.13, 0.00],\n",
    "    [0.20, -0.27, -0.27, -0.07, -0.07, -0.17, -0.10, -0.27, -0.43, 0.07, -0.10, -0.07],\n",
    "    [0.13, -0.07, -0.17, -0.07, -0.20, -0.27, 0.00, 0.03, -0.03, -0.20, -0.13, -0.27],\n",
    "    [-0.07, 0.07, -0.13, -0.20, 0.03, -0.17, -0.20, -0.27, -0.27, 0.23, 0.37, -0.07],\n",
    "    [-0.10, -0.03, -0.27, -0.13, -0.07, -0.77, 0.03, 0.03, 0.00, -0.27, -0.13, -0.20],\n",
    "    [-0.10, -0.30, -0.23, -0.97, -0.93, 0.00, 0.50, -0.33, -0.03, -0.07, -0.10, 0.00],\n",
    "    [0.40, -0.20, 0.33, -0.43, -0.13, -0.83, -0.93, 0.40, -0.20, 0.07, 0.00, -1.13]\n",
    "])\n",
    "class_10_data = np.array([\n",
    "    [-0.07, 0.07, 0.07, 0.13, 0.00, 0.27, -0.07, 0.10, 0.23, 0.63, -0.07, 0.00],\n",
    "    [-0.07, -0.07, -0.37, -0.03, -0.13, -0.03, -0.03, -0.13, 0.00, -0.17, -0.07, 0.00],\n",
    "    [-0.13, 0.20, -0.10, -0.10, 0.10, 0.20, -0.10, 0.07, 0.00, -0.03, 0.20, -0.20],\n",
    "    [0.03, 0.07, -0.07, 0.10, 0.00, 0.07, -0.10, 0.03, 0.17, 0.00, -0.03, 0.17],\n",
    "    [-0.10, -0.07, -0.20, 0.13, 0.03, -0.10, 0.03, -0.03, -0.13, -0.13, 0.00, 0.00],\n",
    "    [0.10, -0.03, -0.07, -0.03, -0.07, 0.00, 0.00, 0.03, -0.03, 0.07, 0.13, -0.13],\n",
    "    [0.33, -0.10, 0.07, 0.07, -0.10, 0.17, 0.07, -0.03, -0.07, -0.03, 0.03, 0.07],\n",
    "    [0.63, 0.07, 0.00, -0.07, 0.20, -0.27, -0.07, -0.03, 0.07, 0.17, -0.10, -0.03],\n",
    "    [-0.07, 0.13, 0.10, 0.00, -0.03, 0.00, 0.03, -0.03, 0.40, 0.33, 0.33, 0.20],\n",
    "    [-0.20, -0.33, 0.30, 0.07, 0.23, -0.60, 0.00, 0.13, 0.37, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.03, 0.37, 0.20, -0.37, -0.07, 0.90, -1.03, -0.03, 0.00, 0.07, 0.03],\n",
    "    [-0.43, -0.17, -0.03, -0.07, -0.17, 0.23, -0.13, -0.37, 0.30, -0.07, 0.33, 0.33]\n",
    "])\n",
    "\n",
    "class_1_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, -0.09, -0.03, -0.06, -0.04, -0.02, -0.09, -0.06, 0.00, 0.00, -0.03, -0.03],\n",
    "    [-0.06, -0.05, -0.03, -0.06, -0.05, -0.09, -0.03, -0.04, -0.03, -0.02, -0.01, 0.05],\n",
    "    [-0.08, -0.06, -0.01, -0.02, 0.00, -0.06, 0.04, -0.04, -0.01, -0.06, 0.03, -0.06],\n",
    "    [0.13, -0.03, -0.07, -0.01, 0.02, -0.03, 0.01, -0.02, -0.02, -0.02, -0.03, -0.03],\n",
    "    [0.07, 0.06, 0.07, -0.04, -0.11, -0.03, -0.04, -0.02, -0.02, 0.00, 0.00, 0.00],\n",
    "    [-0.01, -0.02, -0.05, -0.02, -0.05, -0.08, -0.07, 0.01, 0.00, 0.01, -0.04, 0.01],\n",
    "    [-0.01, -0.05, -0.06, 0.00, 0.00, 0.01, -0.03, -0.12, -0.15, -0.03, -0.03, 0.04],\n",
    "    [0.06, -0.04, -0.05, -0.03, -0.07, -0.22, -0.01, -0.01, 0.13, -0.04, 0.00, -0.02],\n",
    "    [-0.02, 0.09, -0.05, -0.02, 0.03, -0.02, 0.05, -0.05, -0.12, 0.01, 0.10, 0.01],\n",
    "    [0.04, -0.06, -0.01, 0.05, 0.01, -0.64, -0.05, -0.03, 0.00, -0.05, -0.09, 0.00],\n",
    "    [-0.09, 0.05, -0.06, -0.06, 0.03, -0.01, -0.22, -0.02, -0.07, -0.03, -0.03, -0.22],\n",
    "    [-0.20, 0.02, 0.05, 0.02, 0.09, -0.02, -0.25, 0.05, 0.05, -0.03, 0.05, -0.30]\n",
    "]   \n",
    ")\n",
    "class_2_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, -0.02, -0.01, 0.05, -0.03, 0.02, -0.01, 0.03, 0.10, -0.04, 0.01, -0.04],\n",
    "    [-0.01, 0.02, 0.01, -0.02, 0.01, 0.00, 0.00, 0.08, 0.00, 0.04, -0.01, 0.05],\n",
    "    [0.00, -0.01, 0.01, 0.01, 0.00, -0.02, 0.01, 0.03, 0.00, -0.01, 0.02, 0.02],\n",
    "    [-0.04, -0.01, -0.02, 0.01, -0.01, 0.03, -0.02, 0.00, 0.04, -0.01, -0.01, 0.00],\n",
    "    [0.03, 0.00, -0.01, 0.03, 0.00, 0.03, 0.04, 0.02, 0.00, 0.00, -0.02, 0.01],\n",
    "    [0.00, -0.01, -0.02, -0.02, -0.02, -0.01, 0.00, 0.01, -0.02, -0.02, -0.01, 0.00],\n",
    "    [0.03, -0.01, 0.00, 0.00, -0.01, 0.01, 0.01, 0.00, 0.05, 0.00, 0.01, -0.01],\n",
    "    [-0.01, -0.01, -0.02, 0.02, -0.01, 0.00, 0.02, 0.00, -0.01, -0.01, 0.02, -0.01],\n",
    "    [0.01, 0.01, 0.02, 0.00, 0.02, 0.01, 0.01, 0.02, -0.02, 0.01, -0.01, 0.01],\n",
    "    [0.00, 0.04, -0.01, 0.01, -0.02, 0.12, 0.03, 0.00, 0.08, 0.03, 0.00, 0.04],\n",
    "    [0.01, 0.04, -0.02, -0.04, -0.01, 0.02, 0.06, 0.06, 0.04, 0.00, 0.00, 0.13],\n",
    "    [0.21, 0.03, -0.05, -0.01, 0.02, 0.02, 0.05, 0.02, -0.05, -0.04, 0.02, 0.05]\n",
    "]\n",
    ")\n",
    "class_3_neg_acc = np.array(\n",
    "[\n",
    "    [-0.05, -0.04, -0.02, 0.00, 0.04, 0.12, -0.01, -0.04, -0.02, 0.08, -0.02, -0.02],\n",
    "    [-0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02, 0.00, -0.01, -0.01, 0.02],\n",
    "    [0.02, 0.03, -0.02, 0.02, 0.01, 0.02, -0.01, 0.02, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.02, 0.03, 0.05, 0.02, -0.05, -0.04, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02],\n",
    "    [0.03, -0.02, 0.04, -0.04, -0.01, 0.02, 0.03, 0.03, 0.00, 0.00, 0.02, 0.03],\n",
    "    [0.00, 0.00, 0.00, 0.04, -0.03, 0.02, 0.02, -0.01, 0.01, 0.06, 0.02, -0.02],\n",
    "    [0.04, 0.06, 0.04, 0.01, 0.01, 0.07, -0.03, 0.02, 0.02, 0.05, 0.04, 0.05],\n",
    "    [0.02, 0.02, 0.02, 0.00, -0.01, 0.07, 0.00, -0.01, 0.02, -0.02, 0.06, -0.01],\n",
    "    [0.00, 0.03, 0.02, 0.02, 0.02, 0.01, -0.02, -0.01, 0.01, 0.06, 0.12, 0.00],\n",
    "    [-0.05, 0.03, 0.03, -0.05, 0.00, 0.11, 0.03, 0.05, 0.10, 0.04, 0.01, -0.03],\n",
    "    [-0.04, -0.01, 0.07, -0.03, 0.07, 0.00, 0.28, -0.10, 0.00, 0.00, 0.01, 0.06],\n",
    "    [0.19, -0.01, -0.01, -0.04, 0.07, -0.07, -0.10, 0.04, 0.05, -0.04, 0.00, 0.30]\n",
    "]\n",
    ")\n",
    "class_4_neg_acc = np.array(\n",
    "[\n",
    "    [-0.04, -0.03, 0.01, 0.02, 0.03, -0.09, -0.05, 0.00, 0.02, -0.03, -0.01, 0.02],\n",
    "    [0.03, 0.02, 0.02, -0.01, -0.01, 0.00, 0.00, -0.04, -0.02, 0.01, -0.01, -0.06],\n",
    "    [-0.04, 0.00, 0.03, 0.00, 0.00, -0.01, -0.01, 0.01, 0.00, -0.04, -0.04, 0.01],\n",
    "    [-0.09, 0.02, -0.03, -0.03, -0.02, -0.01, 0.02, 0.00, 0.01, -0.03, -0.04, -0.02],\n",
    "    [-0.09, -0.01, -0.01, -0.02, -0.01, -0.03, 0.02, -0.05, -0.02, -0.02, -0.02, -0.03],\n",
    "    [0.01, -0.01, 0.00, -0.01, 0.02, 0.00, 0.02, -0.01, 0.02, 0.01, -0.04, -0.02],\n",
    "    [-0.03, -0.04, -0.02, -0.01, 0.00, -0.03, 0.00, 0.05, -0.01, -0.04, 0.00, 0.01],\n",
    "    [0.00, 0.02, 0.02, 0.00, 0.00, 0.05, -0.02, -0.01, -0.03, 0.00, 0.00, -0.03],\n",
    "    [-0.01, -0.03, 0.03, -0.04, -0.03, -0.01, -0.03, 0.03, 0.08, 0.02, 0.00, 0.03],\n",
    "    [-0.03, -0.06, 0.03, -0.03, 0.04, 0.14, -0.02, 0.06, -0.05, -0.04, -0.06, 0.05],\n",
    "    [0.18, -0.08, 0.02, -0.16, -0.06, 0.00, -0.21, -0.20, 0.03, 0.01, 0.01, -0.02],\n",
    "    [-0.31, -0.06, 0.03, 0.05, -0.09, -0.02, -0.16, -0.01, -0.07, 0.08, 0.02, -0.19]\n",
    "]  \n",
    ")\n",
    "class_5_neg_acc = np.array(\n",
    "[\n",
    "    [0.03, 0.04, 0.03, -0.02, -0.01, -0.04, 0.00, 0.00, -0.03, 0.01, -0.01, -0.01],\n",
    "    [-0.01, 0.00, -0.02, 0.03, 0.00, 0.00, 0.00, 0.02, 0.01, 0.01, -0.02, 0.02],\n",
    "    [0.02, 0.01, -0.01, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, 0.00, 0.00],\n",
    "    [0.02, 0.01, 0.01, 0.02, -0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02],\n",
    "    [0.03, -0.01, 0.01, 0.00, 0.02, 0.02, -0.01, 0.01, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.00, 0.01, 0.00, 0.01, -0.01, -0.01, 0.02, 0.02, -0.01, 0.00, 0.01, 0.00],\n",
    "    [0.00, -0.01, 0.01, 0.00, 0.00, -0.02, 0.00, 0.01, 0.01, -0.01, 0.02, 0.01],\n",
    "    [-0.01, 0.00, 0.00, 0.00, 0.02, 0.03, 0.01, 0.01, -0.03, 0.00, -0.02, 0.02],\n",
    "    [0.00, 0.00, -0.01, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, -0.01, 0.00, -0.05],\n",
    "    [0.03, -0.06, -0.01, -0.01, 0.02, 0.15, 0.00, -0.01, -0.04, 0.00, 0.05, 0.03],\n",
    "    [-0.07, 0.01, 0.01, 0.05, 0.03, 0.00, 0.13, 0.09, 0.00, 0.00, -0.01, 0.01],\n",
    "    [0.03, 0.01, 0.01, -0.03, 0.00, 0.00, 0.07, 0.07, 0.02, -0.03, 0.04, -0.13]\n",
    "] \n",
    ")\n",
    "class_6_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, 0.02, -0.02, 0.00, 0.01, -0.04, 0.02, 0.02, -0.02, -0.02, -0.02, 0.01],\n",
    "    [0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, 0.00, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.01, 0.00, 0.00, 0.01, 0.00, -0.02, 0.00, 0.00, 0.01, 0.01],\n",
    "    [0.00, 0.01, 0.01, 0.00, 0.01, -0.01, 0.01, 0.01, 0.02, -0.01, 0.02, -0.01],\n",
    "    [-0.03, 0.00, -0.02, 0.03, 0.01, 0.01, 0.01, 0.00, 0.02, 0.01, 0.00, 0.02],\n",
    "    [0.01, 0.01, 0.00, 0.01, 0.00, 0.01, 0.01, 0.02, -0.02, 0.01, 0.00, 0.01],\n",
    "    [0.02, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02, 0.00, 0.06, 0.01, 0.01, 0.01],\n",
    "    [0.01, 0.01, 0.00, 0.02, 0.02, 0.02, 0.01, 0.00, -0.01, 0.01, -0.01, 0.01],\n",
    "    [0.01, 0.00, 0.01, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.03, -0.01],\n",
    "    [0.02, 0.03, 0.00, 0.01, -0.01, 0.08, 0.01, 0.00, 0.06, 0.01, 0.01, -0.02],\n",
    "    [0.02, 0.00, 0.00, 0.01, -0.04, 0.01, 0.16, 0.01, 0.03, 0.01, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.03, 0.00, -0.02, -0.01, 0.00, 0.02, 0.00, 0.02, 0.00, 0.14]\n",
    "]  \n",
    ")\n",
    "class_7_neg_acc = np.array(\n",
    "[\n",
    "    [-0.02, 0.03, 0.01, 0.01, 0.00, 0.04, 0.00, 0.04, 0.02, 0.06, -0.02, 0.01],\n",
    "    [-0.01, -0.03, 0.03, 0.01, -0.02, -0.01, -0.01, 0.01, 0.00, 0.00, -0.02, -0.03],\n",
    "    [0.01, 0.03, 0.02, -0.01, -0.01, -0.02, 0.01, 0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "    [0.03, -0.02, 0.01, -0.02, -0.01, -0.02, 0.00, 0.00, 0.01, 0.01, 0.02, -0.02],\n",
    "    [0.03, 0.00, 0.01, 0.00, -0.01, -0.01, -0.01, -0.01, -0.01, 0.00, 0.00, -0.03],\n",
    "    [-0.04, -0.02, 0.00, 0.00, 0.00, -0.03, 0.00, -0.02, -0.02, -0.02, -0.01, -0.01],\n",
    "    [0.02, 0.01, 0.00, 0.00, -0.03, 0.04, 0.01, -0.03, 0.00, 0.01, -0.01, 0.00],\n",
    "    [0.05, -0.02, 0.00, 0.00, 0.03, 0.00, -0.01, 0.01, -0.04, -0.01, -0.01, 0.00],\n",
    "    [-0.01, -0.02, -0.02, 0.00, -0.01, -0.01, -0.01, -0.07, 0.00, 0.03, 0.02, 0.04],\n",
    "    [-0.01, 0.00, 0.02, 0.00, -0.01, -0.11, -0.03, 0.07, 0.10, -0.01, -0.02, -0.09],\n",
    "    [0.00, 0.04, -0.02, -0.03, -0.05, 0.00, -0.03, -0.14, 0.02, 0.03, -0.01, -0.02],\n",
    "    [-0.08, -0.03, -0.01, -0.06, 0.01, 0.03, -0.15, -0.11, 0.00, 0.00, -0.06, 0.17]\n",
    "]\n",
    ")\n",
    "class_8_neg_acc = np.array(\n",
    "[\n",
    "    [0.02, -0.02, -0.02, -0.05, 0.01, -0.10, -0.02, -0.03, -0.01, -0.10, -0.01, 0.00],\n",
    "    [0.02, 0.00, -0.04, -0.02, 0.00, -0.02, 0.00, -0.08, 0.00, -0.06, -0.01, -0.04],\n",
    "    [-0.04, 0.00, 0.00, 0.01, 0.00, 0.03, -0.09, -0.06, -0.06, -0.06, -0.13, 0.00],\n",
    "    [-0.13, -0.03, -0.02, -0.02, 0.02, 0.02, -0.05, -0.05, -0.09, -0.02, -0.09, 0.02],\n",
    "    [-0.17, 0.00, -0.10, 0.04, 0.04, -0.03, -0.01, -0.03, 0.00, -0.01, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.02, 0.00, 0.01, 0.01, 0.00, -0.04, -0.03, -0.03, 0.03, -0.03],\n",
    "    [-0.02, -0.05, -0.03, -0.02, -0.05, -0.10, -0.01, -0.03, -0.03, -0.01, -0.03, -0.02],\n",
    "    [-0.01, -0.03, -0.02, -0.03, -0.02, -0.20, -0.04, 0.01, -0.04, 0.01, -0.05, -0.02],\n",
    "    [-0.01, -0.06, -0.03, 0.00, -0.05, -0.02, -0.07, 0.01, -0.02, -0.06, -0.05, -0.06],\n",
    "    [-0.02, -0.09, -0.03, -0.03, -0.05, -0.35, 0.03, -0.14, -0.15, -0.08, 0.04, -0.07],\n",
    "    [0.03, -0.12, 0.00, 0.04, -0.36, -0.02, 0.10, -0.05, -0.03, -0.02, 0.00, 0.00],\n",
    "    [0.03, -0.02, 0.04, 0.02, -0.15, -0.07, 0.14, -0.08, 0.01, 0.12, 0.01, -0.33]\n",
    "]\n",
    ")\n",
    "class_9_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, 0.03, 0.00, -0.04, 0.01, 0.10, 0.10, 0.02, -0.08, 0.06, -0.02, -0.01],\n",
    "    [-0.03, -0.02, 0.00, 0.04, -0.04, 0.00, -0.02, -0.02, 0.03, 0.01, -0.02, -0.02],\n",
    "    [0.00, -0.01, -0.02, -0.03, 0.02, 0.01, -0.02, 0.01, 0.00, 0.02, -0.01, -0.02],\n",
    "    [-0.03, 0.02, 0.00, 0.03, 0.00, 0.01, -0.01, 0.00, -0.03, 0.00, 0.01, -0.04],\n",
    "    [-0.06, -0.04, 0.01, -0.08, 0.02, -0.03, -0.06, -0.03, -0.01, 0.02, -0.05, -0.01],\n",
    "    [-0.02, -0.03, 0.01, -0.03, 0.02, -0.02, 0.00, -0.01, 0.02, -0.05, -0.03, 0.00],\n",
    "    [-0.04, 0.00, 0.00, -0.01, 0.00, 0.01, 0.01, 0.04, 0.05, -0.01, 0.00, 0.01],\n",
    "    [-0.07, -0.01, 0.01, -0.01, -0.02, 0.03, -0.02, -0.02, -0.01, 0.00, -0.02, 0.01],\n",
    "    [-0.01, -0.02, 0.00, 0.01, -0.03, 0.00, -0.02, 0.03, 0.06, -0.06, -0.16, 0.00],\n",
    "    [0.01, 0.00, 0.02, 0.04, -0.02, 0.11, -0.04, -0.01, -0.07, 0.04, 0.00, 0.03],\n",
    "    [-0.02, 0.02, 0.01, 0.15, 0.17, -0.01, -0.12, 0.06, 0.00, -0.01, 0.04, -0.02],\n",
    "    [-0.11, 0.01, -0.12, 0.07, 0.00, 0.12, 0.25, -0.10, 0.00, -0.04, -0.05, 0.31]\n",
    "]\n",
    ")\n",
    "class_10_neg_acc = np.array(\n",
    "[\n",
    "    [0.01, 0.05, -0.01, 0.03, 0.01, -0.06, 0.01, -0.02, 0.01, -0.09, 0.03, 0.00],\n",
    "    [0.03, 0.01, 0.08, 0.03, 0.04, -0.02, 0.02, 0.01, 0.03, 0.02, -0.02, 0.03],\n",
    "    [0.00, 0.00, 0.03, 0.01, 0.02, -0.02, 0.04, 0.01, 0.02, 0.00, 0.02, 0.05],\n",
    "    [0.02, 0.04, 0.03, 0.00, 0.05, 0.00, 0.05, 0.00, 0.01, 0.01, 0.03, -0.01],\n",
    "    [-0.02, 0.00, 0.04, 0.00, 0.01, 0.01, 0.00, 0.01, 0.01, 0.05, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.02, 0.01, 0.04, 0.04, 0.01, 0.00, 0.01, 0.03, 0.00, 0.02],\n",
    "    [0.00, 0.01, 0.00, 0.02, 0.04, -0.03, 0.00, 0.04, 0.05, 0.03, 0.01, 0.01],\n",
    "    [-0.06, 0.00, 0.02, 0.01, 0.00, 0.11, 0.02, 0.01, 0.00, 0.01, 0.02, 0.00],\n",
    "    [0.01, 0.04, 0.02, 0.01, 0.02, 0.01, 0.00, 0.03, -0.01, -0.06, 0.00, -0.04],\n",
    "    [0.04, 0.14, -0.03, 0.01, 0.02, 0.10, 0.02, -0.02, -0.02, 0.04, 0.00, 0.05],\n",
    "    [0.04, -0.02, -0.03, -0.01, 0.21, 0.00, -0.13, 0.23, 0.02, 0.01, 0.00, 0.03],\n",
    "    [0.12, 0.03, 0.01, 0.02, 0.02, -0.03, 0.10, 0.08, -0.03, 0.03, -0.01, -0.07]\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "923bb456-d3ab-4511-b101-bb88705462f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered head num:  20\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  18\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  18\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n"
     ]
    }
   ],
   "source": [
    "correct_num = []\n",
    "correct_num_neg = []\n",
    "for r in range(20, 20 + 1): # 20개 되살림\n",
    "    print('recovered head num: ', r)\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 1번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_1_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_1_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_1 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_1 = model_1.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_1.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 2번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_2_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_2_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_2 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_2 = model_2.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_2.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 3번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_3_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_3_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_3 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_3 = model_3.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_3.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 4번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_4_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_4_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_4 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_4 = model_4.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_4.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 5번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_5_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_5_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_5 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_5 = model_5.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_5.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #6번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_6_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_6_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_6 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_6 = model_6.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_6.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #7번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_7_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_7_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_7 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_7 = model_7.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_7.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #8번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_8_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_8_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_8 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_8 = model_8.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_8.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #9번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_9_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_9_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_9 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_9 = model_9.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_9.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #10번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_10_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_10_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_10 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_10 = model_10.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_10.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe29a5e-a979-43b1-9011-ebd0fd2bf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2c3cbf-3f47-4212-b73f-555594d67232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Module 0 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [05:50<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0719\n",
      "Precision: 0.6671, Recall: 0.6588, F1-Score: 0.6594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      6000\n",
      "           1       0.74      0.61      0.67      6000\n",
      "           2       0.70      0.73      0.72      6000\n",
      "           3       0.52      0.49      0.51      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.88      0.78      0.83      6000\n",
      "           6       0.53      0.44      0.48      6000\n",
      "           7       0.52      0.74      0.62      6000\n",
      "           8       0.69      0.70      0.69      6000\n",
      "           9       0.75      0.71      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.1769633945708363\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.2968915303548177, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.2837452358669705, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.28920915391710067, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.28491846720377606, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.28997717963324654, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.2847056918674045, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.2878129747178819, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.2999009026421441, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.29638714260525173, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.287375979953342, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.28934351603190106, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2884415520562066, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.28922992282443577, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.2864240010579427, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.2943539089626736, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.28559833102756077, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2917128668891059, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.28582848442925346, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.2892044915093316, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.2857839796278212, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.293709225124783, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.2841224670410156, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.29446962144639754, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.2985263400607639, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2956610785590278, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [05:54<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1135\n",
      "Precision: 0.6606, Recall: 0.6463, F1-Score: 0.6498\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.61      0.54      6000\n",
      "           1       0.77      0.54      0.63      6000\n",
      "           2       0.69      0.73      0.71      6000\n",
      "           3       0.47      0.51      0.49      6000\n",
      "           4       0.80      0.77      0.78      6000\n",
      "           5       0.89      0.76      0.82      6000\n",
      "           6       0.46      0.47      0.47      6000\n",
      "           7       0.60      0.68      0.64      6000\n",
      "           8       0.71      0.66      0.69      6000\n",
      "           9       0.72      0.72      0.72      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.65     60000\n",
      "weighted avg       0.66      0.65      0.65     60000\n",
      "\n",
      "#Module 1 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [05:59<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1081\n",
      "Precision: 0.6734, Recall: 0.6481, F1-Score: 0.6514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.49      0.55      6000\n",
      "           1       0.74      0.62      0.67      6000\n",
      "           2       0.76      0.67      0.71      6000\n",
      "           3       0.55      0.45      0.50      6000\n",
      "           4       0.79      0.79      0.79      6000\n",
      "           5       0.91      0.77      0.84      6000\n",
      "           6       0.49      0.45      0.47      6000\n",
      "           7       0.42      0.81      0.55      6000\n",
      "           8       0.68      0.71      0.70      6000\n",
      "           9       0.76      0.72      0.74      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17719497765505676\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.296630859375, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.2978498670789931, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.2894325256347656, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.29962751600477433, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2877371046278212, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.28729883829752606, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.28647104899088544, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.28748151991102433, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.28674570719401044, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.28696441650390625, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.2887136671278212, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2883533901638455, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.2879685295952691, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.28793377346462673, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.2879346211751302, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.2869427998860677, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2880876329210069, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.2839402092827691, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.285797119140625, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.28582848442925346, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.28801896837022567, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.29819827609592015, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.2883584764268663, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.29889339870876735, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2992028130425347, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1998\n",
      "Precision: 0.6711, Recall: 0.6246, F1-Score: 0.6349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.44      0.54      6000\n",
      "           1       0.72      0.61      0.66      6000\n",
      "           2       0.78      0.60      0.68      6000\n",
      "           3       0.53      0.43      0.48      6000\n",
      "           4       0.77      0.76      0.77      6000\n",
      "           5       0.93      0.72      0.82      6000\n",
      "           6       0.32      0.57      0.41      6000\n",
      "           7       0.49      0.75      0.59      6000\n",
      "           8       0.71      0.65      0.68      6000\n",
      "           9       0.76      0.70      0.73      6000\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.67      0.62      0.63     60000\n",
      "weighted avg       0.67      0.62      0.63     60000\n",
      "\n",
      "#Module 2 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:06<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0775\n",
      "Precision: 0.6673, Recall: 0.6577, F1-Score: 0.6577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55      6000\n",
      "           1       0.73      0.61      0.67      6000\n",
      "           2       0.71      0.72      0.72      6000\n",
      "           3       0.50      0.52      0.51      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.89      0.78      0.83      6000\n",
      "           6       0.56      0.43      0.49      6000\n",
      "           7       0.51      0.75      0.61      6000\n",
      "           8       0.64      0.76      0.69      6000\n",
      "           9       0.76      0.70      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17811074453096135\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.2962426079644097, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.28391435411241317, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.28886455959743923, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.2847548590766059, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2914992438422309, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.28726959228515625, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.28596199883355033, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.2873217264811198, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.2877388000488281, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.2879981994628906, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.28781890869140625, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.28738827175564235, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.2909088134765625, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.29894595675998265, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.2946612040201823, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.28388553195529515, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2893575032552083, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.29825931125217015, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.29947874281141496, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.29646894666883683, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.2888234456380208, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.2893507215711806, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.2895334031846788, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.2982872856987847, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2909189860026042, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:08<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1151\n",
      "Precision: 0.6611, Recall: 0.6453, F1-Score: 0.6489\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.49      0.54      6000\n",
      "           1       0.74      0.58      0.65      6000\n",
      "           2       0.67      0.75      0.71      6000\n",
      "           3       0.45      0.54      0.49      6000\n",
      "           4       0.78      0.76      0.77      6000\n",
      "           5       0.92      0.71      0.81      6000\n",
      "           6       0.43      0.50      0.46      6000\n",
      "           7       0.62      0.67      0.65      6000\n",
      "           8       0.64      0.74      0.69      6000\n",
      "           9       0.75      0.70      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.65     60000\n",
      "weighted avg       0.66      0.65      0.65     60000\n",
      "\n",
      "#Module 3 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:18<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0758\n",
      "Precision: 0.6678, Recall: 0.6579, F1-Score: 0.6591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.56      6000\n",
      "           1       0.73      0.62      0.67      6000\n",
      "           2       0.71      0.74      0.72      6000\n",
      "           3       0.50      0.53      0.51      6000\n",
      "           4       0.81      0.75      0.78      6000\n",
      "           5       0.90      0.75      0.82      6000\n",
      "           6       0.53      0.44      0.48      6000\n",
      "           7       0.53      0.74      0.62      6000\n",
      "           8       0.67      0.73      0.70      6000\n",
      "           9       0.74      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17779696080551471\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.29614851209852433, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.29929563734266496, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.289634280734592, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.28418901231553817, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2893791198730469, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.28532706366644967, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.28842459784613717, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.2856339348687066, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.2877159118652344, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.28831185234917533, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.28712209065755206, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.28909047444661456, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.291603512234158, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.2870042588975694, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.2942572699652778, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.28538428412543404, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.28655454847547746, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.28421274820963544, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.2954216003417969, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.2973497178819444, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.2994503445095486, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.2991566128200955, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.29937659369574654, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.28821733262803817, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2864702012803819, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:22<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1199\n",
      "Precision: 0.6625, Recall: 0.6441, F1-Score: 0.6487\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56      6000\n",
      "           1       0.73      0.59      0.65      6000\n",
      "           2       0.72      0.72      0.72      6000\n",
      "           3       0.45      0.56      0.50      6000\n",
      "           4       0.81      0.73      0.77      6000\n",
      "           5       0.92      0.70      0.80      6000\n",
      "           6       0.47      0.47      0.47      6000\n",
      "           7       0.54      0.72      0.62      6000\n",
      "           8       0.67      0.71      0.69      6000\n",
      "           9       0.75      0.70      0.72      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.65     60000\n",
      "weighted avg       0.66      0.64      0.65     60000\n",
      "\n",
      "#Module 4 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:33<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0837\n",
      "Precision: 0.6717, Recall: 0.6583, F1-Score: 0.6552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55      6000\n",
      "           1       0.76      0.59      0.66      6000\n",
      "           2       0.72      0.71      0.72      6000\n",
      "           3       0.53      0.49      0.51      6000\n",
      "           4       0.77      0.82      0.80      6000\n",
      "           5       0.90      0.79      0.84      6000\n",
      "           6       0.64      0.37      0.47      6000\n",
      "           7       0.51      0.75      0.61      6000\n",
      "           8       0.56      0.80      0.66      6000\n",
      "           9       0.73      0.74      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.1781176206796391\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.2961578369140625, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.2991977267795139, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.2888149685329861, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.29853185017903644, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2891917758517795, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.2878909640842014, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.29875818888346356, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.2883618672688802, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.29017215304904515, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.28716617160373265, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.28940412733289933, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.29607391357421875, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.2913585238986545, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.28457895914713544, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.28724628024631077, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.29150814480251735, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2862205505371094, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.28484005398220485, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.2944420708550347, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.28405719333224827, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.29602983262803817, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.2975955539279514, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.299477047390408, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.2969919840494792, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2935333251953125, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:37<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0893\n",
      "Precision: 0.6622, Recall: 0.6556, F1-Score: 0.6517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.47      0.55      6000\n",
      "           1       0.70      0.66      0.68      6000\n",
      "           2       0.74      0.70      0.72      6000\n",
      "           3       0.48      0.52      0.50      6000\n",
      "           4       0.68      0.87      0.76      6000\n",
      "           5       0.91      0.77      0.83      6000\n",
      "           6       0.56      0.38      0.45      6000\n",
      "           7       0.54      0.72      0.62      6000\n",
      "           8       0.62      0.75      0.68      6000\n",
      "           9       0.73      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.66      0.66      0.65     60000\n",
      "weighted avg       0.66      0.66      0.65     60000\n",
      "\n",
      "#Module 5 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:38<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0485\n",
      "Precision: 0.6713, Recall: 0.6681, F1-Score: 0.6677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      6000\n",
      "           1       0.70      0.68      0.69      6000\n",
      "           2       0.72      0.73      0.72      6000\n",
      "           3       0.55      0.47      0.51      6000\n",
      "           4       0.82      0.78      0.80      6000\n",
      "           5       0.89      0.79      0.84      6000\n",
      "           6       0.52      0.45      0.48      6000\n",
      "           7       0.58      0.72      0.64      6000\n",
      "           8       0.66      0.74      0.69      6000\n",
      "           9       0.74      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.67     60000\n",
      "weighted avg       0.67      0.67      0.67     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17718471515386922\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.29850006103515625, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.29838138156467015, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.28871409098307294, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.2837117513020833, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2891188727484809, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.2888043721516927, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.287994384765625, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.284659915500217, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.29072655571831596, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.28515709771050346, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.2880422804090712, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2876565721299913, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.2891286214192708, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.2867385016547309, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.29193878173828125, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.28527916802300346, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2915327284071181, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.28428946601019967, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.2886984083387587, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.28598064846462673, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.289359622531467, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.29881880018446183, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.28856150309244794, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.2854682074652778, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2942437065972222, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:37<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0924\n",
      "Precision: 0.6654, Recall: 0.6548, F1-Score: 0.6576\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.55      6000\n",
      "           1       0.73      0.63      0.67      6000\n",
      "           2       0.73      0.70      0.71      6000\n",
      "           3       0.54      0.46      0.49      6000\n",
      "           4       0.82      0.77      0.80      6000\n",
      "           5       0.91      0.76      0.83      6000\n",
      "           6       0.42      0.50      0.45      6000\n",
      "           7       0.63      0.67      0.65      6000\n",
      "           8       0.63      0.75      0.68      6000\n",
      "           9       0.72      0.74      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.66     60000\n",
      "weighted avg       0.67      0.65      0.66     60000\n",
      "\n",
      "#Module 6 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:54<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0669\n",
      "Precision: 0.6718, Recall: 0.6635, F1-Score: 0.6634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.56      6000\n",
      "           1       0.72      0.65      0.69      6000\n",
      "           2       0.72      0.72      0.72      6000\n",
      "           3       0.55      0.49      0.52      6000\n",
      "           4       0.78      0.80      0.79      6000\n",
      "           5       0.91      0.77      0.83      6000\n",
      "           6       0.56      0.44      0.49      6000\n",
      "           7       0.52      0.74      0.62      6000\n",
      "           8       0.64      0.75      0.69      6000\n",
      "           9       0.77      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17713987156562425\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.2963104248046875, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.2988082038031684, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.2900072733561198, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.29765743679470485, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.29172982109917533, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.2877489725748698, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.286895751953125, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.2871750725640191, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.29052310519748265, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.2867325676812066, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.28731748792860246, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2875908745659722, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.2919074164496528, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.2876196967230903, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.2953537835015191, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.2861955430772569, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2899742126464844, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.2975815667046441, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.2869436475965712, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.28595564100477433, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.2895541720920139, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.28384950425889754, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.29499223497178817, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.2852054172092014, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2998826768663194, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [06:53<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1278\n",
      "Precision: 0.6594, Recall: 0.6453, F1-Score: 0.6438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.49      0.55      6000\n",
      "           1       0.74      0.60      0.66      6000\n",
      "           2       0.68      0.72      0.70      6000\n",
      "           3       0.62      0.40      0.49      6000\n",
      "           4       0.73      0.80      0.77      6000\n",
      "           5       0.89      0.76      0.82      6000\n",
      "           6       0.45      0.48      0.46      6000\n",
      "           7       0.52      0.72      0.60      6000\n",
      "           8       0.56      0.80      0.66      6000\n",
      "           9       0.78      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.64     60000\n",
      "weighted avg       0.66      0.65      0.64     60000\n",
      "\n",
      "#Module 7 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:07<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0543\n",
      "Precision: 0.6752, Recall: 0.6632, F1-Score: 0.6652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56      6000\n",
      "           1       0.76      0.60      0.67      6000\n",
      "           2       0.72      0.74      0.73      6000\n",
      "           3       0.50      0.52      0.51      6000\n",
      "           4       0.83      0.77      0.80      6000\n",
      "           5       0.89      0.80      0.84      6000\n",
      "           6       0.50      0.47      0.48      6000\n",
      "           7       0.52      0.77      0.62      6000\n",
      "           8       0.71      0.69      0.70      6000\n",
      "           9       0.75      0.73      0.74      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.68      0.66      0.67     60000\n",
      "weighted avg       0.68      0.66      0.67     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17730122129409434\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.2963829040527344, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.2996181911892361, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.29089270697699654, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.2849498324924045, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2889476352267795, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.2965287102593316, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.28858057657877606, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.285774654812283, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.28667916191948783, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.28717676798502606, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.2873802185058594, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2895863850911458, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.2891320122612847, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.28809568617078996, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.29091474745008683, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.2851219177246094, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.289864010281033, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.2841818067762587, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.28620486789279515, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.298187255859375, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.28664610120985246, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.2997046576605903, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.2913712395562066, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.28383763631184894, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2978498670789931, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:01<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0957\n",
      "Precision: 0.6680, Recall: 0.6502, F1-Score: 0.6549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56      6000\n",
      "           1       0.76      0.56      0.65      6000\n",
      "           2       0.70      0.74      0.72      6000\n",
      "           3       0.46      0.53      0.50      6000\n",
      "           4       0.84      0.72      0.78      6000\n",
      "           5       0.89      0.79      0.84      6000\n",
      "           6       0.41      0.50      0.45      6000\n",
      "           7       0.59      0.72      0.65      6000\n",
      "           8       0.74      0.65      0.69      6000\n",
      "           9       0.70      0.76      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "#Module 8 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [08:44<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0986\n",
      "Precision: 0.6658, Recall: 0.6509, F1-Score: 0.6499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.55      6000\n",
      "           1       0.75      0.59      0.66      6000\n",
      "           2       0.73      0.69      0.71      6000\n",
      "           3       0.54      0.48      0.51      6000\n",
      "           4       0.74      0.83      0.78      6000\n",
      "           5       0.90      0.78      0.83      6000\n",
      "           6       0.57      0.42      0.48      6000\n",
      "           7       0.47      0.76      0.58      6000\n",
      "           8       0.58      0.78      0.67      6000\n",
      "           9       0.77      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17671732522147895\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.29765065511067706, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.29775746663411456, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.29148228963216144, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.2982398139105903, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2899971008300781, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.28607474433051217, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.2879803975423177, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.28435728285047746, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.2862718370225694, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.28827497694227433, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.2892405192057292, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2881435818142361, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.28807152642144096, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.2851456536187066, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.29405805799696183, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.2865108913845486, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.29224183824327254, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.284843020968967, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.28977203369140625, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.2856886121961806, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.2921553717719184, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.2838851081000434, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.28989240858289933, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.2910355461968316, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2921600341796875, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:51<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1423\n",
      "Precision: 0.6599, Recall: 0.6390, F1-Score: 0.6388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.46      0.54      6000\n",
      "           1       0.74      0.60      0.66      6000\n",
      "           2       0.75      0.65      0.69      6000\n",
      "           3       0.54      0.45      0.49      6000\n",
      "           4       0.71      0.83      0.76      6000\n",
      "           5       0.93      0.73      0.82      6000\n",
      "           6       0.51      0.45      0.48      6000\n",
      "           7       0.49      0.74      0.59      6000\n",
      "           8       0.52      0.81      0.64      6000\n",
      "           9       0.78      0.67      0.72      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.64     60000\n",
      "weighted avg       0.66      0.64      0.64     60000\n",
      "\n",
      "#Module 9 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:51<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1120\n",
      "Precision: 0.6573, Recall: 0.6471, F1-Score: 0.6480\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55      6000\n",
      "           1       0.74      0.61      0.67      6000\n",
      "           2       0.70      0.71      0.71      6000\n",
      "           3       0.53      0.46      0.49      6000\n",
      "           4       0.79      0.76      0.78      6000\n",
      "           5       0.90      0.75      0.82      6000\n",
      "           6       0.48      0.45      0.46      6000\n",
      "           7       0.50      0.72      0.59      6000\n",
      "           8       0.64      0.74      0.69      6000\n",
      "           9       0.73      0.72      0.72      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.65     60000\n",
      "weighted avg       0.66      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.17758466593970462\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.29509268866644967, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.2845628526475694, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.2893104553222656, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.29550806681315106, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.2935740152994792, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.28506554497612846, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.29185570610894096, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.2868262396918403, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.287056393093533, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.2877057393391927, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.2893113030327691, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.2871025933159722, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.29100079006618923, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.2856280008951823, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.29262034098307294, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.28555933634440106, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.2896982828776042, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.2974052429199219, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.2876519097222222, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.2978477478027344, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.28647952609592015, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.29129113091362846, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.28976694742838544, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.29936303032769096, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.2967088487413194, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [11:27<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1483\n",
      "Precision: 0.6476, Recall: 0.6360, F1-Score: 0.6384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55      6000\n",
      "           1       0.70      0.63      0.66      6000\n",
      "           2       0.68      0.73      0.70      6000\n",
      "           3       0.55      0.42      0.48      6000\n",
      "           4       0.75      0.77      0.76      6000\n",
      "           5       0.89      0.74      0.81      6000\n",
      "           6       0.36      0.49      0.41      6000\n",
      "           7       0.61      0.63      0.62      6000\n",
      "           8       0.67      0.69      0.68      6000\n",
      "           9       0.68      0.74      0.71      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.65      0.64      0.64     60000\n",
      "weighted avg       0.65      0.64      0.64     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "    \n",
    "    positive_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "    \n",
    "    all_samples = sampling_class(\n",
    "        train_dataloader, 200, 20, num_labels, False, 4, device=device\n",
    "    )\n",
    "    \n",
    "    print(\"after head prune\")\n",
    "    evaluate_model(model, model_config, test_dataloader)\n",
    "    \n",
    "    module = copy.deepcopy(model)\n",
    "    wr = WeightRemoverBert(model, p=0.9)\n",
    "    ci = ConcernIdentificationBert(model, p=0.6)\n",
    "    ti = TanglingIdentification(model, p=0.7)\n",
    "    \n",
    "    print(\"after removing weights\")\n",
    "    \n",
    "    eval_step = 5\n",
    "    for idx, batch in enumerate(all_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            wr.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    \n",
    "    print(\"after CI\")\n",
    "    \n",
    "    for idx, batch in enumerate(positive_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            ci.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    \n",
    "    print(\"after TI\")\n",
    "    \n",
    "    for idx, batch in enumerate(negative_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            ti.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    a, b = get_sparsity(module)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    result = evaluate_model(module, model_config, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f57b309-29c9-4dd4-af34-290cca6ede44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/Minwoo/LESN/Decompose/DecomposeTransformer/Models/Configs/classification/fabriceyhc/bert-base-uncased-yahoo_answers_topics exists.\n",
      "Loading the model.\n",
      "The model fabriceyhc/bert-base-uncased-yahoo_answers_topics is loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [16:54<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0044\n",
      "Precision: 0.6874, Recall: 0.6865, F1-Score: 0.6839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57      6000\n",
      "           1       0.74      0.66      0.69      6000\n",
      "           2       0.71      0.78      0.74      6000\n",
      "           3       0.54      0.53      0.53      6000\n",
      "           4       0.80      0.82      0.81      6000\n",
      "           5       0.90      0.84      0.87      6000\n",
      "           6       0.61      0.43      0.50      6000\n",
      "           7       0.62      0.73      0.67      6000\n",
      "           8       0.64      0.76      0.70      6000\n",
      "           9       0.75      0.75      0.75      6000\n",
      "\n",
      "    accuracy                           0.69     60000\n",
      "   macro avg       0.69      0.69      0.68     60000\n",
      "weighted avg       0.69      0.69      0.68     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.0044152938524882,\n",
       " 'precision': 0.6874371369976651,\n",
       " 'recall': 0.6865,\n",
       " 'f1_score': 0.6838781660446563,\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n           0       0.57      0.57      0.57      6000\\n           1       0.74      0.66      0.69      6000\\n           2       0.71      0.78      0.74      6000\\n           3       0.54      0.53      0.53      6000\\n           4       0.80      0.82      0.81      6000\\n           5       0.90      0.84      0.87      6000\\n           6       0.61      0.43      0.50      6000\\n           7       0.62      0.73      0.67      6000\\n           8       0.64      0.76      0.70      6000\\n           9       0.75      0.75      0.75      6000\\n\\n    accuracy                           0.69     60000\\n   macro avg       0.69      0.69      0.68     60000\\nweighted avg       0.69      0.69      0.68     60000\\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer, checkpoint = load_model(model_config)\n",
    "evaluate_model(model, model_config, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
