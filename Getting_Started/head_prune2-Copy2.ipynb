{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f1ee1c-0594-49b2-8b60-099cd6398872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d52d686-863f-4ef4-a172-1ae435fa8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5611dd8a-9d8c-42c6-8e2f-83d0cb321880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from utils.model_utils.evaluate import evaluate_model\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.decompose_utils.weight_remover import WeightRemoverBert\n",
    "from utils.decompose_utils.concern_identification import ConcernIdentificationBert\n",
    "from utils.decompose_utils.tangling_identification import TanglingIdentification\n",
    "from transformers import AutoConfig\n",
    "from utils.model_utils.save_module import save_module\n",
    "from datetime import datetime\n",
    "from utils.decompose_utils.concern_modularization import ConcernModularizationBert\n",
    "from utils.decompose_utils.sampling import sampling_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bec197-1db0-45eb-8ad2-eb0fe4cf425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils.evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82e28fa-64e5-4c4a-84e1-e1ba928d9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fabriceyhc/bert-base-uncased-yahoo_answers_topics\"\n",
    "task_type = \"classification\"\n",
    "architectures = \"bert\"\n",
    "dataset_name = \"Yahoo\"\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fda417-04ff-42a1-9ad8-c92a6a5e13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.model_utils.evaluate import get_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca83f3d1-f1a6-4639-a07c-9be575d4180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fcb878-31e0-41f3-8ff1-7220ca9451a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "model_config = ModelConfig(\n",
    "    model_name=model_name,\n",
    "    task_type=task_type,\n",
    "    dataset_name=dataset_name,\n",
    "    checkpoint=checkpoint,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc39d9e-bf4c-48c0-84e6-9d53cc98115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/Minwoo/LESN/Decompose/DecomposeTransformer/Models/Configs/classification/fabriceyhc/bert-base-uncased-yahoo_answers_topics exists.\n",
      "Loading the model.\n",
      "The model fabriceyhc/bert-base-uncased-yahoo_answers_topics is loaded.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, checkpoint = load_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030711ac-259e-4bbf-9b4d-47449daf22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset Yahoo\n",
      "Load cached dataset.\n",
      "The dataset Yahoo is loaded\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
    "        model_config, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6cf8e8-e578-494a-b5a0-b15faf349a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_indices(data):\n",
    "\n",
    "    data_np = np.array(data)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "\n",
    "    result = []\n",
    "\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "def get_sorted_indices_except_max(data):\n",
    "    data_np = np.array(data)\n",
    "    max_indices = np.argmin(data_np, axis=1)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)[::-1]\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        # 각 행의 최대값 인덱스를 제외\n",
    "        if col_indices[i] != max_indices[row_indices[i]]:\n",
    "            result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "ablating_head_num_in_CI = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e6ce30-935f-43da-b1aa-3f41067df563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_data = np.array([\n",
    "    [-0.07, 0.30, 0.07, -0.10, 0.03, -0.30, 0.13, 0.03, -0.27, -0.53, -0.20, -0.13],\n",
    "    [-0.20, -0.10, 0.00, 0.00, 0.03, 0.00, -0.20, -0.37, 0.03, 0.27, -0.47, -0.10],\n",
    "    [0.03, 0.17, 0.07, -0.23, 0.00, 0.10, -0.40, -0.13, -0.17, 0.10, -0.47, -0.03],\n",
    "    [-1.03, 0.13, 0.13, -0.23, -0.07, 0.07, 0.03, 0.03, -0.20, 0.13, -0.23, -0.27],\n",
    "    [-1.03, -0.33, -0.20, -0.40, 0.17, -0.10, 0.10, 0.10, -0.13, 0.03, -0.13, -0.13],\n",
    "    [-0.03, -0.13, 0.07, -0.03, 0.33, 0.00, 0.10, 0.03, -0.27, -0.23, -0.23, -0.17],\n",
    "    [-0.27, 0.03, -0.10, 0.00, -0.13, -0.33, -0.10, 0.40, 1.03, -0.10, 0.13, 0.23],\n",
    "    [-0.57, 0.07, 0.10, 0.03, 0.17, 0.70, 0.07, -0.07, -0.57, 0.07, -0.03, 0.10],\n",
    "    [-0.13, -0.37, -0.03, -0.17, -0.30, 0.03, -0.33, 0.20, 0.13, -0.43, -0.40, -0.50],\n",
    "    [-0.17, -0.17, 0.13, -0.27, -0.40, 1.77, -0.07, 0.03, 0.40, 0.13, 0.17, -0.30],\n",
    "    [0.60, -0.30, -0.03, 0.07, -0.03, 0.00, 0.53, -0.03, 0.43, -0.03, 0.13, 0.80],\n",
    "    [0.50, 0.17, -0.47, -0.10, -0.77, 0.10, 0.60, -0.73, -0.50, -0.03, -0.10, 1.10]\n",
    "])\n",
    "class_2_data = np.array([\n",
    "    [-0.10, -0.07, -0.27, -0.27, 0.27, -0.07, -0.17, -0.33, -0.50, -0.03, -0.17, 0.07],\n",
    "    [0.07, 0.03, -0.17, 0.10, -0.17, -0.23, -0.17, -0.23, -0.07, -0.33, -0.10, -0.27],\n",
    "    [-0.17, -0.13, 0.03, 0.10, 0.03, 0.00, -0.13, -0.20, 0.03, -0.07, -0.10, 0.00],\n",
    "    [0.20, 0.10, 0.07, -0.07, -0.20, -0.43, 0.17, -0.10, -0.37, -0.13, -0.27, -0.07],\n",
    "    [-0.17, -0.07, 0.10, -0.27, -0.20, 0.00, -0.03, -0.20, 0.00, 0.23, 0.00, 0.10],\n",
    "    [-0.10, 0.03, 0.07, 0.03, 0.10, -0.10, 0.17, -0.13, -0.23, 0.17, 0.17, -0.03],\n",
    "    [-0.17, -0.03, 0.13, 0.00, -0.03, -0.13, -0.03, 0.00, -0.47, -0.23, -0.03, 0.30],\n",
    "    [0.17, 0.03, 0.13, -0.07, -0.23, -0.13, 0.00, 0.00, 0.00, -0.13, 0.03, 0.07],\n",
    "    [-0.03, -0.13, -0.03, 0.03, -0.03, -0.10, -0.13, -0.27, -0.07, 0.00, 0.07, -0.03],\n",
    "    [-0.03, -0.70, 0.27, 0.23, 0.10, -1.30, -0.13, -0.10, -0.47, -0.17, 0.03, -0.23],\n",
    "    [0.00, -0.23, 0.13, 0.17, -0.07, 0.03, -0.57, -0.43, -0.43, 0.00, -0.03, -0.93],\n",
    "    [-1.53, 0.07, 0.17, 0.03, 0.03, -0.20, -0.60, -0.07, 0.37, 0.37, -0.23, -0.37]\n",
    "])\n",
    "class_3_data = np.array([\n",
    "    [0.23, 0.33, 0.03, -0.10, 0.07, -0.67, 0.17, 0.30, 0.27, -0.43, 0.00, 0.00],\n",
    "    [0.17, -0.03, 0.20, 0.07, 0.00, 0.00, 0.07, 0.07, 0.20, 0.10, -0.07, 0.07],\n",
    "    [-0.10, -0.10, 0.20, 0.03, -0.03, -0.07, 0.13, -0.03, -0.03, -0.17, 0.00, 0.10],\n",
    "    [-0.20, 0.00, -0.13, 0.00, 0.20, 0.10, -0.07, -0.13, -0.03, 0.07, -0.07, -0.20],\n",
    "    [-0.03, 0.10, -0.20, 0.20, 0.10, -0.27, -0.13, -0.07, -0.07, -0.07, -0.03, 0.03],\n",
    "    [-0.03, -0.13, -0.07, -0.10, 0.10, 0.03, -0.07, 0.00, 0.13, 0.00, -0.17, 0.00],\n",
    "    [0.03, -0.17, -0.07, -0.03, -0.03, -0.07, -0.07, -0.10, 0.00, -0.07, 0.03, 0.03],\n",
    "    [-0.23, -0.13, 0.00, 0.03, 0.07, -0.17, -0.07, 0.00, -0.03, -0.10, -0.20, 0.03],\n",
    "    [-0.03, -0.10, 0.13, -0.07, 0.03, -0.13, -0.10, 0.07, -0.10, 0.00, -0.33, -0.07],\n",
    "    [0.30, -0.03, -0.10, 0.13, 0.07, -0.20, -0.07, -0.40, -0.50, -0.03, -0.13, 0.13],\n",
    "    [0.20, 0.10, -0.20, 0.10, -0.23, 0.07, -1.07, 0.33, 0.27, 0.07, -0.03, -0.20],\n",
    "    [-0.67, 0.07, 0.00, 0.23, -0.23, 0.23, 0.33, -0.07, -0.17, 0.27, 0.07, -0.83]\n",
    "])\n",
    "class_4_data = np.array([\n",
    "    [0.33, 0.20, -0.07, -0.13, -0.10, 0.10, -0.03, 0.23, -0.17, -0.17, -0.30, 0.00],\n",
    "    [-0.07, -0.10, 0.33, 0.07, -0.10, -0.13, 0.03, 0.13, 0.03, 0.03, -0.27, 0.23],\n",
    "    [0.07, -0.43, 0.13, -0.07, 0.10, 0.17, 0.03, -0.13, -0.17, -0.10, 0.07, 0.23],\n",
    "    [0.50, -0.03, -0.03, 0.20, 0.00, 0.10, -0.17, -0.03, -0.17, -0.03, -0.20, 0.07],\n",
    "    [-0.17, -0.17, 0.23, -0.07, -0.13, 0.07, -0.23, -0.03, 0.10, 0.13, -0.17, -0.13],\n",
    "    [-0.07, -0.07, -0.17, 0.00, -0.17, -0.13, -0.07, 0.03, -0.17, 0.10, -0.03, 0.10],\n",
    "    [0.13, -0.03, -0.17, 0.00, -0.07, -0.07, 0.00, -0.27, 0.30, 0.10, -0.03, 0.07],\n",
    "    [0.10, -0.10, -0.10, -0.10, 0.03, -0.50, -0.03, 0.00, 0.10, -0.20, -0.23, 0.03],\n",
    "    [0.07, 0.33, -0.17, 0.10, -0.03, 0.13, 0.03, -0.07, -0.43, -0.13, 0.10, -0.10],\n",
    "    [0.20, 0.33, -0.17, 0.10, -0.23, -0.87, 0.13, -0.60, -0.07, 0.10, 0.50, -0.10],\n",
    "    [-0.70, 0.20, -0.03, 0.67, 0.27, 0.03, 1.47, 0.97, -0.07, -0.03, -0.07, 0.27],\n",
    "    [1.80, 0.30, -0.13, -0.13, 0.57, -0.20, 0.47, 0.03, 0.37, -0.40, 0.03, 0.93]\n",
    "])\n",
    "class_5_data = np.array([\n",
    "    [-0.20, -0.37, -0.10, 0.20, -0.07, 0.33, 0.17, -0.10, 0.03, 0.20, -0.03, 0.03],\n",
    "    [0.03, 0.03, 0.30, -0.10, -0.23, -0.30, 0.00, -0.27, -0.17, -0.10, 0.17, -0.13],\n",
    "    [-0.33, 0.03, 0.03, -0.13, -0.10, -0.33, -0.10, -0.17, -0.17, -0.27, 0.07, 0.07],\n",
    "    [-0.43, -0.10, -0.10, -0.17, -0.13, 0.03, 0.00, -0.10, 0.10, -0.03, 0.13, -0.23],\n",
    "    [-0.33, 0.13, 0.13, -0.07, 0.10, 0.07, -0.07, -0.07, -0.07, 0.20, -0.03, 0.10],\n",
    "    [0.10, 0.00, -0.17, -0.13, 0.03, -0.10, -0.03, 0.00, 0.07, 0.03, -0.10, -0.03],\n",
    "    [0.07, 0.07, -0.17, 0.00, -0.13, 0.17, 0.13, -0.07, 0.20, 0.03, -0.10, 0.00],\n",
    "    [-0.03, -0.23, 0.13, -0.03, -0.13, -0.37, -0.10, 0.13, -0.07, -0.10, 0.20, -0.20],\n",
    "    [-0.03, 0.07, 0.00, 0.03, 0.03, -0.03, -0.20, 0.00, 0.10, 0.03, 0.23, 0.27],\n",
    "    [0.07, 0.57, 0.00, -0.20, -0.13, -1.30, -0.13, 0.20, 0.30, 0.00, -0.53, -0.33],\n",
    "    [0.63, 0.13, -0.30, -0.63, -0.53, 0.03, -0.67, -0.90, 0.20, 0.17, 0.00, -0.07],\n",
    "    [-0.57, -0.23, -0.20, 0.30, 0.03, 0.03, -0.73, -0.70, -0.37, 0.60, -0.53, 1.37]\n",
    "])\n",
    "class_6_data = np.array([\n",
    "    [0.10, -0.17, 0.03, 0.00, 0.10, 0.27, -0.10, -0.07, 0.27, 0.23, 0.07, -0.03],\n",
    "    [0.00, 0.00, 0.03, -0.03, 0.00, -0.03, -0.03, -0.07, 0.13, 0.03, -0.10, -0.23],\n",
    "    [0.10, 0.07, 0.07, 0.03, 0.07, -0.03, -0.10, 0.07, 0.00, 0.03, 0.00, -0.17],\n",
    "    [-0.07, 0.00, -0.03, -0.07, 0.03, 0.07, 0.00, -0.10, -0.13, -0.07, -0.07, 0.00],\n",
    "    [0.03, 0.03, 0.13, 0.03, 0.07, 0.03, -0.07, -0.07, 0.00, 0.00, -0.10, 0.03],\n",
    "    [-0.03, 0.00, 0.00, 0.00, 0.00, -0.07, -0.03, -0.07, -0.07, -0.10, 0.03, -0.03],\n",
    "    [0.00, -0.10, 0.00, 0.00, 0.00, -0.03, -0.03, -0.03, -0.17, 0.00, 0.00, 0.03],\n",
    "    [0.07, -0.07, -0.07, -0.03, -0.10, -0.20, -0.03, 0.03, -0.03, 0.03, 0.03, 0.00],\n",
    "    [-0.07, 0.03, -0.03, -0.03, -0.07, -0.03, -0.03, -0.03, -0.03, -0.03, -0.03, 0.07],\n",
    "    [-0.10, -0.27, 0.03, 0.03, 0.00, -0.60, -0.07, 0.13, -0.10, 0.00, -0.03, 0.13],\n",
    "    [0.00, -0.07, 0.13, 0.03, 0.13, -0.03, -1.00, -0.13, -0.10, 0.00, 0.00, 0.03],\n",
    "    [-0.10, -0.13, 0.03, -0.07, -0.03, -0.03, 0.13, 0.03, 0.07, 0.03, 0.07, -1.00]\n",
    "])\n",
    "class_7_data = np.array([\n",
    "    [0.20, -0.23, -0.17, -0.27, 0.00, -0.33, 0.07, -0.30, -0.30, -0.17, 0.13, -0.50],\n",
    "    [-0.13, -0.13, -0.30, 0.17, 0.17, -0.03, -0.07, 0.13, 0.00, -0.10, -0.13, 0.23],\n",
    "    [-0.30, 0.03, -0.13, 0.13, -0.07, -0.13, -0.10, 0.23, -0.07, -0.20, -0.30, 0.07],\n",
    "    [-0.13, 0.30, -0.20, 0.03, 0.23, -0.27, -0.17, 0.07, -0.53, -0.07, -0.17, -0.03],\n",
    "    [-0.43, -0.07, -0.03, -0.13, -0.20, -0.03, -0.03, -0.10, 0.07, 0.27, -0.23, 0.07],\n",
    "    [0.03, 0.03, -0.10, -0.10, -0.17, -0.10, -0.03, -0.30, 0.10, 0.00, -0.23, -0.20],\n",
    "    [-0.07, -0.23, -0.10, -0.10, 0.07, -0.27, -0.10, 0.03, -0.30, 0.10, 0.13, 0.07],\n",
    "    [-0.47, -0.07, -0.07, -0.13, -0.13, -0.13, -0.13, -0.20, 0.13, -0.07, 0.03, -0.13],\n",
    "    [-0.07, -0.03, -0.03, -0.03, -0.07, 0.00, -0.07, 0.23, -0.17, -0.57, -0.20, -0.67],\n",
    "    [0.00, 0.07, -0.20, -0.03, 0.03, 0.60, 0.17, -0.17, -0.77, -0.03, -0.07, 0.53],\n",
    "    [0.03, -0.37, 0.03, -0.30, 0.60, -0.30, 0.53, 0.77, -0.03, -0.20, 0.03, -0.10],\n",
    "    [0.17, 0.13, 0.13, 0.50, -0.27, -0.27, 0.97, 0.90, 0.03, -0.13, 0.40, -2.33]\n",
    "])\n",
    "class_8_data = np.array([\n",
    "    [0.00, -0.20, -0.23, -0.03, -0.17, 0.27, -0.37, -0.03, -0.17, 0.03, -0.40, -0.03],\n",
    "    [-0.53, -0.07, 0.03, -0.07, -0.23, -0.23, -0.20, 0.03, -0.10, 0.10, -0.10, 0.33],\n",
    "    [-0.17, -0.07, -0.10, -0.07, -0.03, -0.07, 0.33, 0.13, 0.20, -0.07, 0.17, -0.13],\n",
    "    [0.33, -0.10, 0.03, 0.03, -0.17, -0.33, 0.03, 0.00, 0.30, -0.23, -0.17, -0.17],\n",
    "    [0.27, 0.00, 0.43, -0.30, -0.33, 0.07, -0.13, -0.23, 0.00, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.23, -0.20, 0.13, -0.20, -0.13, 0.03, -0.10, 0.07, 0.00, -0.23, 0.03],\n",
    "    [-0.20, -0.03, 0.20, 0.00, 0.03, 0.07, -0.13, -0.13, 0.13, -0.17, 0.00, 0.03],\n",
    "    [-0.10, -0.20, -0.17, 0.13, -0.13, 0.20, -0.07, -0.13, 0.17, 0.03, 0.20, -0.07],\n",
    "    [-0.03, 0.10, 0.10, 0.00, -0.03, -0.03, 0.13, 0.07, -0.17, 0.10, 0.23, 0.10],\n",
    "    [0.03, 0.13, 0.00, 0.03, 0.20, 0.60, -0.10, 0.37, 0.57, 0.13, -0.40, 0.03],\n",
    "    [-0.23, 0.27, -0.30, -0.27, 0.93, 0.10, -0.60, 0.07, 0.00, 0.03, -0.13, 0.00],\n",
    "    [-0.50, -0.07, -0.13, 0.00, 0.40, 0.20, -0.60, 0.27, -0.13, -0.30, 0.00, 1.37]\n",
    "])\n",
    "class_9_data = np.array([\n",
    "    [-0.40, -0.27, -0.13, -0.07, 0.00, -0.73, -0.37, -0.27, 0.20, -0.53, -0.10, -0.20],\n",
    "    [0.07, 0.00, 0.07, -0.30, -0.10, -0.13, -0.03, 0.17, -0.30, -0.03, -0.27, -0.07],\n",
    "    [-0.13, 0.13, -0.10, 0.00, -0.03, -0.23, 0.00, -0.33, -0.10, -0.33, -0.07, -0.07],\n",
    "    [0.00, -0.17, -0.10, 0.03, -0.03, -0.07, 0.07, -0.20, 0.13, -0.13, -0.20, -0.07],\n",
    "    [0.37, 0.07, -0.13, 0.10, -0.20, -0.17, 0.07, 0.00, -0.10, -0.20, -0.03, -0.10],\n",
    "    [-0.13, -0.07, -0.13, -0.03, -0.17, -0.03, -0.17, 0.00, -0.17, -0.07, -0.13, 0.00],\n",
    "    [0.20, -0.27, -0.27, -0.07, -0.07, -0.17, -0.10, -0.27, -0.43, 0.07, -0.10, -0.07],\n",
    "    [0.13, -0.07, -0.17, -0.07, -0.20, -0.27, 0.00, 0.03, -0.03, -0.20, -0.13, -0.27],\n",
    "    [-0.07, 0.07, -0.13, -0.20, 0.03, -0.17, -0.20, -0.27, -0.27, 0.23, 0.37, -0.07],\n",
    "    [-0.10, -0.03, -0.27, -0.13, -0.07, -0.77, 0.03, 0.03, 0.00, -0.27, -0.13, -0.20],\n",
    "    [-0.10, -0.30, -0.23, -0.97, -0.93, 0.00, 0.50, -0.33, -0.03, -0.07, -0.10, 0.00],\n",
    "    [0.40, -0.20, 0.33, -0.43, -0.13, -0.83, -0.93, 0.40, -0.20, 0.07, 0.00, -1.13]\n",
    "])\n",
    "class_10_data = np.array([\n",
    "    [-0.07, 0.07, 0.07, 0.13, 0.00, 0.27, -0.07, 0.10, 0.23, 0.63, -0.07, 0.00],\n",
    "    [-0.07, -0.07, -0.37, -0.03, -0.13, -0.03, -0.03, -0.13, 0.00, -0.17, -0.07, 0.00],\n",
    "    [-0.13, 0.20, -0.10, -0.10, 0.10, 0.20, -0.10, 0.07, 0.00, -0.03, 0.20, -0.20],\n",
    "    [0.03, 0.07, -0.07, 0.10, 0.00, 0.07, -0.10, 0.03, 0.17, 0.00, -0.03, 0.17],\n",
    "    [-0.10, -0.07, -0.20, 0.13, 0.03, -0.10, 0.03, -0.03, -0.13, -0.13, 0.00, 0.00],\n",
    "    [0.10, -0.03, -0.07, -0.03, -0.07, 0.00, 0.00, 0.03, -0.03, 0.07, 0.13, -0.13],\n",
    "    [0.33, -0.10, 0.07, 0.07, -0.10, 0.17, 0.07, -0.03, -0.07, -0.03, 0.03, 0.07],\n",
    "    [0.63, 0.07, 0.00, -0.07, 0.20, -0.27, -0.07, -0.03, 0.07, 0.17, -0.10, -0.03],\n",
    "    [-0.07, 0.13, 0.10, 0.00, -0.03, 0.00, 0.03, -0.03, 0.40, 0.33, 0.33, 0.20],\n",
    "    [-0.20, -0.33, 0.30, 0.07, 0.23, -0.60, 0.00, 0.13, 0.37, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.03, 0.37, 0.20, -0.37, -0.07, 0.90, -1.03, -0.03, 0.00, 0.07, 0.03],\n",
    "    [-0.43, -0.17, -0.03, -0.07, -0.17, 0.23, -0.13, -0.37, 0.30, -0.07, 0.33, 0.33]\n",
    "])\n",
    "\n",
    "class_1_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, -0.09, -0.03, -0.06, -0.04, -0.02, -0.09, -0.06, 0.00, 0.00, -0.03, -0.03],\n",
    "    [-0.06, -0.05, -0.03, -0.06, -0.05, -0.09, -0.03, -0.04, -0.03, -0.02, -0.01, 0.05],\n",
    "    [-0.08, -0.06, -0.01, -0.02, 0.00, -0.06, 0.04, -0.04, -0.01, -0.06, 0.03, -0.06],\n",
    "    [0.13, -0.03, -0.07, -0.01, 0.02, -0.03, 0.01, -0.02, -0.02, -0.02, -0.03, -0.03],\n",
    "    [0.07, 0.06, 0.07, -0.04, -0.11, -0.03, -0.04, -0.02, -0.02, 0.00, 0.00, 0.00],\n",
    "    [-0.01, -0.02, -0.05, -0.02, -0.05, -0.08, -0.07, 0.01, 0.00, 0.01, -0.04, 0.01],\n",
    "    [-0.01, -0.05, -0.06, 0.00, 0.00, 0.01, -0.03, -0.12, -0.15, -0.03, -0.03, 0.04],\n",
    "    [0.06, -0.04, -0.05, -0.03, -0.07, -0.22, -0.01, -0.01, 0.13, -0.04, 0.00, -0.02],\n",
    "    [-0.02, 0.09, -0.05, -0.02, 0.03, -0.02, 0.05, -0.05, -0.12, 0.01, 0.10, 0.01],\n",
    "    [0.04, -0.06, -0.01, 0.05, 0.01, -0.64, -0.05, -0.03, 0.00, -0.05, -0.09, 0.00],\n",
    "    [-0.09, 0.05, -0.06, -0.06, 0.03, -0.01, -0.22, -0.02, -0.07, -0.03, -0.03, -0.22],\n",
    "    [-0.20, 0.02, 0.05, 0.02, 0.09, -0.02, -0.25, 0.05, 0.05, -0.03, 0.05, -0.30]\n",
    "]   \n",
    ")\n",
    "class_2_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, -0.02, -0.01, 0.05, -0.03, 0.02, -0.01, 0.03, 0.10, -0.04, 0.01, -0.04],\n",
    "    [-0.01, 0.02, 0.01, -0.02, 0.01, 0.00, 0.00, 0.08, 0.00, 0.04, -0.01, 0.05],\n",
    "    [0.00, -0.01, 0.01, 0.01, 0.00, -0.02, 0.01, 0.03, 0.00, -0.01, 0.02, 0.02],\n",
    "    [-0.04, -0.01, -0.02, 0.01, -0.01, 0.03, -0.02, 0.00, 0.04, -0.01, -0.01, 0.00],\n",
    "    [0.03, 0.00, -0.01, 0.03, 0.00, 0.03, 0.04, 0.02, 0.00, 0.00, -0.02, 0.01],\n",
    "    [0.00, -0.01, -0.02, -0.02, -0.02, -0.01, 0.00, 0.01, -0.02, -0.02, -0.01, 0.00],\n",
    "    [0.03, -0.01, 0.00, 0.00, -0.01, 0.01, 0.01, 0.00, 0.05, 0.00, 0.01, -0.01],\n",
    "    [-0.01, -0.01, -0.02, 0.02, -0.01, 0.00, 0.02, 0.00, -0.01, -0.01, 0.02, -0.01],\n",
    "    [0.01, 0.01, 0.02, 0.00, 0.02, 0.01, 0.01, 0.02, -0.02, 0.01, -0.01, 0.01],\n",
    "    [0.00, 0.04, -0.01, 0.01, -0.02, 0.12, 0.03, 0.00, 0.08, 0.03, 0.00, 0.04],\n",
    "    [0.01, 0.04, -0.02, -0.04, -0.01, 0.02, 0.06, 0.06, 0.04, 0.00, 0.00, 0.13],\n",
    "    [0.21, 0.03, -0.05, -0.01, 0.02, 0.02, 0.05, 0.02, -0.05, -0.04, 0.02, 0.05]\n",
    "]\n",
    ")\n",
    "class_3_neg_acc = np.array(\n",
    "[\n",
    "    [-0.05, -0.04, -0.02, 0.00, 0.04, 0.12, -0.01, -0.04, -0.02, 0.08, -0.02, -0.02],\n",
    "    [-0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02, 0.00, -0.01, -0.01, 0.02],\n",
    "    [0.02, 0.03, -0.02, 0.02, 0.01, 0.02, -0.01, 0.02, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.02, 0.03, 0.05, 0.02, -0.05, -0.04, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02],\n",
    "    [0.03, -0.02, 0.04, -0.04, -0.01, 0.02, 0.03, 0.03, 0.00, 0.00, 0.02, 0.03],\n",
    "    [0.00, 0.00, 0.00, 0.04, -0.03, 0.02, 0.02, -0.01, 0.01, 0.06, 0.02, -0.02],\n",
    "    [0.04, 0.06, 0.04, 0.01, 0.01, 0.07, -0.03, 0.02, 0.02, 0.05, 0.04, 0.05],\n",
    "    [0.02, 0.02, 0.02, 0.00, -0.01, 0.07, 0.00, -0.01, 0.02, -0.02, 0.06, -0.01],\n",
    "    [0.00, 0.03, 0.02, 0.02, 0.02, 0.01, -0.02, -0.01, 0.01, 0.06, 0.12, 0.00],\n",
    "    [-0.05, 0.03, 0.03, -0.05, 0.00, 0.11, 0.03, 0.05, 0.10, 0.04, 0.01, -0.03],\n",
    "    [-0.04, -0.01, 0.07, -0.03, 0.07, 0.00, 0.28, -0.10, 0.00, 0.00, 0.01, 0.06],\n",
    "    [0.19, -0.01, -0.01, -0.04, 0.07, -0.07, -0.10, 0.04, 0.05, -0.04, 0.00, 0.30]\n",
    "]\n",
    ")\n",
    "class_4_neg_acc = np.array(\n",
    "[\n",
    "    [-0.04, -0.03, 0.01, 0.02, 0.03, -0.09, -0.05, 0.00, 0.02, -0.03, -0.01, 0.02],\n",
    "    [0.03, 0.02, 0.02, -0.01, -0.01, 0.00, 0.00, -0.04, -0.02, 0.01, -0.01, -0.06],\n",
    "    [-0.04, 0.00, 0.03, 0.00, 0.00, -0.01, -0.01, 0.01, 0.00, -0.04, -0.04, 0.01],\n",
    "    [-0.09, 0.02, -0.03, -0.03, -0.02, -0.01, 0.02, 0.00, 0.01, -0.03, -0.04, -0.02],\n",
    "    [-0.09, -0.01, -0.01, -0.02, -0.01, -0.03, 0.02, -0.05, -0.02, -0.02, -0.02, -0.03],\n",
    "    [0.01, -0.01, 0.00, -0.01, 0.02, 0.00, 0.02, -0.01, 0.02, 0.01, -0.04, -0.02],\n",
    "    [-0.03, -0.04, -0.02, -0.01, 0.00, -0.03, 0.00, 0.05, -0.01, -0.04, 0.00, 0.01],\n",
    "    [0.00, 0.02, 0.02, 0.00, 0.00, 0.05, -0.02, -0.01, -0.03, 0.00, 0.00, -0.03],\n",
    "    [-0.01, -0.03, 0.03, -0.04, -0.03, -0.01, -0.03, 0.03, 0.08, 0.02, 0.00, 0.03],\n",
    "    [-0.03, -0.06, 0.03, -0.03, 0.04, 0.14, -0.02, 0.06, -0.05, -0.04, -0.06, 0.05],\n",
    "    [0.18, -0.08, 0.02, -0.16, -0.06, 0.00, -0.21, -0.20, 0.03, 0.01, 0.01, -0.02],\n",
    "    [-0.31, -0.06, 0.03, 0.05, -0.09, -0.02, -0.16, -0.01, -0.07, 0.08, 0.02, -0.19]\n",
    "]  \n",
    ")\n",
    "class_5_neg_acc = np.array(\n",
    "[\n",
    "    [0.03, 0.04, 0.03, -0.02, -0.01, -0.04, 0.00, 0.00, -0.03, 0.01, -0.01, -0.01],\n",
    "    [-0.01, 0.00, -0.02, 0.03, 0.00, 0.00, 0.00, 0.02, 0.01, 0.01, -0.02, 0.02],\n",
    "    [0.02, 0.01, -0.01, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, 0.00, 0.00],\n",
    "    [0.02, 0.01, 0.01, 0.02, -0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02],\n",
    "    [0.03, -0.01, 0.01, 0.00, 0.02, 0.02, -0.01, 0.01, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.00, 0.01, 0.00, 0.01, -0.01, -0.01, 0.02, 0.02, -0.01, 0.00, 0.01, 0.00],\n",
    "    [0.00, -0.01, 0.01, 0.00, 0.00, -0.02, 0.00, 0.01, 0.01, -0.01, 0.02, 0.01],\n",
    "    [-0.01, 0.00, 0.00, 0.00, 0.02, 0.03, 0.01, 0.01, -0.03, 0.00, -0.02, 0.02],\n",
    "    [0.00, 0.00, -0.01, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, -0.01, 0.00, -0.05],\n",
    "    [0.03, -0.06, -0.01, -0.01, 0.02, 0.15, 0.00, -0.01, -0.04, 0.00, 0.05, 0.03],\n",
    "    [-0.07, 0.01, 0.01, 0.05, 0.03, 0.00, 0.13, 0.09, 0.00, 0.00, -0.01, 0.01],\n",
    "    [0.03, 0.01, 0.01, -0.03, 0.00, 0.00, 0.07, 0.07, 0.02, -0.03, 0.04, -0.13]\n",
    "] \n",
    ")\n",
    "class_6_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, 0.02, -0.02, 0.00, 0.01, -0.04, 0.02, 0.02, -0.02, -0.02, -0.02, 0.01],\n",
    "    [0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, 0.00, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.01, 0.00, 0.00, 0.01, 0.00, -0.02, 0.00, 0.00, 0.01, 0.01],\n",
    "    [0.00, 0.01, 0.01, 0.00, 0.01, -0.01, 0.01, 0.01, 0.02, -0.01, 0.02, -0.01],\n",
    "    [-0.03, 0.00, -0.02, 0.03, 0.01, 0.01, 0.01, 0.00, 0.02, 0.01, 0.00, 0.02],\n",
    "    [0.01, 0.01, 0.00, 0.01, 0.00, 0.01, 0.01, 0.02, -0.02, 0.01, 0.00, 0.01],\n",
    "    [0.02, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02, 0.00, 0.06, 0.01, 0.01, 0.01],\n",
    "    [0.01, 0.01, 0.00, 0.02, 0.02, 0.02, 0.01, 0.00, -0.01, 0.01, -0.01, 0.01],\n",
    "    [0.01, 0.00, 0.01, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.03, -0.01],\n",
    "    [0.02, 0.03, 0.00, 0.01, -0.01, 0.08, 0.01, 0.00, 0.06, 0.01, 0.01, -0.02],\n",
    "    [0.02, 0.00, 0.00, 0.01, -0.04, 0.01, 0.16, 0.01, 0.03, 0.01, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.03, 0.00, -0.02, -0.01, 0.00, 0.02, 0.00, 0.02, 0.00, 0.14]\n",
    "]  \n",
    ")\n",
    "class_7_neg_acc = np.array(\n",
    "[\n",
    "    [-0.02, 0.03, 0.01, 0.01, 0.00, 0.04, 0.00, 0.04, 0.02, 0.06, -0.02, 0.01],\n",
    "    [-0.01, -0.03, 0.03, 0.01, -0.02, -0.01, -0.01, 0.01, 0.00, 0.00, -0.02, -0.03],\n",
    "    [0.01, 0.03, 0.02, -0.01, -0.01, -0.02, 0.01, 0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "    [0.03, -0.02, 0.01, -0.02, -0.01, -0.02, 0.00, 0.00, 0.01, 0.01, 0.02, -0.02],\n",
    "    [0.03, 0.00, 0.01, 0.00, -0.01, -0.01, -0.01, -0.01, -0.01, 0.00, 0.00, -0.03],\n",
    "    [-0.04, -0.02, 0.00, 0.00, 0.00, -0.03, 0.00, -0.02, -0.02, -0.02, -0.01, -0.01],\n",
    "    [0.02, 0.01, 0.00, 0.00, -0.03, 0.04, 0.01, -0.03, 0.00, 0.01, -0.01, 0.00],\n",
    "    [0.05, -0.02, 0.00, 0.00, 0.03, 0.00, -0.01, 0.01, -0.04, -0.01, -0.01, 0.00],\n",
    "    [-0.01, -0.02, -0.02, 0.00, -0.01, -0.01, -0.01, -0.07, 0.00, 0.03, 0.02, 0.04],\n",
    "    [-0.01, 0.00, 0.02, 0.00, -0.01, -0.11, -0.03, 0.07, 0.10, -0.01, -0.02, -0.09],\n",
    "    [0.00, 0.04, -0.02, -0.03, -0.05, 0.00, -0.03, -0.14, 0.02, 0.03, -0.01, -0.02],\n",
    "    [-0.08, -0.03, -0.01, -0.06, 0.01, 0.03, -0.15, -0.11, 0.00, 0.00, -0.06, 0.17]\n",
    "]\n",
    ")\n",
    "class_8_neg_acc = np.array(\n",
    "[\n",
    "    [0.02, -0.02, -0.02, -0.05, 0.01, -0.10, -0.02, -0.03, -0.01, -0.10, -0.01, 0.00],\n",
    "    [0.02, 0.00, -0.04, -0.02, 0.00, -0.02, 0.00, -0.08, 0.00, -0.06, -0.01, -0.04],\n",
    "    [-0.04, 0.00, 0.00, 0.01, 0.00, 0.03, -0.09, -0.06, -0.06, -0.06, -0.13, 0.00],\n",
    "    [-0.13, -0.03, -0.02, -0.02, 0.02, 0.02, -0.05, -0.05, -0.09, -0.02, -0.09, 0.02],\n",
    "    [-0.17, 0.00, -0.10, 0.04, 0.04, -0.03, -0.01, -0.03, 0.00, -0.01, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.02, 0.00, 0.01, 0.01, 0.00, -0.04, -0.03, -0.03, 0.03, -0.03],\n",
    "    [-0.02, -0.05, -0.03, -0.02, -0.05, -0.10, -0.01, -0.03, -0.03, -0.01, -0.03, -0.02],\n",
    "    [-0.01, -0.03, -0.02, -0.03, -0.02, -0.20, -0.04, 0.01, -0.04, 0.01, -0.05, -0.02],\n",
    "    [-0.01, -0.06, -0.03, 0.00, -0.05, -0.02, -0.07, 0.01, -0.02, -0.06, -0.05, -0.06],\n",
    "    [-0.02, -0.09, -0.03, -0.03, -0.05, -0.35, 0.03, -0.14, -0.15, -0.08, 0.04, -0.07],\n",
    "    [0.03, -0.12, 0.00, 0.04, -0.36, -0.02, 0.10, -0.05, -0.03, -0.02, 0.00, 0.00],\n",
    "    [0.03, -0.02, 0.04, 0.02, -0.15, -0.07, 0.14, -0.08, 0.01, 0.12, 0.01, -0.33]\n",
    "]\n",
    ")\n",
    "class_9_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, 0.03, 0.00, -0.04, 0.01, 0.10, 0.10, 0.02, -0.08, 0.06, -0.02, -0.01],\n",
    "    [-0.03, -0.02, 0.00, 0.04, -0.04, 0.00, -0.02, -0.02, 0.03, 0.01, -0.02, -0.02],\n",
    "    [0.00, -0.01, -0.02, -0.03, 0.02, 0.01, -0.02, 0.01, 0.00, 0.02, -0.01, -0.02],\n",
    "    [-0.03, 0.02, 0.00, 0.03, 0.00, 0.01, -0.01, 0.00, -0.03, 0.00, 0.01, -0.04],\n",
    "    [-0.06, -0.04, 0.01, -0.08, 0.02, -0.03, -0.06, -0.03, -0.01, 0.02, -0.05, -0.01],\n",
    "    [-0.02, -0.03, 0.01, -0.03, 0.02, -0.02, 0.00, -0.01, 0.02, -0.05, -0.03, 0.00],\n",
    "    [-0.04, 0.00, 0.00, -0.01, 0.00, 0.01, 0.01, 0.04, 0.05, -0.01, 0.00, 0.01],\n",
    "    [-0.07, -0.01, 0.01, -0.01, -0.02, 0.03, -0.02, -0.02, -0.01, 0.00, -0.02, 0.01],\n",
    "    [-0.01, -0.02, 0.00, 0.01, -0.03, 0.00, -0.02, 0.03, 0.06, -0.06, -0.16, 0.00],\n",
    "    [0.01, 0.00, 0.02, 0.04, -0.02, 0.11, -0.04, -0.01, -0.07, 0.04, 0.00, 0.03],\n",
    "    [-0.02, 0.02, 0.01, 0.15, 0.17, -0.01, -0.12, 0.06, 0.00, -0.01, 0.04, -0.02],\n",
    "    [-0.11, 0.01, -0.12, 0.07, 0.00, 0.12, 0.25, -0.10, 0.00, -0.04, -0.05, 0.31]\n",
    "]\n",
    ")\n",
    "class_10_neg_acc = np.array(\n",
    "[\n",
    "    [0.01, 0.05, -0.01, 0.03, 0.01, -0.06, 0.01, -0.02, 0.01, -0.09, 0.03, 0.00],\n",
    "    [0.03, 0.01, 0.08, 0.03, 0.04, -0.02, 0.02, 0.01, 0.03, 0.02, -0.02, 0.03],\n",
    "    [0.00, 0.00, 0.03, 0.01, 0.02, -0.02, 0.04, 0.01, 0.02, 0.00, 0.02, 0.05],\n",
    "    [0.02, 0.04, 0.03, 0.00, 0.05, 0.00, 0.05, 0.00, 0.01, 0.01, 0.03, -0.01],\n",
    "    [-0.02, 0.00, 0.04, 0.00, 0.01, 0.01, 0.00, 0.01, 0.01, 0.05, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.02, 0.01, 0.04, 0.04, 0.01, 0.00, 0.01, 0.03, 0.00, 0.02],\n",
    "    [0.00, 0.01, 0.00, 0.02, 0.04, -0.03, 0.00, 0.04, 0.05, 0.03, 0.01, 0.01],\n",
    "    [-0.06, 0.00, 0.02, 0.01, 0.00, 0.11, 0.02, 0.01, 0.00, 0.01, 0.02, 0.00],\n",
    "    [0.01, 0.04, 0.02, 0.01, 0.02, 0.01, 0.00, 0.03, -0.01, -0.06, 0.00, -0.04],\n",
    "    [0.04, 0.14, -0.03, 0.01, 0.02, 0.10, 0.02, -0.02, -0.02, 0.04, 0.00, 0.05],\n",
    "    [0.04, -0.02, -0.03, -0.01, 0.21, 0.00, -0.13, 0.23, 0.02, 0.01, 0.00, 0.03],\n",
    "    [0.12, 0.03, 0.01, 0.02, 0.02, -0.03, 0.10, 0.08, -0.03, 0.03, -0.01, -0.07]\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "923bb456-d3ab-4511-b101-bb88705462f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered head num:  20\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  18\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  18\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n"
     ]
    }
   ],
   "source": [
    "correct_num = []\n",
    "correct_num_neg = []\n",
    "for r in range(20, 20 + 1): # 20개 되살림\n",
    "    print('recovered head num: ', r)\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 1번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_1_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_1_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_1 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_1 = model_1.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_1.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 2번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_2_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_2_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_2 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_2 = model_2.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_2.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 3번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_3_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_3_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_3 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_3 = model_3.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_3.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 4번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_4_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_4_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_4 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_4 = model_4.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_4.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 5번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_5_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_5_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_5 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_5 = model_5.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_5.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #6번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_6_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_6_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_6 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_6 = model_6.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_6.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #7번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_7_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_7_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_7 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_7 = model_7.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_7.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #8번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_8_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_8_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_8 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_8 = model_8.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_8.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #9번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_9_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_9_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_9 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_9 = model_9.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_9.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #10번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_10_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_10_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_10 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_10 = model_10.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_10.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe29a5e-a979-43b1-9011-ebd0fd2bf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2c3cbf-3f47-4212-b73f-555594d67232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Module 0 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [11:53<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0719\n",
      "Precision: 0.6671, Recall: 0.6588, F1-Score: 0.6594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      6000\n",
      "           1       0.74      0.61      0.67      6000\n",
      "           2       0.70      0.73      0.72      6000\n",
      "           3       0.52      0.49      0.51      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.88      0.78      0.83      6000\n",
      "           6       0.53      0.44      0.48      6000\n",
      "           7       0.52      0.74      0.62      6000\n",
      "           8       0.69      0.70      0.69      6000\n",
      "           9       0.75      0.71      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.2376937324105697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:48<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2011\n",
      "Precision: 0.6648, Recall: 0.6362, F1-Score: 0.6443\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.58      0.55      6000\n",
      "           1       0.78      0.52      0.62      6000\n",
      "           2       0.78      0.62      0.69      6000\n",
      "           3       0.47      0.51      0.49      6000\n",
      "           4       0.81      0.75      0.78      6000\n",
      "           5       0.91      0.75      0.82      6000\n",
      "           6       0.37      0.54      0.44      6000\n",
      "           7       0.64      0.66      0.65      6000\n",
      "           8       0.67      0.70      0.68      6000\n",
      "           9       0.71      0.73      0.72      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.64     60000\n",
      "weighted avg       0.66      0.64      0.64     60000\n",
      "\n",
      "#Module 1 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:16<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1081\n",
      "Precision: 0.6734, Recall: 0.6481, F1-Score: 0.6514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.49      0.55      6000\n",
      "           1       0.74      0.62      0.67      6000\n",
      "           2       0.76      0.67      0.71      6000\n",
      "           3       0.55      0.45      0.50      6000\n",
      "           4       0.79      0.79      0.79      6000\n",
      "           5       0.91      0.77      0.84      6000\n",
      "           6       0.49      0.45      0.47      6000\n",
      "           7       0.42      0.81      0.55      6000\n",
      "           8       0.68      0.71      0.70      6000\n",
      "           9       0.76      0.72      0.74      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.2380255273352088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [08:34<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2473\n",
      "Precision: 0.6622, Recall: 0.6213, F1-Score: 0.6280\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.45      0.54      6000\n",
      "           1       0.71      0.61      0.66      6000\n",
      "           2       0.80      0.55      0.65      6000\n",
      "           3       0.57      0.41      0.47      6000\n",
      "           4       0.72      0.79      0.76      6000\n",
      "           5       0.94      0.69      0.80      6000\n",
      "           6       0.33      0.53      0.41      6000\n",
      "           7       0.47      0.76      0.58      6000\n",
      "           8       0.68      0.69      0.68      6000\n",
      "           9       0.73      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.66      0.62      0.63     60000\n",
      "weighted avg       0.66      0.62      0.63     60000\n",
      "\n",
      "#Module 2 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [08:28<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0775\n",
      "Precision: 0.6673, Recall: 0.6577, F1-Score: 0.6577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55      6000\n",
      "           1       0.73      0.61      0.67      6000\n",
      "           2       0.71      0.72      0.72      6000\n",
      "           3       0.50      0.52      0.51      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.89      0.78      0.83      6000\n",
      "           6       0.56      0.43      0.49      6000\n",
      "           7       0.51      0.75      0.61      6000\n",
      "           8       0.64      0.76      0.69      6000\n",
      "           9       0.76      0.70      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23842010156575633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:57<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1963\n",
      "Precision: 0.6608, Recall: 0.6262, F1-Score: 0.6357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.48      0.54      6000\n",
      "           1       0.73      0.56      0.63      6000\n",
      "           2       0.69      0.72      0.70      6000\n",
      "           3       0.45      0.52      0.48      6000\n",
      "           4       0.83      0.67      0.74      6000\n",
      "           5       0.93      0.69      0.79      6000\n",
      "           6       0.34      0.55      0.42      6000\n",
      "           7       0.59      0.69      0.64      6000\n",
      "           8       0.69      0.67      0.68      6000\n",
      "           9       0.73      0.71      0.72      6000\n",
      "\n",
      "    accuracy                           0.63     60000\n",
      "   macro avg       0.66      0.63      0.64     60000\n",
      "weighted avg       0.66      0.63      0.64     60000\n",
      "\n",
      "#Module 3 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:58<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0758\n",
      "Precision: 0.6678, Recall: 0.6579, F1-Score: 0.6591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.56      6000\n",
      "           1       0.73      0.62      0.67      6000\n",
      "           2       0.71      0.74      0.72      6000\n",
      "           3       0.50      0.53      0.51      6000\n",
      "           4       0.81      0.75      0.78      6000\n",
      "           5       0.90      0.75      0.82      6000\n",
      "           6       0.53      0.44      0.48      6000\n",
      "           7       0.53      0.74      0.62      6000\n",
      "           8       0.67      0.73      0.70      6000\n",
      "           9       0.74      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23952626817587722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:36<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1475\n",
      "Precision: 0.6611, Recall: 0.6402, F1-Score: 0.6457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55      6000\n",
      "           1       0.73      0.59      0.65      6000\n",
      "           2       0.74      0.69      0.72      6000\n",
      "           3       0.50      0.51      0.50      6000\n",
      "           4       0.79      0.74      0.76      6000\n",
      "           5       0.93      0.69      0.79      6000\n",
      "           6       0.39      0.51      0.44      6000\n",
      "           7       0.55      0.72      0.63      6000\n",
      "           8       0.70      0.68      0.69      6000\n",
      "           9       0.69      0.75      0.72      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.65     60000\n",
      "weighted avg       0.66      0.64      0.65     60000\n",
      "\n",
      "#Module 4 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:41<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0837\n",
      "Precision: 0.6717, Recall: 0.6583, F1-Score: 0.6552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55      6000\n",
      "           1       0.76      0.59      0.66      6000\n",
      "           2       0.72      0.71      0.72      6000\n",
      "           3       0.53      0.49      0.51      6000\n",
      "           4       0.77      0.82      0.80      6000\n",
      "           5       0.90      0.79      0.84      6000\n",
      "           6       0.64      0.37      0.47      6000\n",
      "           7       0.51      0.75      0.61      6000\n",
      "           8       0.56      0.80      0.66      6000\n",
      "           9       0.73      0.74      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.2375054149017456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:26<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1053\n",
      "Precision: 0.6565, Recall: 0.6511, F1-Score: 0.6465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.45      0.53      6000\n",
      "           1       0.72      0.63      0.67      6000\n",
      "           2       0.71      0.72      0.72      6000\n",
      "           3       0.47      0.52      0.50      6000\n",
      "           4       0.70      0.86      0.77      6000\n",
      "           5       0.90      0.77      0.83      6000\n",
      "           6       0.55      0.36      0.44      6000\n",
      "           7       0.53      0.72      0.61      6000\n",
      "           8       0.65      0.71      0.68      6000\n",
      "           9       0.67      0.77      0.72      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.65     60000\n",
      "weighted avg       0.66      0.65      0.65     60000\n",
      "\n",
      "#Module 5 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:25<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0485\n",
      "Precision: 0.6713, Recall: 0.6681, F1-Score: 0.6677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      6000\n",
      "           1       0.70      0.68      0.69      6000\n",
      "           2       0.72      0.73      0.72      6000\n",
      "           3       0.55      0.47      0.51      6000\n",
      "           4       0.82      0.78      0.80      6000\n",
      "           5       0.89      0.79      0.84      6000\n",
      "           6       0.52      0.45      0.48      6000\n",
      "           7       0.58      0.72      0.64      6000\n",
      "           8       0.66      0.74      0.69      6000\n",
      "           9       0.74      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.67     60000\n",
      "weighted avg       0.67      0.67      0.67     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.2384231545158005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:11<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2609\n",
      "Precision: 0.6624, Recall: 0.6385, F1-Score: 0.6436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.55      6000\n",
      "           1       0.76      0.56      0.65      6000\n",
      "           2       0.75      0.66      0.71      6000\n",
      "           3       0.49      0.48      0.48      6000\n",
      "           4       0.88      0.66      0.75      6000\n",
      "           5       0.88      0.77      0.82      6000\n",
      "           6       0.43      0.49      0.46      6000\n",
      "           7       0.50      0.76      0.60      6000\n",
      "           8       0.66      0.72      0.69      6000\n",
      "           9       0.72      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.64     60000\n",
      "weighted avg       0.66      0.64      0.64     60000\n",
      "\n",
      "#Module 6 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:31<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0669\n",
      "Precision: 0.6718, Recall: 0.6635, F1-Score: 0.6634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.56      6000\n",
      "           1       0.72      0.65      0.69      6000\n",
      "           2       0.72      0.72      0.72      6000\n",
      "           3       0.55      0.49      0.52      6000\n",
      "           4       0.78      0.80      0.79      6000\n",
      "           5       0.91      0.77      0.83      6000\n",
      "           6       0.56      0.44      0.49      6000\n",
      "           7       0.52      0.74      0.62      6000\n",
      "           8       0.64      0.75      0.69      6000\n",
      "           9       0.77      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23813538358411934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:14<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3470\n",
      "Precision: 0.6581, Recall: 0.6033, F1-Score: 0.6068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.63      0.54      6000\n",
      "           1       0.84      0.32      0.46      6000\n",
      "           2       0.75      0.62      0.68      6000\n",
      "           3       0.58      0.36      0.45      6000\n",
      "           4       0.80      0.71      0.75      6000\n",
      "           5       0.94      0.69      0.80      6000\n",
      "           6       0.34      0.52      0.41      6000\n",
      "           7       0.46      0.76      0.57      6000\n",
      "           8       0.64      0.73      0.68      6000\n",
      "           9       0.75      0.70      0.72      6000\n",
      "\n",
      "    accuracy                           0.60     60000\n",
      "   macro avg       0.66      0.60      0.61     60000\n",
      "weighted avg       0.66      0.60      0.61     60000\n",
      "\n",
      "#Module 7 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:22<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0543\n",
      "Precision: 0.6752, Recall: 0.6632, F1-Score: 0.6652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56      6000\n",
      "           1       0.76      0.60      0.67      6000\n",
      "           2       0.72      0.74      0.73      6000\n",
      "           3       0.50      0.52      0.51      6000\n",
      "           4       0.83      0.77      0.80      6000\n",
      "           5       0.89      0.80      0.84      6000\n",
      "           6       0.50      0.47      0.48      6000\n",
      "           7       0.52      0.77      0.62      6000\n",
      "           8       0.71      0.69      0.70      6000\n",
      "           9       0.75      0.73      0.74      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.68      0.66      0.67     60000\n",
      "weighted avg       0.68      0.66      0.67     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23841547216954342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:08<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1522\n",
      "Precision: 0.6676, Recall: 0.6385, F1-Score: 0.6444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.51      0.55      6000\n",
      "           1       0.79      0.51      0.61      6000\n",
      "           2       0.75      0.67      0.71      6000\n",
      "           3       0.45      0.54      0.49      6000\n",
      "           4       0.83      0.73      0.78      6000\n",
      "           5       0.92      0.75      0.83      6000\n",
      "           6       0.42      0.50      0.45      6000\n",
      "           7       0.50      0.77      0.61      6000\n",
      "           8       0.71      0.67      0.69      6000\n",
      "           9       0.71      0.75      0.73      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.67      0.64      0.64     60000\n",
      "weighted avg       0.67      0.64      0.64     60000\n",
      "\n",
      "#Module 8 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:13<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0986\n",
      "Precision: 0.6658, Recall: 0.6509, F1-Score: 0.6499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.55      6000\n",
      "           1       0.75      0.59      0.66      6000\n",
      "           2       0.73      0.69      0.71      6000\n",
      "           3       0.54      0.48      0.51      6000\n",
      "           4       0.74      0.83      0.78      6000\n",
      "           5       0.90      0.78      0.83      6000\n",
      "           6       0.57      0.42      0.48      6000\n",
      "           7       0.47      0.76      0.58      6000\n",
      "           8       0.58      0.78      0.67      6000\n",
      "           9       0.77      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23775811920581513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:07<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2571\n",
      "Precision: 0.6507, Recall: 0.6190, F1-Score: 0.6191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.45      0.53      6000\n",
      "           1       0.77      0.53      0.63      6000\n",
      "           2       0.75      0.61      0.67      6000\n",
      "           3       0.57      0.41      0.48      6000\n",
      "           4       0.65      0.82      0.72      6000\n",
      "           5       0.94      0.69      0.80      6000\n",
      "           6       0.48      0.44      0.46      6000\n",
      "           7       0.44      0.78      0.56      6000\n",
      "           8       0.53      0.76      0.63      6000\n",
      "           9       0.74      0.70      0.72      6000\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.65      0.62      0.62     60000\n",
      "weighted avg       0.65      0.62      0.62     60000\n",
      "\n",
      "#Module 9 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:17<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1120\n",
      "Precision: 0.6573, Recall: 0.6471, F1-Score: 0.6480\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55      6000\n",
      "           1       0.74      0.61      0.67      6000\n",
      "           2       0.70      0.71      0.71      6000\n",
      "           3       0.53      0.46      0.49      6000\n",
      "           4       0.79      0.76      0.78      6000\n",
      "           5       0.90      0.75      0.82      6000\n",
      "           6       0.48      0.45      0.46      6000\n",
      "           7       0.50      0.72      0.59      6000\n",
      "           8       0.64      0.74      0.69      6000\n",
      "           9       0.73      0.72      0.72      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.65     60000\n",
      "weighted avg       0.66      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23926789057257936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:05<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2750\n",
      "Precision: 0.6465, Recall: 0.6167, F1-Score: 0.6210\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.48      0.54      6000\n",
      "           1       0.73      0.60      0.66      6000\n",
      "           2       0.73      0.65      0.69      6000\n",
      "           3       0.61      0.35      0.44      6000\n",
      "           4       0.81      0.69      0.74      6000\n",
      "           5       0.87      0.73      0.79      6000\n",
      "           6       0.33      0.50      0.40      6000\n",
      "           7       0.48      0.71      0.58      6000\n",
      "           8       0.63      0.72      0.67      6000\n",
      "           9       0.68      0.74      0.71      6000\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.65      0.62      0.62     60000\n",
      "weighted avg       0.65      0.62      0.62     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "    \n",
    "    positive_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "    \n",
    "    all_samples = sampling_class(\n",
    "        train_dataloader, 200, 20, num_labels, False, 4, device=device\n",
    "    )\n",
    "    \n",
    "    print(\"after head prune\")\n",
    "    evaluate_model(model, model_config, test_dataloader)\n",
    "    \n",
    "    module = copy.deepcopy(model)\n",
    "    # wr = WeightRemoverBert(model, p=0.9)\n",
    "    ci = ConcernIdentificationBert(model, p=0.4)\n",
    "    ti = TanglingIdentification(model, p=0.6)\n",
    "    \n",
    "    print(\"after removing weights\")\n",
    "    \n",
    "    eval_step = 5\n",
    "    # for idx, batch in enumerate(all_samples):\n",
    "    #     input_ids, attn_mask, _, total_sampled = batch\n",
    "    #     with torch.no_grad():\n",
    "    #         wr.propagate(module, input_ids)\n",
    "    #     if idx % eval_step:\n",
    "    #         # result = evaluate_model(module, model_config, test_dataloader)\n",
    "    #         pass\n",
    "    \n",
    "    print(\"after CI\")\n",
    "    \n",
    "    for idx, batch in enumerate(positive_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            ci.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    \n",
    "    print(\"after TI\")\n",
    "    \n",
    "    for idx, batch in enumerate(negative_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            ti.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    a, b = get_sparsity(module)\n",
    "    print(a)\n",
    "    result = evaluate_model(module, model_config, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
