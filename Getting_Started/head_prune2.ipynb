{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1ee1c-0594-49b2-8b60-099cd6398872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d52d686-863f-4ef4-a172-1ae435fa8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5611dd8a-9d8c-42c6-8e2f-83d0cb321880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from utils.model_utils.evaluate import evaluate_model\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.decompose_utils.weight_remover import WeightRemoverBert\n",
    "from utils.decompose_utils.concern_identification import ConcernIdentificationBert\n",
    "from utils.decompose_utils.tangling_identification import TanglingIdentification\n",
    "from transformers import AutoConfig\n",
    "from utils.model_utils.save_module import save_module\n",
    "from datetime import datetime\n",
    "from utils.decompose_utils.concern_modularization import ConcernModularizationBert\n",
    "from utils.decompose_utils.sampling import sampling_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bec197-1db0-45eb-8ad2-eb0fe4cf425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model_utils.evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82e28fa-64e5-4c4a-84e1-e1ba928d9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"fabriceyhc/bert-base-uncased-yahoo_answers_topics\"\n",
    "task_type = \"classification\"\n",
    "architectures = \"bert\"\n",
    "dataset_name = \"Yahoo\"\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fda417-04ff-42a1-9ad8-c92a6a5e13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.model_utils.evaluate import get_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca83f3d1-f1a6-4639-a07c-9be575d4180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fcb878-31e0-41f3-8ff1-7220ca9451a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "model_config = ModelConfig(\n",
    "    model_name=model_name,\n",
    "    task_type=task_type,\n",
    "    dataset_name=dataset_name,\n",
    "    checkpoint=checkpoint,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc39d9e-bf4c-48c0-84e6-9d53cc98115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/Minwoo/LESN/Decompose/DecomposeTransformer/Models/Configs/classification/fabriceyhc/bert-base-uncased-yahoo_answers_topics exists.\n",
      "Loading the model.\n",
      "The model fabriceyhc/bert-base-uncased-yahoo_answers_topics is loaded.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, checkpoint = load_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030711ac-259e-4bbf-9b4d-47449daf22d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset Yahoo\n",
      "Load cached dataset.\n",
      "The dataset Yahoo is loaded\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
    "        model_config, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6cf8e8-e578-494a-b5a0-b15faf349a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_indices(data):\n",
    "\n",
    "    data_np = np.array(data)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "\n",
    "    result = []\n",
    "\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "def get_sorted_indices_except_max(data):\n",
    "    data_np = np.array(data)\n",
    "    max_indices = np.argmin(data_np, axis=1)\n",
    "    data_flattened = data_np.flatten()\n",
    "    sorted_indices = np.argsort(data_flattened)[::-1]\n",
    "    row_indices = sorted_indices // 12\n",
    "    col_indices = sorted_indices % 12\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(len(row_indices)):\n",
    "        # 각 행의 최대값 인덱스를 제외\n",
    "        if col_indices[i] != max_indices[row_indices[i]]:\n",
    "            result.append((row_indices[i], col_indices[i]))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "ablating_head_num_in_CI = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e6ce30-935f-43da-b1aa-3f41067df563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_data = np.array([\n",
    "    [-0.07, 0.30, 0.07, -0.10, 0.03, -0.30, 0.13, 0.03, -0.27, -0.53, -0.20, -0.13],\n",
    "    [-0.20, -0.10, 0.00, 0.00, 0.03, 0.00, -0.20, -0.37, 0.03, 0.27, -0.47, -0.10],\n",
    "    [0.03, 0.17, 0.07, -0.23, 0.00, 0.10, -0.40, -0.13, -0.17, 0.10, -0.47, -0.03],\n",
    "    [-1.03, 0.13, 0.13, -0.23, -0.07, 0.07, 0.03, 0.03, -0.20, 0.13, -0.23, -0.27],\n",
    "    [-1.03, -0.33, -0.20, -0.40, 0.17, -0.10, 0.10, 0.10, -0.13, 0.03, -0.13, -0.13],\n",
    "    [-0.03, -0.13, 0.07, -0.03, 0.33, 0.00, 0.10, 0.03, -0.27, -0.23, -0.23, -0.17],\n",
    "    [-0.27, 0.03, -0.10, 0.00, -0.13, -0.33, -0.10, 0.40, 1.03, -0.10, 0.13, 0.23],\n",
    "    [-0.57, 0.07, 0.10, 0.03, 0.17, 0.70, 0.07, -0.07, -0.57, 0.07, -0.03, 0.10],\n",
    "    [-0.13, -0.37, -0.03, -0.17, -0.30, 0.03, -0.33, 0.20, 0.13, -0.43, -0.40, -0.50],\n",
    "    [-0.17, -0.17, 0.13, -0.27, -0.40, 1.77, -0.07, 0.03, 0.40, 0.13, 0.17, -0.30],\n",
    "    [0.60, -0.30, -0.03, 0.07, -0.03, 0.00, 0.53, -0.03, 0.43, -0.03, 0.13, 0.80],\n",
    "    [0.50, 0.17, -0.47, -0.10, -0.77, 0.10, 0.60, -0.73, -0.50, -0.03, -0.10, 1.10]\n",
    "])\n",
    "class_2_data = np.array([\n",
    "    [-0.10, -0.07, -0.27, -0.27, 0.27, -0.07, -0.17, -0.33, -0.50, -0.03, -0.17, 0.07],\n",
    "    [0.07, 0.03, -0.17, 0.10, -0.17, -0.23, -0.17, -0.23, -0.07, -0.33, -0.10, -0.27],\n",
    "    [-0.17, -0.13, 0.03, 0.10, 0.03, 0.00, -0.13, -0.20, 0.03, -0.07, -0.10, 0.00],\n",
    "    [0.20, 0.10, 0.07, -0.07, -0.20, -0.43, 0.17, -0.10, -0.37, -0.13, -0.27, -0.07],\n",
    "    [-0.17, -0.07, 0.10, -0.27, -0.20, 0.00, -0.03, -0.20, 0.00, 0.23, 0.00, 0.10],\n",
    "    [-0.10, 0.03, 0.07, 0.03, 0.10, -0.10, 0.17, -0.13, -0.23, 0.17, 0.17, -0.03],\n",
    "    [-0.17, -0.03, 0.13, 0.00, -0.03, -0.13, -0.03, 0.00, -0.47, -0.23, -0.03, 0.30],\n",
    "    [0.17, 0.03, 0.13, -0.07, -0.23, -0.13, 0.00, 0.00, 0.00, -0.13, 0.03, 0.07],\n",
    "    [-0.03, -0.13, -0.03, 0.03, -0.03, -0.10, -0.13, -0.27, -0.07, 0.00, 0.07, -0.03],\n",
    "    [-0.03, -0.70, 0.27, 0.23, 0.10, -1.30, -0.13, -0.10, -0.47, -0.17, 0.03, -0.23],\n",
    "    [0.00, -0.23, 0.13, 0.17, -0.07, 0.03, -0.57, -0.43, -0.43, 0.00, -0.03, -0.93],\n",
    "    [-1.53, 0.07, 0.17, 0.03, 0.03, -0.20, -0.60, -0.07, 0.37, 0.37, -0.23, -0.37]\n",
    "])\n",
    "class_3_data = np.array([\n",
    "    [0.23, 0.33, 0.03, -0.10, 0.07, -0.67, 0.17, 0.30, 0.27, -0.43, 0.00, 0.00],\n",
    "    [0.17, -0.03, 0.20, 0.07, 0.00, 0.00, 0.07, 0.07, 0.20, 0.10, -0.07, 0.07],\n",
    "    [-0.10, -0.10, 0.20, 0.03, -0.03, -0.07, 0.13, -0.03, -0.03, -0.17, 0.00, 0.10],\n",
    "    [-0.20, 0.00, -0.13, 0.00, 0.20, 0.10, -0.07, -0.13, -0.03, 0.07, -0.07, -0.20],\n",
    "    [-0.03, 0.10, -0.20, 0.20, 0.10, -0.27, -0.13, -0.07, -0.07, -0.07, -0.03, 0.03],\n",
    "    [-0.03, -0.13, -0.07, -0.10, 0.10, 0.03, -0.07, 0.00, 0.13, 0.00, -0.17, 0.00],\n",
    "    [0.03, -0.17, -0.07, -0.03, -0.03, -0.07, -0.07, -0.10, 0.00, -0.07, 0.03, 0.03],\n",
    "    [-0.23, -0.13, 0.00, 0.03, 0.07, -0.17, -0.07, 0.00, -0.03, -0.10, -0.20, 0.03],\n",
    "    [-0.03, -0.10, 0.13, -0.07, 0.03, -0.13, -0.10, 0.07, -0.10, 0.00, -0.33, -0.07],\n",
    "    [0.30, -0.03, -0.10, 0.13, 0.07, -0.20, -0.07, -0.40, -0.50, -0.03, -0.13, 0.13],\n",
    "    [0.20, 0.10, -0.20, 0.10, -0.23, 0.07, -1.07, 0.33, 0.27, 0.07, -0.03, -0.20],\n",
    "    [-0.67, 0.07, 0.00, 0.23, -0.23, 0.23, 0.33, -0.07, -0.17, 0.27, 0.07, -0.83]\n",
    "])\n",
    "class_4_data = np.array([\n",
    "    [0.33, 0.20, -0.07, -0.13, -0.10, 0.10, -0.03, 0.23, -0.17, -0.17, -0.30, 0.00],\n",
    "    [-0.07, -0.10, 0.33, 0.07, -0.10, -0.13, 0.03, 0.13, 0.03, 0.03, -0.27, 0.23],\n",
    "    [0.07, -0.43, 0.13, -0.07, 0.10, 0.17, 0.03, -0.13, -0.17, -0.10, 0.07, 0.23],\n",
    "    [0.50, -0.03, -0.03, 0.20, 0.00, 0.10, -0.17, -0.03, -0.17, -0.03, -0.20, 0.07],\n",
    "    [-0.17, -0.17, 0.23, -0.07, -0.13, 0.07, -0.23, -0.03, 0.10, 0.13, -0.17, -0.13],\n",
    "    [-0.07, -0.07, -0.17, 0.00, -0.17, -0.13, -0.07, 0.03, -0.17, 0.10, -0.03, 0.10],\n",
    "    [0.13, -0.03, -0.17, 0.00, -0.07, -0.07, 0.00, -0.27, 0.30, 0.10, -0.03, 0.07],\n",
    "    [0.10, -0.10, -0.10, -0.10, 0.03, -0.50, -0.03, 0.00, 0.10, -0.20, -0.23, 0.03],\n",
    "    [0.07, 0.33, -0.17, 0.10, -0.03, 0.13, 0.03, -0.07, -0.43, -0.13, 0.10, -0.10],\n",
    "    [0.20, 0.33, -0.17, 0.10, -0.23, -0.87, 0.13, -0.60, -0.07, 0.10, 0.50, -0.10],\n",
    "    [-0.70, 0.20, -0.03, 0.67, 0.27, 0.03, 1.47, 0.97, -0.07, -0.03, -0.07, 0.27],\n",
    "    [1.80, 0.30, -0.13, -0.13, 0.57, -0.20, 0.47, 0.03, 0.37, -0.40, 0.03, 0.93]\n",
    "])\n",
    "class_5_data = np.array([\n",
    "    [-0.20, -0.37, -0.10, 0.20, -0.07, 0.33, 0.17, -0.10, 0.03, 0.20, -0.03, 0.03],\n",
    "    [0.03, 0.03, 0.30, -0.10, -0.23, -0.30, 0.00, -0.27, -0.17, -0.10, 0.17, -0.13],\n",
    "    [-0.33, 0.03, 0.03, -0.13, -0.10, -0.33, -0.10, -0.17, -0.17, -0.27, 0.07, 0.07],\n",
    "    [-0.43, -0.10, -0.10, -0.17, -0.13, 0.03, 0.00, -0.10, 0.10, -0.03, 0.13, -0.23],\n",
    "    [-0.33, 0.13, 0.13, -0.07, 0.10, 0.07, -0.07, -0.07, -0.07, 0.20, -0.03, 0.10],\n",
    "    [0.10, 0.00, -0.17, -0.13, 0.03, -0.10, -0.03, 0.00, 0.07, 0.03, -0.10, -0.03],\n",
    "    [0.07, 0.07, -0.17, 0.00, -0.13, 0.17, 0.13, -0.07, 0.20, 0.03, -0.10, 0.00],\n",
    "    [-0.03, -0.23, 0.13, -0.03, -0.13, -0.37, -0.10, 0.13, -0.07, -0.10, 0.20, -0.20],\n",
    "    [-0.03, 0.07, 0.00, 0.03, 0.03, -0.03, -0.20, 0.00, 0.10, 0.03, 0.23, 0.27],\n",
    "    [0.07, 0.57, 0.00, -0.20, -0.13, -1.30, -0.13, 0.20, 0.30, 0.00, -0.53, -0.33],\n",
    "    [0.63, 0.13, -0.30, -0.63, -0.53, 0.03, -0.67, -0.90, 0.20, 0.17, 0.00, -0.07],\n",
    "    [-0.57, -0.23, -0.20, 0.30, 0.03, 0.03, -0.73, -0.70, -0.37, 0.60, -0.53, 1.37]\n",
    "])\n",
    "class_6_data = np.array([\n",
    "    [0.10, -0.17, 0.03, 0.00, 0.10, 0.27, -0.10, -0.07, 0.27, 0.23, 0.07, -0.03],\n",
    "    [0.00, 0.00, 0.03, -0.03, 0.00, -0.03, -0.03, -0.07, 0.13, 0.03, -0.10, -0.23],\n",
    "    [0.10, 0.07, 0.07, 0.03, 0.07, -0.03, -0.10, 0.07, 0.00, 0.03, 0.00, -0.17],\n",
    "    [-0.07, 0.00, -0.03, -0.07, 0.03, 0.07, 0.00, -0.10, -0.13, -0.07, -0.07, 0.00],\n",
    "    [0.03, 0.03, 0.13, 0.03, 0.07, 0.03, -0.07, -0.07, 0.00, 0.00, -0.10, 0.03],\n",
    "    [-0.03, 0.00, 0.00, 0.00, 0.00, -0.07, -0.03, -0.07, -0.07, -0.10, 0.03, -0.03],\n",
    "    [0.00, -0.10, 0.00, 0.00, 0.00, -0.03, -0.03, -0.03, -0.17, 0.00, 0.00, 0.03],\n",
    "    [0.07, -0.07, -0.07, -0.03, -0.10, -0.20, -0.03, 0.03, -0.03, 0.03, 0.03, 0.00],\n",
    "    [-0.07, 0.03, -0.03, -0.03, -0.07, -0.03, -0.03, -0.03, -0.03, -0.03, -0.03, 0.07],\n",
    "    [-0.10, -0.27, 0.03, 0.03, 0.00, -0.60, -0.07, 0.13, -0.10, 0.00, -0.03, 0.13],\n",
    "    [0.00, -0.07, 0.13, 0.03, 0.13, -0.03, -1.00, -0.13, -0.10, 0.00, 0.00, 0.03],\n",
    "    [-0.10, -0.13, 0.03, -0.07, -0.03, -0.03, 0.13, 0.03, 0.07, 0.03, 0.07, -1.00]\n",
    "])\n",
    "class_7_data = np.array([\n",
    "    [0.20, -0.23, -0.17, -0.27, 0.00, -0.33, 0.07, -0.30, -0.30, -0.17, 0.13, -0.50],\n",
    "    [-0.13, -0.13, -0.30, 0.17, 0.17, -0.03, -0.07, 0.13, 0.00, -0.10, -0.13, 0.23],\n",
    "    [-0.30, 0.03, -0.13, 0.13, -0.07, -0.13, -0.10, 0.23, -0.07, -0.20, -0.30, 0.07],\n",
    "    [-0.13, 0.30, -0.20, 0.03, 0.23, -0.27, -0.17, 0.07, -0.53, -0.07, -0.17, -0.03],\n",
    "    [-0.43, -0.07, -0.03, -0.13, -0.20, -0.03, -0.03, -0.10, 0.07, 0.27, -0.23, 0.07],\n",
    "    [0.03, 0.03, -0.10, -0.10, -0.17, -0.10, -0.03, -0.30, 0.10, 0.00, -0.23, -0.20],\n",
    "    [-0.07, -0.23, -0.10, -0.10, 0.07, -0.27, -0.10, 0.03, -0.30, 0.10, 0.13, 0.07],\n",
    "    [-0.47, -0.07, -0.07, -0.13, -0.13, -0.13, -0.13, -0.20, 0.13, -0.07, 0.03, -0.13],\n",
    "    [-0.07, -0.03, -0.03, -0.03, -0.07, 0.00, -0.07, 0.23, -0.17, -0.57, -0.20, -0.67],\n",
    "    [0.00, 0.07, -0.20, -0.03, 0.03, 0.60, 0.17, -0.17, -0.77, -0.03, -0.07, 0.53],\n",
    "    [0.03, -0.37, 0.03, -0.30, 0.60, -0.30, 0.53, 0.77, -0.03, -0.20, 0.03, -0.10],\n",
    "    [0.17, 0.13, 0.13, 0.50, -0.27, -0.27, 0.97, 0.90, 0.03, -0.13, 0.40, -2.33]\n",
    "])\n",
    "class_8_data = np.array([\n",
    "    [0.00, -0.20, -0.23, -0.03, -0.17, 0.27, -0.37, -0.03, -0.17, 0.03, -0.40, -0.03],\n",
    "    [-0.53, -0.07, 0.03, -0.07, -0.23, -0.23, -0.20, 0.03, -0.10, 0.10, -0.10, 0.33],\n",
    "    [-0.17, -0.07, -0.10, -0.07, -0.03, -0.07, 0.33, 0.13, 0.20, -0.07, 0.17, -0.13],\n",
    "    [0.33, -0.10, 0.03, 0.03, -0.17, -0.33, 0.03, 0.00, 0.30, -0.23, -0.17, -0.17],\n",
    "    [0.27, 0.00, 0.43, -0.30, -0.33, 0.07, -0.13, -0.23, 0.00, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.23, -0.20, 0.13, -0.20, -0.13, 0.03, -0.10, 0.07, 0.00, -0.23, 0.03],\n",
    "    [-0.20, -0.03, 0.20, 0.00, 0.03, 0.07, -0.13, -0.13, 0.13, -0.17, 0.00, 0.03],\n",
    "    [-0.10, -0.20, -0.17, 0.13, -0.13, 0.20, -0.07, -0.13, 0.17, 0.03, 0.20, -0.07],\n",
    "    [-0.03, 0.10, 0.10, 0.00, -0.03, -0.03, 0.13, 0.07, -0.17, 0.10, 0.23, 0.10],\n",
    "    [0.03, 0.13, 0.00, 0.03, 0.20, 0.60, -0.10, 0.37, 0.57, 0.13, -0.40, 0.03],\n",
    "    [-0.23, 0.27, -0.30, -0.27, 0.93, 0.10, -0.60, 0.07, 0.00, 0.03, -0.13, 0.00],\n",
    "    [-0.50, -0.07, -0.13, 0.00, 0.40, 0.20, -0.60, 0.27, -0.13, -0.30, 0.00, 1.37]\n",
    "])\n",
    "class_9_data = np.array([\n",
    "    [-0.40, -0.27, -0.13, -0.07, 0.00, -0.73, -0.37, -0.27, 0.20, -0.53, -0.10, -0.20],\n",
    "    [0.07, 0.00, 0.07, -0.30, -0.10, -0.13, -0.03, 0.17, -0.30, -0.03, -0.27, -0.07],\n",
    "    [-0.13, 0.13, -0.10, 0.00, -0.03, -0.23, 0.00, -0.33, -0.10, -0.33, -0.07, -0.07],\n",
    "    [0.00, -0.17, -0.10, 0.03, -0.03, -0.07, 0.07, -0.20, 0.13, -0.13, -0.20, -0.07],\n",
    "    [0.37, 0.07, -0.13, 0.10, -0.20, -0.17, 0.07, 0.00, -0.10, -0.20, -0.03, -0.10],\n",
    "    [-0.13, -0.07, -0.13, -0.03, -0.17, -0.03, -0.17, 0.00, -0.17, -0.07, -0.13, 0.00],\n",
    "    [0.20, -0.27, -0.27, -0.07, -0.07, -0.17, -0.10, -0.27, -0.43, 0.07, -0.10, -0.07],\n",
    "    [0.13, -0.07, -0.17, -0.07, -0.20, -0.27, 0.00, 0.03, -0.03, -0.20, -0.13, -0.27],\n",
    "    [-0.07, 0.07, -0.13, -0.20, 0.03, -0.17, -0.20, -0.27, -0.27, 0.23, 0.37, -0.07],\n",
    "    [-0.10, -0.03, -0.27, -0.13, -0.07, -0.77, 0.03, 0.03, 0.00, -0.27, -0.13, -0.20],\n",
    "    [-0.10, -0.30, -0.23, -0.97, -0.93, 0.00, 0.50, -0.33, -0.03, -0.07, -0.10, 0.00],\n",
    "    [0.40, -0.20, 0.33, -0.43, -0.13, -0.83, -0.93, 0.40, -0.20, 0.07, 0.00, -1.13]\n",
    "])\n",
    "class_10_data = np.array([\n",
    "    [-0.07, 0.07, 0.07, 0.13, 0.00, 0.27, -0.07, 0.10, 0.23, 0.63, -0.07, 0.00],\n",
    "    [-0.07, -0.07, -0.37, -0.03, -0.13, -0.03, -0.03, -0.13, 0.00, -0.17, -0.07, 0.00],\n",
    "    [-0.13, 0.20, -0.10, -0.10, 0.10, 0.20, -0.10, 0.07, 0.00, -0.03, 0.20, -0.20],\n",
    "    [0.03, 0.07, -0.07, 0.10, 0.00, 0.07, -0.10, 0.03, 0.17, 0.00, -0.03, 0.17],\n",
    "    [-0.10, -0.07, -0.20, 0.13, 0.03, -0.10, 0.03, -0.03, -0.13, -0.13, 0.00, 0.00],\n",
    "    [0.10, -0.03, -0.07, -0.03, -0.07, 0.00, 0.00, 0.03, -0.03, 0.07, 0.13, -0.13],\n",
    "    [0.33, -0.10, 0.07, 0.07, -0.10, 0.17, 0.07, -0.03, -0.07, -0.03, 0.03, 0.07],\n",
    "    [0.63, 0.07, 0.00, -0.07, 0.20, -0.27, -0.07, -0.03, 0.07, 0.17, -0.10, -0.03],\n",
    "    [-0.07, 0.13, 0.10, 0.00, -0.03, 0.00, 0.03, -0.03, 0.40, 0.33, 0.33, 0.20],\n",
    "    [-0.20, -0.33, 0.30, 0.07, 0.23, -0.60, 0.00, 0.13, 0.37, -0.07, -0.03, 0.03],\n",
    "    [-0.07, -0.03, 0.37, 0.20, -0.37, -0.07, 0.90, -1.03, -0.03, 0.00, 0.07, 0.03],\n",
    "    [-0.43, -0.17, -0.03, -0.07, -0.17, 0.23, -0.13, -0.37, 0.30, -0.07, 0.33, 0.33]\n",
    "])\n",
    "\n",
    "class_1_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, -0.09, -0.03, -0.06, -0.04, -0.02, -0.09, -0.06, 0.00, 0.00, -0.03, -0.03],\n",
    "    [-0.06, -0.05, -0.03, -0.06, -0.05, -0.09, -0.03, -0.04, -0.03, -0.02, -0.01, 0.05],\n",
    "    [-0.08, -0.06, -0.01, -0.02, 0.00, -0.06, 0.04, -0.04, -0.01, -0.06, 0.03, -0.06],\n",
    "    [0.13, -0.03, -0.07, -0.01, 0.02, -0.03, 0.01, -0.02, -0.02, -0.02, -0.03, -0.03],\n",
    "    [0.07, 0.06, 0.07, -0.04, -0.11, -0.03, -0.04, -0.02, -0.02, 0.00, 0.00, 0.00],\n",
    "    [-0.01, -0.02, -0.05, -0.02, -0.05, -0.08, -0.07, 0.01, 0.00, 0.01, -0.04, 0.01],\n",
    "    [-0.01, -0.05, -0.06, 0.00, 0.00, 0.01, -0.03, -0.12, -0.15, -0.03, -0.03, 0.04],\n",
    "    [0.06, -0.04, -0.05, -0.03, -0.07, -0.22, -0.01, -0.01, 0.13, -0.04, 0.00, -0.02],\n",
    "    [-0.02, 0.09, -0.05, -0.02, 0.03, -0.02, 0.05, -0.05, -0.12, 0.01, 0.10, 0.01],\n",
    "    [0.04, -0.06, -0.01, 0.05, 0.01, -0.64, -0.05, -0.03, 0.00, -0.05, -0.09, 0.00],\n",
    "    [-0.09, 0.05, -0.06, -0.06, 0.03, -0.01, -0.22, -0.02, -0.07, -0.03, -0.03, -0.22],\n",
    "    [-0.20, 0.02, 0.05, 0.02, 0.09, -0.02, -0.25, 0.05, 0.05, -0.03, 0.05, -0.30]\n",
    "]   \n",
    ")\n",
    "class_2_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, -0.02, -0.01, 0.05, -0.03, 0.02, -0.01, 0.03, 0.10, -0.04, 0.01, -0.04],\n",
    "    [-0.01, 0.02, 0.01, -0.02, 0.01, 0.00, 0.00, 0.08, 0.00, 0.04, -0.01, 0.05],\n",
    "    [0.00, -0.01, 0.01, 0.01, 0.00, -0.02, 0.01, 0.03, 0.00, -0.01, 0.02, 0.02],\n",
    "    [-0.04, -0.01, -0.02, 0.01, -0.01, 0.03, -0.02, 0.00, 0.04, -0.01, -0.01, 0.00],\n",
    "    [0.03, 0.00, -0.01, 0.03, 0.00, 0.03, 0.04, 0.02, 0.00, 0.00, -0.02, 0.01],\n",
    "    [0.00, -0.01, -0.02, -0.02, -0.02, -0.01, 0.00, 0.01, -0.02, -0.02, -0.01, 0.00],\n",
    "    [0.03, -0.01, 0.00, 0.00, -0.01, 0.01, 0.01, 0.00, 0.05, 0.00, 0.01, -0.01],\n",
    "    [-0.01, -0.01, -0.02, 0.02, -0.01, 0.00, 0.02, 0.00, -0.01, -0.01, 0.02, -0.01],\n",
    "    [0.01, 0.01, 0.02, 0.00, 0.02, 0.01, 0.01, 0.02, -0.02, 0.01, -0.01, 0.01],\n",
    "    [0.00, 0.04, -0.01, 0.01, -0.02, 0.12, 0.03, 0.00, 0.08, 0.03, 0.00, 0.04],\n",
    "    [0.01, 0.04, -0.02, -0.04, -0.01, 0.02, 0.06, 0.06, 0.04, 0.00, 0.00, 0.13],\n",
    "    [0.21, 0.03, -0.05, -0.01, 0.02, 0.02, 0.05, 0.02, -0.05, -0.04, 0.02, 0.05]\n",
    "]\n",
    ")\n",
    "class_3_neg_acc = np.array(\n",
    "[\n",
    "    [-0.05, -0.04, -0.02, 0.00, 0.04, 0.12, -0.01, -0.04, -0.02, 0.08, -0.02, -0.02],\n",
    "    [-0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02, 0.00, -0.01, -0.01, 0.02],\n",
    "    [0.02, 0.03, -0.02, 0.02, 0.01, 0.02, -0.01, 0.02, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.02, 0.03, 0.05, 0.02, -0.05, -0.04, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02],\n",
    "    [0.03, -0.02, 0.04, -0.04, -0.01, 0.02, 0.03, 0.03, 0.00, 0.00, 0.02, 0.03],\n",
    "    [0.00, 0.00, 0.00, 0.04, -0.03, 0.02, 0.02, -0.01, 0.01, 0.06, 0.02, -0.02],\n",
    "    [0.04, 0.06, 0.04, 0.01, 0.01, 0.07, -0.03, 0.02, 0.02, 0.05, 0.04, 0.05],\n",
    "    [0.02, 0.02, 0.02, 0.00, -0.01, 0.07, 0.00, -0.01, 0.02, -0.02, 0.06, -0.01],\n",
    "    [0.00, 0.03, 0.02, 0.02, 0.02, 0.01, -0.02, -0.01, 0.01, 0.06, 0.12, 0.00],\n",
    "    [-0.05, 0.03, 0.03, -0.05, 0.00, 0.11, 0.03, 0.05, 0.10, 0.04, 0.01, -0.03],\n",
    "    [-0.04, -0.01, 0.07, -0.03, 0.07, 0.00, 0.28, -0.10, 0.00, 0.00, 0.01, 0.06],\n",
    "    [0.19, -0.01, -0.01, -0.04, 0.07, -0.07, -0.10, 0.04, 0.05, -0.04, 0.00, 0.30]\n",
    "]\n",
    ")\n",
    "class_4_neg_acc = np.array(\n",
    "[\n",
    "    [-0.04, -0.03, 0.01, 0.02, 0.03, -0.09, -0.05, 0.00, 0.02, -0.03, -0.01, 0.02],\n",
    "    [0.03, 0.02, 0.02, -0.01, -0.01, 0.00, 0.00, -0.04, -0.02, 0.01, -0.01, -0.06],\n",
    "    [-0.04, 0.00, 0.03, 0.00, 0.00, -0.01, -0.01, 0.01, 0.00, -0.04, -0.04, 0.01],\n",
    "    [-0.09, 0.02, -0.03, -0.03, -0.02, -0.01, 0.02, 0.00, 0.01, -0.03, -0.04, -0.02],\n",
    "    [-0.09, -0.01, -0.01, -0.02, -0.01, -0.03, 0.02, -0.05, -0.02, -0.02, -0.02, -0.03],\n",
    "    [0.01, -0.01, 0.00, -0.01, 0.02, 0.00, 0.02, -0.01, 0.02, 0.01, -0.04, -0.02],\n",
    "    [-0.03, -0.04, -0.02, -0.01, 0.00, -0.03, 0.00, 0.05, -0.01, -0.04, 0.00, 0.01],\n",
    "    [0.00, 0.02, 0.02, 0.00, 0.00, 0.05, -0.02, -0.01, -0.03, 0.00, 0.00, -0.03],\n",
    "    [-0.01, -0.03, 0.03, -0.04, -0.03, -0.01, -0.03, 0.03, 0.08, 0.02, 0.00, 0.03],\n",
    "    [-0.03, -0.06, 0.03, -0.03, 0.04, 0.14, -0.02, 0.06, -0.05, -0.04, -0.06, 0.05],\n",
    "    [0.18, -0.08, 0.02, -0.16, -0.06, 0.00, -0.21, -0.20, 0.03, 0.01, 0.01, -0.02],\n",
    "    [-0.31, -0.06, 0.03, 0.05, -0.09, -0.02, -0.16, -0.01, -0.07, 0.08, 0.02, -0.19]\n",
    "]  \n",
    ")\n",
    "class_5_neg_acc = np.array(\n",
    "[\n",
    "    [0.03, 0.04, 0.03, -0.02, -0.01, -0.04, 0.00, 0.00, -0.03, 0.01, -0.01, -0.01],\n",
    "    [-0.01, 0.00, -0.02, 0.03, 0.00, 0.00, 0.00, 0.02, 0.01, 0.01, -0.02, 0.02],\n",
    "    [0.02, 0.01, -0.01, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, 0.00, 0.00],\n",
    "    [0.02, 0.01, 0.01, 0.02, -0.01, 0.00, -0.02, 0.00, -0.01, 0.02, -0.02, 0.02],\n",
    "    [0.03, -0.01, 0.01, 0.00, 0.02, 0.02, -0.01, 0.01, 0.01, 0.02, 0.02, 0.00],\n",
    "    [0.00, 0.01, 0.00, 0.01, -0.01, -0.01, 0.02, 0.02, -0.01, 0.00, 0.01, 0.00],\n",
    "    [0.00, -0.01, 0.01, 0.00, 0.00, -0.02, 0.00, 0.01, 0.01, -0.01, 0.02, 0.01],\n",
    "    [-0.01, 0.00, 0.00, 0.00, 0.02, 0.03, 0.01, 0.01, -0.03, 0.00, -0.02, 0.02],\n",
    "    [0.00, 0.00, -0.01, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, -0.01, 0.00, -0.05],\n",
    "    [0.03, -0.06, -0.01, -0.01, 0.02, 0.15, 0.00, -0.01, -0.04, 0.00, 0.05, 0.03],\n",
    "    [-0.07, 0.01, 0.01, 0.05, 0.03, 0.00, 0.13, 0.09, 0.00, 0.00, -0.01, 0.01],\n",
    "    [0.03, 0.01, 0.01, -0.03, 0.00, 0.00, 0.07, 0.07, 0.02, -0.03, 0.04, -0.13]\n",
    "] \n",
    ")\n",
    "class_6_neg_acc = np.array(\n",
    "[\n",
    "    [-0.01, 0.02, -0.02, 0.00, 0.01, -0.04, 0.02, 0.02, -0.02, -0.02, -0.02, 0.01],\n",
    "    [0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.01, 0.00, -0.02, 0.00, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.01, 0.00, 0.00, 0.01, 0.00, -0.02, 0.00, 0.00, 0.01, 0.01],\n",
    "    [0.00, 0.01, 0.01, 0.00, 0.01, -0.01, 0.01, 0.01, 0.02, -0.01, 0.02, -0.01],\n",
    "    [-0.03, 0.00, -0.02, 0.03, 0.01, 0.01, 0.01, 0.00, 0.02, 0.01, 0.00, 0.02],\n",
    "    [0.01, 0.01, 0.00, 0.01, 0.00, 0.01, 0.01, 0.02, -0.02, 0.01, 0.00, 0.01],\n",
    "    [0.02, 0.01, 0.01, 0.01, 0.01, -0.01, 0.02, 0.00, 0.06, 0.01, 0.01, 0.01],\n",
    "    [0.01, 0.01, 0.00, 0.02, 0.02, 0.02, 0.01, 0.00, -0.01, 0.01, -0.01, 0.01],\n",
    "    [0.01, 0.00, 0.01, 0.01, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.03, -0.01],\n",
    "    [0.02, 0.03, 0.00, 0.01, -0.01, 0.08, 0.01, 0.00, 0.06, 0.01, 0.01, -0.02],\n",
    "    [0.02, 0.00, 0.00, 0.01, -0.04, 0.01, 0.16, 0.01, 0.03, 0.01, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.03, 0.00, -0.02, -0.01, 0.00, 0.02, 0.00, 0.02, 0.00, 0.14]\n",
    "]  \n",
    ")\n",
    "class_7_neg_acc = np.array(\n",
    "[\n",
    "    [-0.02, 0.03, 0.01, 0.01, 0.00, 0.04, 0.00, 0.04, 0.02, 0.06, -0.02, 0.01],\n",
    "    [-0.01, -0.03, 0.03, 0.01, -0.02, -0.01, -0.01, 0.01, 0.00, 0.00, -0.02, -0.03],\n",
    "    [0.01, 0.03, 0.02, -0.01, -0.01, -0.02, 0.01, 0.00, 0.00, 0.00, 0.04, 0.00],\n",
    "    [0.03, -0.02, 0.01, -0.02, -0.01, -0.02, 0.00, 0.00, 0.01, 0.01, 0.02, -0.02],\n",
    "    [0.03, 0.00, 0.01, 0.00, -0.01, -0.01, -0.01, -0.01, -0.01, 0.00, 0.00, -0.03],\n",
    "    [-0.04, -0.02, 0.00, 0.00, 0.00, -0.03, 0.00, -0.02, -0.02, -0.02, -0.01, -0.01],\n",
    "    [0.02, 0.01, 0.00, 0.00, -0.03, 0.04, 0.01, -0.03, 0.00, 0.01, -0.01, 0.00],\n",
    "    [0.05, -0.02, 0.00, 0.00, 0.03, 0.00, -0.01, 0.01, -0.04, -0.01, -0.01, 0.00],\n",
    "    [-0.01, -0.02, -0.02, 0.00, -0.01, -0.01, -0.01, -0.07, 0.00, 0.03, 0.02, 0.04],\n",
    "    [-0.01, 0.00, 0.02, 0.00, -0.01, -0.11, -0.03, 0.07, 0.10, -0.01, -0.02, -0.09],\n",
    "    [0.00, 0.04, -0.02, -0.03, -0.05, 0.00, -0.03, -0.14, 0.02, 0.03, -0.01, -0.02],\n",
    "    [-0.08, -0.03, -0.01, -0.06, 0.01, 0.03, -0.15, -0.11, 0.00, 0.00, -0.06, 0.17]\n",
    "]\n",
    ")\n",
    "class_8_neg_acc = np.array(\n",
    "[\n",
    "    [0.02, -0.02, -0.02, -0.05, 0.01, -0.10, -0.02, -0.03, -0.01, -0.10, -0.01, 0.00],\n",
    "    [0.02, 0.00, -0.04, -0.02, 0.00, -0.02, 0.00, -0.08, 0.00, -0.06, -0.01, -0.04],\n",
    "    [-0.04, 0.00, 0.00, 0.01, 0.00, 0.03, -0.09, -0.06, -0.06, -0.06, -0.13, 0.00],\n",
    "    [-0.13, -0.03, -0.02, -0.02, 0.02, 0.02, -0.05, -0.05, -0.09, -0.02, -0.09, 0.02],\n",
    "    [-0.17, 0.00, -0.10, 0.04, 0.04, -0.03, -0.01, -0.03, 0.00, -0.01, -0.01, 0.00],\n",
    "    [0.01, 0.01, -0.02, 0.00, 0.01, 0.01, 0.00, -0.04, -0.03, -0.03, 0.03, -0.03],\n",
    "    [-0.02, -0.05, -0.03, -0.02, -0.05, -0.10, -0.01, -0.03, -0.03, -0.01, -0.03, -0.02],\n",
    "    [-0.01, -0.03, -0.02, -0.03, -0.02, -0.20, -0.04, 0.01, -0.04, 0.01, -0.05, -0.02],\n",
    "    [-0.01, -0.06, -0.03, 0.00, -0.05, -0.02, -0.07, 0.01, -0.02, -0.06, -0.05, -0.06],\n",
    "    [-0.02, -0.09, -0.03, -0.03, -0.05, -0.35, 0.03, -0.14, -0.15, -0.08, 0.04, -0.07],\n",
    "    [0.03, -0.12, 0.00, 0.04, -0.36, -0.02, 0.10, -0.05, -0.03, -0.02, 0.00, 0.00],\n",
    "    [0.03, -0.02, 0.04, 0.02, -0.15, -0.07, 0.14, -0.08, 0.01, 0.12, 0.01, -0.33]\n",
    "]\n",
    ")\n",
    "class_9_neg_acc = np.array(\n",
    "[\n",
    "    [0.05, 0.03, 0.00, -0.04, 0.01, 0.10, 0.10, 0.02, -0.08, 0.06, -0.02, -0.01],\n",
    "    [-0.03, -0.02, 0.00, 0.04, -0.04, 0.00, -0.02, -0.02, 0.03, 0.01, -0.02, -0.02],\n",
    "    [0.00, -0.01, -0.02, -0.03, 0.02, 0.01, -0.02, 0.01, 0.00, 0.02, -0.01, -0.02],\n",
    "    [-0.03, 0.02, 0.00, 0.03, 0.00, 0.01, -0.01, 0.00, -0.03, 0.00, 0.01, -0.04],\n",
    "    [-0.06, -0.04, 0.01, -0.08, 0.02, -0.03, -0.06, -0.03, -0.01, 0.02, -0.05, -0.01],\n",
    "    [-0.02, -0.03, 0.01, -0.03, 0.02, -0.02, 0.00, -0.01, 0.02, -0.05, -0.03, 0.00],\n",
    "    [-0.04, 0.00, 0.00, -0.01, 0.00, 0.01, 0.01, 0.04, 0.05, -0.01, 0.00, 0.01],\n",
    "    [-0.07, -0.01, 0.01, -0.01, -0.02, 0.03, -0.02, -0.02, -0.01, 0.00, -0.02, 0.01],\n",
    "    [-0.01, -0.02, 0.00, 0.01, -0.03, 0.00, -0.02, 0.03, 0.06, -0.06, -0.16, 0.00],\n",
    "    [0.01, 0.00, 0.02, 0.04, -0.02, 0.11, -0.04, -0.01, -0.07, 0.04, 0.00, 0.03],\n",
    "    [-0.02, 0.02, 0.01, 0.15, 0.17, -0.01, -0.12, 0.06, 0.00, -0.01, 0.04, -0.02],\n",
    "    [-0.11, 0.01, -0.12, 0.07, 0.00, 0.12, 0.25, -0.10, 0.00, -0.04, -0.05, 0.31]\n",
    "]\n",
    ")\n",
    "class_10_neg_acc = np.array(\n",
    "[\n",
    "    [0.01, 0.05, -0.01, 0.03, 0.01, -0.06, 0.01, -0.02, 0.01, -0.09, 0.03, 0.00],\n",
    "    [0.03, 0.01, 0.08, 0.03, 0.04, -0.02, 0.02, 0.01, 0.03, 0.02, -0.02, 0.03],\n",
    "    [0.00, 0.00, 0.03, 0.01, 0.02, -0.02, 0.04, 0.01, 0.02, 0.00, 0.02, 0.05],\n",
    "    [0.02, 0.04, 0.03, 0.00, 0.05, 0.00, 0.05, 0.00, 0.01, 0.01, 0.03, -0.01],\n",
    "    [-0.02, 0.00, 0.04, 0.00, 0.01, 0.01, 0.00, 0.01, 0.01, 0.05, 0.00, 0.03],\n",
    "    [0.03, 0.01, 0.02, 0.01, 0.04, 0.04, 0.01, 0.00, 0.01, 0.03, 0.00, 0.02],\n",
    "    [0.00, 0.01, 0.00, 0.02, 0.04, -0.03, 0.00, 0.04, 0.05, 0.03, 0.01, 0.01],\n",
    "    [-0.06, 0.00, 0.02, 0.01, 0.00, 0.11, 0.02, 0.01, 0.00, 0.01, 0.02, 0.00],\n",
    "    [0.01, 0.04, 0.02, 0.01, 0.02, 0.01, 0.00, 0.03, -0.01, -0.06, 0.00, -0.04],\n",
    "    [0.04, 0.14, -0.03, 0.01, 0.02, 0.10, 0.02, -0.02, -0.02, 0.04, 0.00, 0.05],\n",
    "    [0.04, -0.02, -0.03, -0.01, 0.21, 0.00, -0.13, 0.23, 0.02, 0.01, 0.00, 0.03],\n",
    "    [0.12, 0.03, 0.01, 0.02, 0.02, -0.03, 0.10, 0.08, -0.03, 0.03, -0.01, -0.07]\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "923bb456-d3ab-4511-b101-bb88705462f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recovered head num:  20\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  18\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  18\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n",
      "actually_recovered_head_num:  20\n",
      "actually_recovered_head_num:  19\n"
     ]
    }
   ],
   "source": [
    "correct_num = []\n",
    "correct_num_neg = []\n",
    "for r in range(20, 20 + 1): # 20개 되살림\n",
    "    print('recovered head num: ', r)\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 1번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_1_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_1_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_1 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_1 = model_1.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_1.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 2번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_2_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_2_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_2 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_2 = model_2.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_2.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 3번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_3_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_3_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_3 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_3 = model_3.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_3.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 4번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_4_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_4_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_4 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_4 = model_4.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_4.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    # 5번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_5_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_5_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_5 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_5 = model_5.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_5.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #6번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_6_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_6_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_6 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_6 = model_6.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_6.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #7번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_7_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_7_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_7 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_7 = model_7.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_7.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #8번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_8_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_8_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_8 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_8 = model_8.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_8.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #9번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_9_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_9_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_9 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_9 = model_9.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_9.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)\n",
    "    # ================================================================================================\n",
    "\n",
    "    # ================================================================================================\n",
    "    #10번 클래스 모듈에 대해서 프루닝\n",
    "    prune_head_index = get_sorted_indices_except_max(class_10_data) ##############################\n",
    "    prune_head_index = prune_head_index[:ablating_head_num_in_CI] # 100개의 head를 pruning\n",
    "    recovering_head_index = get_sorted_indices(class_10_neg_acc) ##############################\n",
    "\n",
    "    ##########################\n",
    "    model_10 = AutoModelForSequenceClassification.from_pretrained(\"fabriceyhc/bert-base-uncased-yahoo_answers_topics\")\n",
    "    model_10 = model_10.to(device)\n",
    "    ##########################\n",
    "\n",
    "    recovering_head_num_in_TI = r\n",
    "    actually_recovered_head_num = 0\n",
    "\n",
    "    for i in recovering_head_index: # 제외할 헤드에서, 나중에 어차피 회복시킬 헤드를 제외시킴\n",
    "        recovering_head_num_in_TI -= 1\n",
    "        if i in prune_head_index:\n",
    "            prune_head_index.remove(i)\n",
    "            actually_recovered_head_num += 1      \n",
    "\n",
    "        if recovering_head_num_in_TI == 0:\n",
    "            break\n",
    "\n",
    "    for layer_index, head_index in prune_head_index: # 헤드를 제외하는 부분\n",
    "        model_10.bert.encoder.layer[layer_index].attention.prune_heads([head_index])\n",
    "        ##########################\n",
    "\n",
    "    print('actually_recovered_head_num: ', actually_recovered_head_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe29a5e-a979-43b1-9011-ebd0fd2bf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9, model_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2c3cbf-3f47-4212-b73f-555594d67232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Module 0 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:03<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0719\n",
      "Precision: 0.6671, Recall: 0.6588, F1-Score: 0.6594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      6000\n",
      "           1       0.74      0.61      0.67      6000\n",
      "           2       0.70      0.73      0.72      6000\n",
      "           3       0.52      0.49      0.51      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.88      0.78      0.83      6000\n",
      "           6       0.53      0.44      0.48      6000\n",
      "           7       0.52      0.74      0.62      6000\n",
      "           8       0.69      0.70      0.69      6000\n",
      "           9       0.75      0.71      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23794152983494857\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38935428195529515, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.38838789198133683, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.3889211018880208, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.39072206285264754, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.38673104180230033, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3889973958333333, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.39342668321397567, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.38526280721028644, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.38908979627821183, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.3950487772623698, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.3954264322916667, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39532216389973956, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3916893005371094, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.3927103678385417, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.38758087158203125, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.39127222696940106, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.3964199490017361, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.38958485921223956, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.3894212510850694, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.3878644307454427, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.39931615193684894, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3845405578613281, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.37917794121636283, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.38110393948025173, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3886566162109375, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:34<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1636\n",
      "Precision: 0.6545, Recall: 0.6336, F1-Score: 0.6388\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.63      0.54      6000\n",
      "           1       0.76      0.53      0.62      6000\n",
      "           2       0.75      0.65      0.69      6000\n",
      "           3       0.45      0.50      0.47      6000\n",
      "           4       0.80      0.77      0.78      6000\n",
      "           5       0.91      0.72      0.81      6000\n",
      "           6       0.42      0.45      0.43      6000\n",
      "           7       0.60      0.68      0.64      6000\n",
      "           8       0.71      0.66      0.68      6000\n",
      "           9       0.68      0.75      0.71      6000\n",
      "\n",
      "    accuracy                           0.63     60000\n",
      "   macro avg       0.65      0.63      0.64     60000\n",
      "weighted avg       0.65      0.63      0.64     60000\n",
      "\n",
      "#Module 1 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:08<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1081\n",
      "Precision: 0.6734, Recall: 0.6481, F1-Score: 0.6514\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.49      0.55      6000\n",
      "           1       0.74      0.62      0.67      6000\n",
      "           2       0.76      0.67      0.71      6000\n",
      "           3       0.55      0.45      0.50      6000\n",
      "           4       0.79      0.79      0.79      6000\n",
      "           5       0.91      0.77      0.84      6000\n",
      "           6       0.49      0.45      0.47      6000\n",
      "           7       0.42      0.81      0.55      6000\n",
      "           8       0.68      0.71      0.70      6000\n",
      "           9       0.76      0.72      0.74      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23821404859671064\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38957638210720485, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.38616349962022567, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.39002566867404515, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.38686709933810765, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.38718244764539933, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.39346906873914933, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.38596852620442706, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.39340760972764754, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.39638858371310765, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.3941459655761719, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.3870968288845486, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.3943502638075087, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.38739649454752606, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.39349661933051217, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.3882581922743056, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.3925136990017361, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.3868997361924913, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.38773345947265625, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.3880746629503038, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.38817087809244794, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.3894182840983073, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3847465515136719, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.39178170098198783, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.38659244113498265, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3830786810980903, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:02<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2916\n",
      "Precision: 0.6627, Recall: 0.6034, F1-Score: 0.6149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.46      0.54      6000\n",
      "           1       0.75      0.50      0.60      6000\n",
      "           2       0.80      0.51      0.62      6000\n",
      "           3       0.51      0.43      0.46      6000\n",
      "           4       0.76      0.77      0.76      6000\n",
      "           5       0.94      0.70      0.80      6000\n",
      "           6       0.30      0.55      0.39      6000\n",
      "           7       0.44      0.77      0.56      6000\n",
      "           8       0.68      0.67      0.68      6000\n",
      "           9       0.77      0.68      0.72      6000\n",
      "\n",
      "    accuracy                           0.60     60000\n",
      "   macro avg       0.66      0.60      0.61     60000\n",
      "weighted avg       0.66      0.60      0.61     60000\n",
      "\n",
      "#Module 2 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:06<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0775\n",
      "Precision: 0.6673, Recall: 0.6577, F1-Score: 0.6577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55      6000\n",
      "           1       0.73      0.61      0.67      6000\n",
      "           2       0.71      0.72      0.72      6000\n",
      "           3       0.50      0.52      0.51      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.89      0.78      0.83      6000\n",
      "           6       0.56      0.43      0.49      6000\n",
      "           7       0.51      0.75      0.61      6000\n",
      "           8       0.64      0.76      0.69      6000\n",
      "           9       0.76      0.70      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23951556431424903\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38943184746636283, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.3888257344563802, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.39474444919162327, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.38929663764105904, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.3897633022732205, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3936784532335069, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.3868912590874566, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.3931342230902778, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.39171473185221356, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.39579476250542533, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.3875439961751302, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39343685574001735, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3972706264919705, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.3885108100043403, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.3988541497124566, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.3902091979980469, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.39056311713324654, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.3870374891493056, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.38576338026258683, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.39681074354383683, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.393646240234375, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3858316209581163, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.3901295132107205, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.38171301947699654, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3861033121744792, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:05<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1618\n",
      "Precision: 0.6530, Recall: 0.6331, F1-Score: 0.6362\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.52      0.55      6000\n",
      "           1       0.76      0.51      0.61      6000\n",
      "           2       0.69      0.71      0.70      6000\n",
      "           3       0.48      0.49      0.49      6000\n",
      "           4       0.80      0.74      0.77      6000\n",
      "           5       0.92      0.71      0.80      6000\n",
      "           6       0.43      0.47      0.45      6000\n",
      "           7       0.51      0.74      0.60      6000\n",
      "           8       0.62      0.74      0.68      6000\n",
      "           9       0.74      0.70      0.72      6000\n",
      "\n",
      "    accuracy                           0.63     60000\n",
      "   macro avg       0.65      0.63      0.64     60000\n",
      "weighted avg       0.65      0.63      0.64     60000\n",
      "\n",
      "#Module 3 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:30<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0758\n",
      "Precision: 0.6678, Recall: 0.6579, F1-Score: 0.6591\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.56      0.56      6000\n",
      "           1       0.73      0.62      0.67      6000\n",
      "           2       0.71      0.74      0.72      6000\n",
      "           3       0.50      0.53      0.51      6000\n",
      "           4       0.81      0.75      0.78      6000\n",
      "           5       0.90      0.75      0.82      6000\n",
      "           6       0.53      0.44      0.48      6000\n",
      "           7       0.53      0.74      0.62      6000\n",
      "           8       0.67      0.73      0.70      6000\n",
      "           9       0.74      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23831618744075975\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.39021894666883683, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.3879534403483073, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.3878305223253038, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.38962427775065106, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.3875817192925347, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3914315965440538, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.39216698540581596, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.387794918484158, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.38699044121636283, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.3955112033420139, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.39406416151258683, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39574559529622394, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3945278591579861, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.3929528130425347, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.38697263929578996, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.3913688659667969, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.38679207695855033, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.38562774658203125, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.39020199245876735, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.38469229804144967, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.39437781439887154, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3851356506347656, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.38750796847873265, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.38496992323133683, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3900112575954861, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:17<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1515\n",
      "Precision: 0.6589, Recall: 0.6357, F1-Score: 0.6413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.52      0.56      6000\n",
      "           1       0.69      0.64      0.66      6000\n",
      "           2       0.75      0.67      0.70      6000\n",
      "           3       0.47      0.53      0.49      6000\n",
      "           4       0.84      0.66      0.74      6000\n",
      "           5       0.92      0.69      0.79      6000\n",
      "           6       0.43      0.48      0.45      6000\n",
      "           7       0.51      0.74      0.61      6000\n",
      "           8       0.67      0.71      0.69      6000\n",
      "           9       0.72      0.73      0.72      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.64     60000\n",
      "weighted avg       0.66      0.64      0.64     60000\n",
      "\n",
      "#Module 4 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:22<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0837\n",
      "Precision: 0.6717, Recall: 0.6583, F1-Score: 0.6552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.52      0.55      6000\n",
      "           1       0.76      0.59      0.66      6000\n",
      "           2       0.72      0.71      0.72      6000\n",
      "           3       0.53      0.49      0.51      6000\n",
      "           4       0.77      0.82      0.80      6000\n",
      "           5       0.90      0.79      0.84      6000\n",
      "           6       0.64      0.37      0.47      6000\n",
      "           7       0.51      0.75      0.61      6000\n",
      "           8       0.56      0.80      0.66      6000\n",
      "           9       0.73      0.74      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23819031930239531\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38824505276150173, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.38803015814887154, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.3926408555772569, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.3871951633029514, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.39125357733832467, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3929795159233941, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.38890626695421004, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.39457405938042533, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.3917219373914931, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.39370642768012154, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.39770550198025173, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39059702555338544, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.39267942640516496, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.39373736911349827, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.3887006971571181, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.3953772650824653, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.3944668240017361, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.3892563713921441, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.38633770412868923, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.3875389099121094, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.38772031995985246, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3842790391710069, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.3873104519314236, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.38374752468532985, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3828582763671875, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:14<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1233\n",
      "Precision: 0.6501, Recall: 0.6481, F1-Score: 0.6430\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.47      0.54      6000\n",
      "           1       0.62      0.71      0.66      6000\n",
      "           2       0.72      0.70      0.71      6000\n",
      "           3       0.45      0.53      0.49      6000\n",
      "           4       0.66      0.87      0.75      6000\n",
      "           5       0.90      0.77      0.83      6000\n",
      "           6       0.52      0.32      0.40      6000\n",
      "           7       0.63      0.66      0.64      6000\n",
      "           8       0.66      0.69      0.68      6000\n",
      "           9       0.71      0.74      0.72      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.65      0.65      0.64     60000\n",
      "weighted avg       0.65      0.65      0.64     60000\n",
      "\n",
      "#Module 5 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [10:31<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0485\n",
      "Precision: 0.6713, Recall: 0.6681, F1-Score: 0.6677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.59      0.56      6000\n",
      "           1       0.70      0.68      0.69      6000\n",
      "           2       0.72      0.73      0.72      6000\n",
      "           3       0.55      0.47      0.51      6000\n",
      "           4       0.82      0.78      0.80      6000\n",
      "           5       0.89      0.79      0.84      6000\n",
      "           6       0.52      0.45      0.48      6000\n",
      "           7       0.58      0.72      0.64      6000\n",
      "           8       0.66      0.74      0.69      6000\n",
      "           9       0.74      0.73      0.73      6000\n",
      "\n",
      "    accuracy                           0.67     60000\n",
      "   macro avg       0.67      0.67      0.67     60000\n",
      "weighted avg       0.67      0.67      0.67     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23859313420457742\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38743548923068577, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.38673951890733504, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.38894695705837673, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.39018800523546004, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.38538826836480033, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3936042785644531, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.38945431179470485, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.38681920369466144, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.3885917663574219, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.39189232720269096, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.39469655354817706, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.38855234781901044, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3907911512586806, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.39172617594401044, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.39041095309787327, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.38908343844943577, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.39205678304036456, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.387534671359592, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.38883887396918404, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.38773345947265625, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.39375220404730904, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3764682345920139, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.3879852294921875, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.3965805901421441, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3814103868272569, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:59<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1950\n",
      "Precision: 0.6599, Recall: 0.6353, F1-Score: 0.6380\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55      6000\n",
      "           1       0.79      0.49      0.60      6000\n",
      "           2       0.73      0.68      0.70      6000\n",
      "           3       0.55      0.44      0.49      6000\n",
      "           4       0.79      0.78      0.79      6000\n",
      "           5       0.93      0.72      0.81      6000\n",
      "           6       0.40      0.49      0.44      6000\n",
      "           7       0.50      0.75      0.60      6000\n",
      "           8       0.61      0.76      0.68      6000\n",
      "           9       0.74      0.71      0.73      6000\n",
      "\n",
      "    accuracy                           0.64     60000\n",
      "   macro avg       0.66      0.64      0.64     60000\n",
      "weighted avg       0.66      0.64      0.64     60000\n",
      "\n",
      "#Module 6 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [09:14<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0669\n",
      "Precision: 0.6718, Recall: 0.6635, F1-Score: 0.6634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.57      0.56      6000\n",
      "           1       0.72      0.65      0.69      6000\n",
      "           2       0.72      0.72      0.72      6000\n",
      "           3       0.55      0.49      0.52      6000\n",
      "           4       0.78      0.80      0.79      6000\n",
      "           5       0.91      0.77      0.83      6000\n",
      "           6       0.56      0.44      0.49      6000\n",
      "           7       0.52      0.74      0.62      6000\n",
      "           8       0.64      0.75      0.69      6000\n",
      "           9       0.77      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.67      0.66      0.66     60000\n",
      "weighted avg       0.67      0.66      0.66     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23894869889284792\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.3892173767089844, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.3871748182508681, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.3948482937282986, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.3871985541449653, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.3906012641059028, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.39318932427300346, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.3945227728949653, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.39354875352647567, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.39348729451497394, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.39481014675564235, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.3926849365234375, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39408238728841144, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3990033467610677, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.39429812961154515, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.3898518880208333, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.39284303453233504, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.3954196506076389, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.3843604193793403, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.39484575059678817, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.38825395372178817, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.38894017537434894, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.38308800591362846, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.39163631863064235, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.391060299343533, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3823564317491319, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [08:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3488\n",
      "Precision: 0.6500, Recall: 0.5981, F1-Score: 0.5980\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.49      0.53      6000\n",
      "           1       0.81      0.42      0.55      6000\n",
      "           2       0.79      0.50      0.61      6000\n",
      "           3       0.49      0.47      0.48      6000\n",
      "           4       0.67      0.81      0.74      6000\n",
      "           5       0.87      0.74      0.80      6000\n",
      "           6       0.58      0.37      0.45      6000\n",
      "           7       0.45      0.73      0.55      6000\n",
      "           8       0.43      0.85      0.57      6000\n",
      "           9       0.83      0.59      0.69      6000\n",
      "\n",
      "    accuracy                           0.60     60000\n",
      "   macro avg       0.65      0.60      0.60     60000\n",
      "weighted avg       0.65      0.60      0.60     60000\n",
      "\n",
      "#Module 7 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [08:05<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0543\n",
      "Precision: 0.6752, Recall: 0.6632, F1-Score: 0.6652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56      6000\n",
      "           1       0.76      0.60      0.67      6000\n",
      "           2       0.72      0.74      0.73      6000\n",
      "           3       0.50      0.52      0.51      6000\n",
      "           4       0.83      0.77      0.80      6000\n",
      "           5       0.89      0.80      0.84      6000\n",
      "           6       0.50      0.47      0.48      6000\n",
      "           7       0.52      0.77      0.62      6000\n",
      "           8       0.71      0.69      0.70      6000\n",
      "           9       0.75      0.73      0.74      6000\n",
      "\n",
      "    accuracy                           0.66     60000\n",
      "   macro avg       0.68      0.66      0.67     60000\n",
      "weighted avg       0.68      0.66      0.67     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.238317181570796\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38917626274956596, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.3878902859157986, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.38882742987738717, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.3883344862196181, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.39115142822265625, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3831308152940538, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.39562013414171004, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.3906979031032986, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.3856883578830295, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.3935173882378472, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.3897505866156684, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39571380615234375, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3904401991102431, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.39441553751627606, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.39656109280056423, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.39206695556640625, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.39030371771918404, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.38810136583116317, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.3885921902126736, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.3765525817871094, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.39334657457139754, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3844795227050781, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.39236874050564235, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.38425784640842015, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3943939208984375, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:32<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1525\n",
      "Precision: 0.6625, Recall: 0.6343, F1-Score: 0.6381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.54      6000\n",
      "           1       0.80      0.47      0.59      6000\n",
      "           2       0.77      0.62      0.69      6000\n",
      "           3       0.42      0.58      0.48      6000\n",
      "           4       0.81      0.75      0.78      6000\n",
      "           5       0.89      0.77      0.83      6000\n",
      "           6       0.44      0.45      0.45      6000\n",
      "           7       0.51      0.76      0.61      6000\n",
      "           8       0.70      0.68      0.69      6000\n",
      "           9       0.67      0.76      0.72      6000\n",
      "\n",
      "    accuracy                           0.63     60000\n",
      "   macro avg       0.66      0.63      0.64     60000\n",
      "weighted avg       0.66      0.63      0.64     60000\n",
      "\n",
      "#Module 8 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:41<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0986\n",
      "Precision: 0.6658, Recall: 0.6509, F1-Score: 0.6499\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.49      0.55      6000\n",
      "           1       0.75      0.59      0.66      6000\n",
      "           2       0.73      0.69      0.71      6000\n",
      "           3       0.54      0.48      0.51      6000\n",
      "           4       0.74      0.83      0.78      6000\n",
      "           5       0.90      0.78      0.83      6000\n",
      "           6       0.57      0.42      0.48      6000\n",
      "           7       0.47      0.76      0.58      6000\n",
      "           8       0.58      0.78      0.67      6000\n",
      "           9       0.77      0.69      0.73      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.67      0.65      0.65     60000\n",
      "weighted avg       0.67      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.2379658722250662\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.38804033067491317, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.38727908664279515, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.3956430223253038, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.3864911397298177, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.3857493930392795, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.39176856146918404, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.3911404079861111, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.3862372504340278, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.39465586344401044, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.3947436014811198, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.38934410942925346, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39425023396809894, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3960227966308594, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.3897942437065972, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.387687259250217, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.39195844862196183, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.39190673828125, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.38416120741102433, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.3953141106499566, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.388100094265408, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.38989893595377606, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3758803473578559, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.38694805569118923, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.3966013590494792, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3835483127170139, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:16<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2261\n",
      "Precision: 0.6488, Recall: 0.6186, F1-Score: 0.6186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.49      0.53      6000\n",
      "           1       0.74      0.59      0.66      6000\n",
      "           2       0.77      0.57      0.65      6000\n",
      "           3       0.56      0.40      0.47      6000\n",
      "           4       0.69      0.82      0.75      6000\n",
      "           5       0.94      0.71      0.81      6000\n",
      "           6       0.51      0.40      0.45      6000\n",
      "           7       0.49      0.71      0.58      6000\n",
      "           8       0.45      0.84      0.58      6000\n",
      "           9       0.76      0.67      0.71      6000\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.65      0.62      0.62     60000\n",
      "weighted avg       0.65      0.62      0.62     60000\n",
      "\n",
      "#Module 9 in progress....\n",
      "after head prune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:21<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1120\n",
      "Precision: 0.6573, Recall: 0.6471, F1-Score: 0.6480\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55      6000\n",
      "           1       0.74      0.61      0.67      6000\n",
      "           2       0.70      0.71      0.71      6000\n",
      "           3       0.53      0.46      0.49      6000\n",
      "           4       0.79      0.76      0.78      6000\n",
      "           5       0.90      0.75      0.82      6000\n",
      "           6       0.48      0.45      0.46      6000\n",
      "           7       0.50      0.72      0.59      6000\n",
      "           8       0.64      0.74      0.69      6000\n",
      "           9       0.73      0.72      0.72      6000\n",
      "\n",
      "    accuracy                           0.65     60000\n",
      "   macro avg       0.66      0.65      0.65     60000\n",
      "weighted avg       0.66      0.65      0.65     60000\n",
      "\n",
      "after removing weights\n",
      "after CI\n",
      "after TI\n",
      "0.23821603685678316\n",
      "{'bert.embeddings.word_embeddings.weight': 0.0, 'bert.embeddings.position_embeddings.weight': 0.0, 'bert.embeddings.token_type_embeddings.weight': 0.0, 'bert.embeddings.LayerNorm.weight': 0.0, 'bert.embeddings.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.attention.self.query.weight': 0.0, 'bert.encoder.layer.0.attention.self.query.bias': 0.0, 'bert.encoder.layer.0.attention.self.key.weight': 0.0, 'bert.encoder.layer.0.attention.self.key.bias': 0.0, 'bert.encoder.layer.0.attention.self.value.weight': 0.0, 'bert.encoder.layer.0.attention.self.value.bias': 0.0, 'bert.encoder.layer.0.attention.output.dense.weight': 0.0, 'bert.encoder.layer.0.attention.output.dense.bias': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.0.intermediate.dense.weight': 0.3886617024739583, 'bert.encoder.layer.0.intermediate.dense.bias': 0.0, 'bert.encoder.layer.0.output.dense.weight': 0.38881810506184894, 'bert.encoder.layer.0.output.dense.bias': 0.0, 'bert.encoder.layer.0.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.0.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.attention.self.query.weight': 0.0, 'bert.encoder.layer.1.attention.self.query.bias': 0.0, 'bert.encoder.layer.1.attention.self.key.weight': 0.0, 'bert.encoder.layer.1.attention.self.key.bias': 0.0, 'bert.encoder.layer.1.attention.self.value.weight': 0.0, 'bert.encoder.layer.1.attention.self.value.bias': 0.0, 'bert.encoder.layer.1.attention.output.dense.weight': 0.0, 'bert.encoder.layer.1.attention.output.dense.bias': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.1.intermediate.dense.weight': 0.3946668836805556, 'bert.encoder.layer.1.intermediate.dense.bias': 0.0, 'bert.encoder.layer.1.output.dense.weight': 0.38395775689019096, 'bert.encoder.layer.1.output.dense.bias': 0.0, 'bert.encoder.layer.1.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.1.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.attention.self.query.weight': 0.0, 'bert.encoder.layer.2.attention.self.query.bias': 0.0, 'bert.encoder.layer.2.attention.self.key.weight': 0.0, 'bert.encoder.layer.2.attention.self.key.bias': 0.0, 'bert.encoder.layer.2.attention.self.value.weight': 0.0, 'bert.encoder.layer.2.attention.self.value.bias': 0.0, 'bert.encoder.layer.2.attention.output.dense.weight': 0.0, 'bert.encoder.layer.2.attention.output.dense.bias': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.2.intermediate.dense.weight': 0.3918529086642795, 'bert.encoder.layer.2.intermediate.dense.bias': 0.0, 'bert.encoder.layer.2.output.dense.weight': 0.3900722927517361, 'bert.encoder.layer.2.output.dense.bias': 0.0, 'bert.encoder.layer.2.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.2.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.attention.self.query.weight': 0.0, 'bert.encoder.layer.3.attention.self.query.bias': 0.0, 'bert.encoder.layer.3.attention.self.key.weight': 0.0, 'bert.encoder.layer.3.attention.self.key.bias': 0.0, 'bert.encoder.layer.3.attention.self.value.weight': 0.0, 'bert.encoder.layer.3.attention.self.value.bias': 0.0, 'bert.encoder.layer.3.attention.output.dense.weight': 0.0, 'bert.encoder.layer.3.attention.output.dense.bias': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.3.intermediate.dense.weight': 0.3856709798177083, 'bert.encoder.layer.3.intermediate.dense.bias': 0.0, 'bert.encoder.layer.3.output.dense.weight': 0.389046139187283, 'bert.encoder.layer.3.output.dense.bias': 0.0, 'bert.encoder.layer.3.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.3.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.attention.self.query.weight': 0.0, 'bert.encoder.layer.4.attention.self.query.bias': 0.0, 'bert.encoder.layer.4.attention.self.key.weight': 0.0, 'bert.encoder.layer.4.attention.self.key.bias': 0.0, 'bert.encoder.layer.4.attention.self.value.weight': 0.0, 'bert.encoder.layer.4.attention.self.value.bias': 0.0, 'bert.encoder.layer.4.attention.output.dense.weight': 0.0, 'bert.encoder.layer.4.attention.output.dense.bias': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.4.intermediate.dense.weight': 0.3906063503689236, 'bert.encoder.layer.4.intermediate.dense.bias': 0.0, 'bert.encoder.layer.4.output.dense.weight': 0.39466603597005206, 'bert.encoder.layer.4.output.dense.bias': 0.0, 'bert.encoder.layer.4.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.4.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.attention.self.query.weight': 0.0, 'bert.encoder.layer.5.attention.self.query.bias': 0.0, 'bert.encoder.layer.5.attention.self.key.weight': 0.0, 'bert.encoder.layer.5.attention.self.key.bias': 0.0, 'bert.encoder.layer.5.attention.self.value.weight': 0.0, 'bert.encoder.layer.5.attention.self.value.bias': 0.0, 'bert.encoder.layer.5.attention.output.dense.weight': 0.0, 'bert.encoder.layer.5.attention.output.dense.bias': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.5.intermediate.dense.weight': 0.3875206841362847, 'bert.encoder.layer.5.intermediate.dense.bias': 0.0, 'bert.encoder.layer.5.output.dense.weight': 0.39177025689019096, 'bert.encoder.layer.5.output.dense.bias': 0.0, 'bert.encoder.layer.5.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.5.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.attention.self.query.weight': 0.0, 'bert.encoder.layer.6.attention.self.query.bias': 0.0, 'bert.encoder.layer.6.attention.self.key.weight': 0.0, 'bert.encoder.layer.6.attention.self.key.bias': 0.0, 'bert.encoder.layer.6.attention.self.value.weight': 0.0, 'bert.encoder.layer.6.attention.self.value.bias': 0.0, 'bert.encoder.layer.6.attention.output.dense.weight': 0.0, 'bert.encoder.layer.6.attention.output.dense.bias': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.6.intermediate.dense.weight': 0.3927599589029948, 'bert.encoder.layer.6.intermediate.dense.bias': 0.0, 'bert.encoder.layer.6.output.dense.weight': 0.39116795857747394, 'bert.encoder.layer.6.output.dense.bias': 0.0, 'bert.encoder.layer.6.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.6.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.attention.self.query.weight': 0.0, 'bert.encoder.layer.7.attention.self.query.bias': 0.0, 'bert.encoder.layer.7.attention.self.key.weight': 0.0, 'bert.encoder.layer.7.attention.self.key.bias': 0.0, 'bert.encoder.layer.7.attention.self.value.weight': 0.0, 'bert.encoder.layer.7.attention.self.value.bias': 0.0, 'bert.encoder.layer.7.attention.output.dense.weight': 0.0, 'bert.encoder.layer.7.attention.output.dense.bias': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.7.intermediate.dense.weight': 0.39072248670789933, 'bert.encoder.layer.7.intermediate.dense.bias': 0.0, 'bert.encoder.layer.7.output.dense.weight': 0.38945049709743923, 'bert.encoder.layer.7.output.dense.bias': 0.0, 'bert.encoder.layer.7.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.7.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.attention.self.query.weight': 0.0, 'bert.encoder.layer.8.attention.self.query.bias': 0.0, 'bert.encoder.layer.8.attention.self.key.weight': 0.0, 'bert.encoder.layer.8.attention.self.key.bias': 0.0, 'bert.encoder.layer.8.attention.self.value.weight': 0.0, 'bert.encoder.layer.8.attention.self.value.bias': 0.0, 'bert.encoder.layer.8.attention.output.dense.weight': 0.0, 'bert.encoder.layer.8.attention.output.dense.bias': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.8.intermediate.dense.weight': 0.38749991522894967, 'bert.encoder.layer.8.intermediate.dense.bias': 0.0, 'bert.encoder.layer.8.output.dense.weight': 0.3823924594455295, 'bert.encoder.layer.8.output.dense.bias': 0.0, 'bert.encoder.layer.8.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.8.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.attention.self.query.weight': 0.0, 'bert.encoder.layer.9.attention.self.query.bias': 0.0, 'bert.encoder.layer.9.attention.self.key.weight': 0.0, 'bert.encoder.layer.9.attention.self.key.bias': 0.0, 'bert.encoder.layer.9.attention.self.value.weight': 0.0, 'bert.encoder.layer.9.attention.self.value.bias': 0.0, 'bert.encoder.layer.9.attention.output.dense.weight': 0.0, 'bert.encoder.layer.9.attention.output.dense.bias': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.9.intermediate.dense.weight': 0.38782374064127606, 'bert.encoder.layer.9.intermediate.dense.bias': 0.0, 'bert.encoder.layer.9.output.dense.weight': 0.3996908399793837, 'bert.encoder.layer.9.output.dense.bias': 0.0, 'bert.encoder.layer.9.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.9.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.attention.self.query.weight': 0.0, 'bert.encoder.layer.10.attention.self.query.bias': 0.0, 'bert.encoder.layer.10.attention.self.key.weight': 0.0, 'bert.encoder.layer.10.attention.self.key.bias': 0.0, 'bert.encoder.layer.10.attention.self.value.weight': 0.0, 'bert.encoder.layer.10.attention.self.value.bias': 0.0, 'bert.encoder.layer.10.attention.output.dense.weight': 0.0, 'bert.encoder.layer.10.attention.output.dense.bias': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.10.intermediate.dense.weight': 0.3882463243272569, 'bert.encoder.layer.10.intermediate.dense.bias': 0.0, 'bert.encoder.layer.10.output.dense.weight': 0.3917376200358073, 'bert.encoder.layer.10.output.dense.bias': 0.0, 'bert.encoder.layer.10.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.10.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.attention.self.query.weight': 0.0, 'bert.encoder.layer.11.attention.self.query.bias': 0.0, 'bert.encoder.layer.11.attention.self.key.weight': 0.0, 'bert.encoder.layer.11.attention.self.key.bias': 0.0, 'bert.encoder.layer.11.attention.self.value.weight': 0.0, 'bert.encoder.layer.11.attention.self.value.bias': 0.0, 'bert.encoder.layer.11.attention.output.dense.weight': 0.0, 'bert.encoder.layer.11.attention.output.dense.bias': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.attention.output.LayerNorm.bias': 0.0, 'bert.encoder.layer.11.intermediate.dense.weight': 0.39035118950737846, 'bert.encoder.layer.11.intermediate.dense.bias': 0.0, 'bert.encoder.layer.11.output.dense.weight': 0.380462646484375, 'bert.encoder.layer.11.output.dense.bias': 0.0, 'bert.encoder.layer.11.output.LayerNorm.weight': 0.0, 'bert.encoder.layer.11.output.LayerNorm.bias': 0.0, 'bert.pooler.dense.weight': 0.3838297526041667, 'bert.pooler.dense.bias': 0.0, 'classifier.weight': 0.0, 'classifier.bias': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:09<00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2178\n",
      "Precision: 0.6493, Recall: 0.6205, F1-Score: 0.6252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.49      0.54      6000\n",
      "           1       0.77      0.50      0.61      6000\n",
      "           2       0.71      0.67      0.69      6000\n",
      "           3       0.56      0.40      0.47      6000\n",
      "           4       0.76      0.76      0.76      6000\n",
      "           5       0.90      0.72      0.80      6000\n",
      "           6       0.32      0.52      0.40      6000\n",
      "           7       0.53      0.68      0.60      6000\n",
      "           8       0.63      0.71      0.67      6000\n",
      "           9       0.67      0.75      0.71      6000\n",
      "\n",
      "    accuracy                           0.62     60000\n",
      "   macro avg       0.65      0.62      0.63     60000\n",
      "weighted avg       0.65      0.62      0.63     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "    \n",
    "    positive_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "    \n",
    "    all_samples = sampling_class(\n",
    "        train_dataloader, 200, 20, num_labels, False, 4, device=device\n",
    "    )\n",
    "    \n",
    "    print(\"after head prune\")\n",
    "    evaluate_model(model, model_config, test_dataloader)\n",
    "    \n",
    "    module = copy.deepcopy(model)\n",
    "    wr = WeightRemoverBert(model, p=0.9)\n",
    "    ci = ConcernIdentificationBert(model, p=0.5)\n",
    "    ti = TanglingIdentification(model, p=0.6)\n",
    "    \n",
    "    print(\"after removing weights\")\n",
    "    \n",
    "    eval_step = 5\n",
    "    for idx, batch in enumerate(all_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            wr.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    \n",
    "    print(\"after CI\")\n",
    "    \n",
    "    for idx, batch in enumerate(positive_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            ci.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    \n",
    "    print(\"after TI\")\n",
    "    \n",
    "    for idx, batch in enumerate(negative_samples):\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            ti.propagate(module, input_ids)\n",
    "        if idx % eval_step:\n",
    "            # result = evaluate_model(module, model_config, test_dataloader)\n",
    "            pass\n",
    "    a, b = get_sparsity(module)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    result = evaluate_model(module, model_config, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f57b309-29c9-4dd4-af34-290cca6ede44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/Minwoo/LESN/Decompose/DecomposeTransformer/Models/Configs/classification/fabriceyhc/bert-base-uncased-yahoo_answers_topics exists.\n",
      "Loading the model.\n",
      "The model fabriceyhc/bert-base-uncased-yahoo_answers_topics is loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1875/1875 [07:48<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0044\n",
      "Precision: 0.6874, Recall: 0.6865, F1-Score: 0.6839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57      6000\n",
      "           1       0.74      0.66      0.69      6000\n",
      "           2       0.71      0.78      0.74      6000\n",
      "           3       0.54      0.53      0.53      6000\n",
      "           4       0.80      0.82      0.81      6000\n",
      "           5       0.90      0.84      0.87      6000\n",
      "           6       0.61      0.43      0.50      6000\n",
      "           7       0.62      0.73      0.67      6000\n",
      "           8       0.64      0.76      0.70      6000\n",
      "           9       0.75      0.75      0.75      6000\n",
      "\n",
      "    accuracy                           0.69     60000\n",
      "   macro avg       0.69      0.69      0.68     60000\n",
      "weighted avg       0.69      0.69      0.68     60000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.0044152938524882,\n",
       " 'precision': 0.6874371369976651,\n",
       " 'recall': 0.6865,\n",
       " 'f1_score': 0.6838781660446563,\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n           0       0.57      0.57      0.57      6000\\n           1       0.74      0.66      0.69      6000\\n           2       0.71      0.78      0.74      6000\\n           3       0.54      0.53      0.53      6000\\n           4       0.80      0.82      0.81      6000\\n           5       0.90      0.84      0.87      6000\\n           6       0.61      0.43      0.50      6000\\n           7       0.62      0.73      0.67      6000\\n           8       0.64      0.76      0.70      6000\\n           9       0.75      0.75      0.75      6000\\n\\n    accuracy                           0.69     60000\\n   macro avg       0.69      0.69      0.68     60000\\nweighted avg       0.69      0.69      0.68     60000\\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer, checkpoint = load_model(model_config)\n",
    "evaluate_model(model, model_config, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
