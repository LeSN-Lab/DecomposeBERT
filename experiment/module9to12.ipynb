{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d40e255-4d96-4feb-8cfd-d1dcdc3101a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d81d918-e9be-4901-b232-b14f3d99c421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/minwoo/LESN/DecomposeBERT/experiment'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95f388e7-7fe7-44a1-8b68-19b2e78c19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/minwoo/LESN/DecomposeBERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a759425-116a-475b-957f-55809410876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 15:40:17.451614: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/minwoo/anaconda3/envs/DecomposeBERT/lib/python3.8/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from utils.model_utils.evaluate import evaluate_model, test_f1, test\n",
    "\n",
    "from utils.model_utils.load_model import *\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "from utils.dataset_utils.load_dataset import load_dataset\n",
    "from utils.decompose_utils.weight_remover import WeightRemoverBert\n",
    "from utils.decompose_utils.concern_identification import ConcernIdentificationBert\n",
    "from utils.decompose_utils.tangling_identification import TanglingIdentification\n",
    "from transformers import AutoConfig\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from pprint import pp\n",
    "from utils.decompose_utils.sampling import sampling_class\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c44dcd0a-95e0-4626-9869-cf640c91344b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Loading preprocessed data...\n",
      "Loading preprocessed data...\n",
      "Loading preprocessed data...\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"prajjwal1/bert-tiny\"\n",
    "# model_dir = \"tiny bert\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "model_dir = \"bert\"\n",
    "data = \"SDG\"\n",
    "num_labels = 16\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_name = \"best_model.pt\"\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model_config = ModelConfig(\n",
    "    _model_name=model_name,\n",
    "    _model_dir=model_dir,\n",
    "    _data=data,\n",
    "    _is_pretrained=True,\n",
    "    _transformer_config=config,\n",
    "    _checkpoint_name=checkpoint_name,\n",
    "    _device=device,\n",
    ")\n",
    "model, tokenizer, checkpoint = load_classification_model(model_config)\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader = load_dataset(\n",
    "    model_config, tokenizer, batch_size=32, test_size=0.3\n",
    ")\n",
    "# print(\"Start Time:\" + datetime.now().strftime(\"%H:%M:%S\"))\n",
    "# evaluate_model(model, model_config, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc014e64-fe25-46e0-8732-0ede01cfa615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Module 4 in progress....\n",
      "origin\n",
      "0\n",
      "{'precision': 0.8224719101123595,\n",
      " 'recall': 0.8433179723502304,\n",
      " 'f1-score': 0.832764505119454,\n",
      " 'support': 434.0}\n",
      "Start Positive CI sparse\n",
      "1\n",
      "{'precision': 0.8317307692307693,\n",
      " 'recall': 0.7972350230414746,\n",
      " 'f1-score': 0.8141176470588236,\n",
      " 'support': 434.0}\n",
      "2\n",
      "{'precision': 0.8390243902439024,\n",
      " 'recall': 0.7926267281105991,\n",
      " 'f1-score': 0.8151658767772512,\n",
      " 'support': 434.0}\n",
      "3\n",
      "{'precision': 0.8493827160493828,\n",
      " 'recall': 0.7926267281105991,\n",
      " 'f1-score': 0.8200238379022646,\n",
      " 'support': 434.0}\n",
      "4\n",
      "{'precision': 0.8511166253101737,\n",
      " 'recall': 0.7903225806451613,\n",
      " 'f1-score': 0.8195937873357229,\n",
      " 'support': 434.0}\n",
      "5\n",
      "{'precision': 0.8528678304239401,\n",
      " 'recall': 0.7880184331797235,\n",
      " 'f1-score': 0.8191616766467066,\n",
      " 'support': 434.0}\n",
      "Start Positive CI after sparse\n",
      "6\n",
      "{'precision': 0.8484107579462102,\n",
      " 'recall': 0.7995391705069125,\n",
      " 'f1-score': 0.8232502965599051,\n",
      " 'support': 434.0}\n",
      "7\n",
      "{'precision': 0.7645788336933045,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.7892976588628763,\n",
      " 'support': 434.0}\n",
      "8\n",
      "{'precision': 0.7941176470588235,\n",
      " 'recall': 0.8087557603686636,\n",
      " 'f1-score': 0.8013698630136986,\n",
      " 'support': 434.0}\n",
      "9\n",
      "{'precision': 0.7725321888412017,\n",
      " 'recall': 0.8294930875576036,\n",
      " 'f1-score': 0.7999999999999999,\n",
      " 'support': 434.0}\n",
      "10\n",
      "{'precision': 0.7563025210084033,\n",
      " 'recall': 0.8294930875576036,\n",
      " 'f1-score': 0.7912087912087912,\n",
      " 'support': 434.0}\n",
      "11\n",
      "{'precision': 0.7672413793103449,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7928730512249442,\n",
      " 'support': 434.0}\n",
      "12\n",
      "{'precision': 0.7705627705627706,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7946428571428571,\n",
      " 'support': 434.0}\n",
      "13\n",
      "{'precision': 0.7722342733188721,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7955307262569833,\n",
      " 'support': 434.0}\n",
      "14\n",
      "{'precision': 0.7722342733188721,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7955307262569833,\n",
      " 'support': 434.0}\n",
      "15\n",
      "{'precision': 0.7751091703056768,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7959641255605381,\n",
      " 'support': 434.0}\n",
      "16\n",
      "{'precision': 0.7751091703056768,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7959641255605381,\n",
      " 'support': 434.0}\n",
      "17\n",
      "{'precision': 0.7751091703056768,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7959641255605381,\n",
      " 'support': 434.0}\n",
      "18\n",
      "{'precision': 0.7751091703056768,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7959641255605381,\n",
      " 'support': 434.0}\n",
      "19\n",
      "{'precision': 0.7734204793028322,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7950727883538634,\n",
      " 'support': 434.0}\n",
      "20\n",
      "{'precision': 0.7734204793028322,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7950727883538634,\n",
      " 'support': 434.0}\n",
      "21\n",
      "{'precision': 0.7734204793028322,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7950727883538634,\n",
      " 'support': 434.0}\n",
      "Start Negative TI\n",
      "22\n",
      "{'precision': 0.5653495440729484,\n",
      " 'recall': 0.8571428571428571,\n",
      " 'f1-score': 0.6813186813186812,\n",
      " 'support': 434.0}\n",
      "23\n",
      "{'precision': 0.5670731707317073,\n",
      " 'recall': 0.8571428571428571,\n",
      " 'f1-score': 0.6825688073394495,\n",
      " 'support': 434.0}\n",
      "24\n",
      "{'precision': 0.5679389312977099,\n",
      " 'recall': 0.8571428571428571,\n",
      " 'f1-score': 0.6831955922865013,\n",
      " 'support': 434.0}\n",
      "25\n",
      "{'precision': 0.577639751552795,\n",
      " 'recall': 0.8571428571428571,\n",
      " 'f1-score': 0.6901669758812615,\n",
      " 'support': 434.0}\n",
      "26\n",
      "{'precision': 0.5769828926905132,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.6889507892293406,\n",
      " 'support': 434.0}\n",
      "27\n",
      "{'precision': 0.5749613601236476,\n",
      " 'recall': 0.8571428571428571,\n",
      " 'f1-score': 0.6882516188714154,\n",
      " 'support': 434.0}\n",
      "28\n",
      "{'precision': 0.5725308641975309,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.6857670979667282,\n",
      " 'support': 434.0}\n",
      "29\n",
      "{'precision': 0.5754276827371695,\n",
      " 'recall': 0.8525345622119815,\n",
      " 'f1-score': 0.6870937790157845,\n",
      " 'support': 434.0}\n",
      "30\n",
      "{'precision': 0.5743034055727554,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.687037037037037,\n",
      " 'support': 434.0}\n",
      "31\n",
      "{'precision': 0.5751937984496124,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.6876737720111213,\n",
      " 'support': 434.0}\n",
      "32\n",
      "{'precision': 0.5751937984496124,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.6876737720111213,\n",
      " 'support': 434.0}\n",
      "33\n",
      "{'precision': 0.5751937984496124,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.6876737720111213,\n",
      " 'support': 434.0}\n",
      "34\n",
      "{'precision': 0.5751937984496124,\n",
      " 'recall': 0.8548387096774194,\n",
      " 'f1-score': 0.6876737720111213,\n",
      " 'support': 434.0}\n",
      "35\n",
      "{'precision': 0.5754276827371695,\n",
      " 'recall': 0.8525345622119815,\n",
      " 'f1-score': 0.6870937790157845,\n",
      " 'support': 434.0}\n",
      "36\n",
      "{'precision': 0.5790297339593115,\n",
      " 'recall': 0.8525345622119815,\n",
      " 'f1-score': 0.689655172413793,\n",
      " 'support': 434.0}\n",
      "37\n",
      "{'precision': 0.5790297339593115,\n",
      " 'recall': 0.8525345622119815,\n",
      " 'f1-score': 0.689655172413793,\n",
      " 'support': 434.0}\n",
      "f([2359296, 2241792, 2171136, 1986048, 1986048, 1986048, 1920576, 1831461, 1738390, 1630108, 1536381, 1477513, 1466593, 1463806, 1463040, 1462851, 1462297, 1461826, 1461382, 1460971, 1460849, 1460470, 1532215, 1532529, 1532616, 1532688, 1532721, 1532750, 1532750, 1532750, 1532798, 1532840, 1532840, 1532840, 1532840, 1532876, 1532905, 1532905])\n",
      "\n",
      "\n",
      "f([2359296, 2075136, 2075136, 2075136, 2075136, 2075136, 1969743, 1879356, 1742807, 1631019, 1513529, 1448404, 1434653, 1428751, 1427976, 1427244, 1426945, 1426585, 1426547, 1426294, 1426262, 1425963, 1533146, 1533497, 1533544, 1533630, 1533633, 1533633, 1533681, 1533681, 1533681, 1533681, 1533681, 1533712, 1533749, 1533749, 1533833, 1533840])\n",
      "\n",
      "\n",
      "f([2359296, 2036736, 2036736, 2036736, 2036736, 2036736, 1939091, 1829026, 1756297, 1664777, 1575101, 1532515, 1526114, 1524791, 1524680, 1524530, 1524130, 1523963, 1523696, 1523086, 1523065, 1523042, 1628180, 1628984, 1629000, 1629035, 1629035, 1629035, 1629035, 1629083, 1629084, 1629084, 1629084, 1629091, 1629133, 1629154, 1629154, 1629154])\n",
      "\n",
      "\n",
      "f([2359296, 2048256, 2048256, 2048256, 2048256, 2048256, 1965931, 1847995, 1715915, 1606371, 1501318, 1448083, 1440496, 1439331, 1438918, 1438573, 1438498, 1438437, 1438229, 1438229, 1438081, 1438078, 1525646, 1527637, 1527738, 1527871, 1527898, 1527898, 1527951, 1528047, 1528047, 1528047, 1528047, 1528076, 1528076, 1528076, 1528101, 1528101])\n",
      "\n",
      "\n",
      "f([2359296, 2085120, 2085120, 2085120, 2085120, 2085120, 2000224, 1914436, 1788079, 1694156, 1586586, 1514806, 1503731, 1500615, 1499936, 1499168, 1499088, 1499021, 1498871, 1498737, 1498706, 1498706, 1585832, 1587871, 1588145, 1588487, 1588597, 1588639, 1588671, 1588878, 1588878, 1588912, 1588936, 1588936, 1588942, 1588942, 1588942, 1588944])\n",
      "\n",
      "\n",
      "f([2359296, 2127360, 2048256, 2048256, 2048256, 2048256, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2018182, 2030439, 2030835, 2031008, 2031053, 2031053, 2031053, 2031106, 2031229, 2031229, 2031229, 2031229, 2031282, 2031282, 2031282, 2031282, 2031282])\n",
      "\n",
      "\n",
      "f([2359296, 2111232, 2111232, 2111232, 2111232, 2111232, 2032031, 1929226, 1831130, 1724108, 1637915, 1574265, 1568703, 1567242, 1566855, 1566020, 1565695, 1565478, 1565478, 1565324, 1565324, 1565282, 1647943, 1650880, 1651454, 1651535, 1651673, 1651673, 1651799, 1652299, 1652340, 1652477, 1652568, 1652590, 1652590, 1652621, 1652621, 1652715])\n",
      "\n",
      "\n",
      "f([2359296, 2142720, 2069760, 2069760, 2069760, 2069760, 1971275, 1848225, 1736329, 1629456, 1542097, 1494344, 1485429, 1482783, 1482040, 1481599, 1481530, 1481074, 1480967, 1480664, 1480361, 1480255, 1586827, 1591301, 1591545, 1591682, 1591932, 1591932, 1592050, 1592495, 1592495, 1592495, 1592622, 1592622, 1592692, 1592692, 1592692, 1592725])\n",
      "\n",
      "\n",
      "f([2359296, 2161152, 2068224, 2068224, 2068224, 2068224, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2051440, 2065725, 2065739, 2065773, 2065773, 2065773, 2065773, 2065773, 2065790, 2065790, 2065790, 2065790, 2065790, 2065790, 2065790, 2065790, 2065790])\n",
      "\n",
      "\n",
      "f([2359296, 2157312, 2082816, 2082816, 2082816, 2082816, 1926093, 1827917, 1730763, 1653168, 1567791, 1493216, 1451525, 1449452, 1448241, 1447219, 1447093, 1446958, 1446905, 1446813, 1446813, 1446661, 1607857, 1615261, 1616063, 1616184, 1616424, 1616724, 1616730, 1617132, 1617132, 1617132, 1617212, 1617212, 1617212, 1617212, 1617212, 1617212])\n",
      "\n",
      "\n",
      "f([2359296, 2195712, 2113536, 2113536, 2113536, 2113536, 2015448, 1925993, 1835631, 1739919, 1626998, 1540757, 1484250, 1482225, 1481238, 1480563, 1480337, 1480234, 1480184, 1480149, 1480117, 1480041, 1578624, 1581789, 1582133, 1582297, 1582411, 1582441, 1582468, 1582640, 1582745, 1582745, 1582745, 1582745, 1582745, 1582745, 1582745, 1582745])\n",
      "\n",
      "\n",
      "f([2359296, 2227200, 2137344, 1744128, 1744128, 1744128, 1676899, 1592499, 1514854, 1439080, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1327417, 1402663, 1406364, 1407010, 1407339, 1407619, 1407699, 1407699, 1407897, 1408117, 1408219, 1408229, 1408288, 1408340, 1408340, 1408379, 1408379])\n",
      "\n",
      "\n",
      "f([2359296, 2285568, 2260992, 2190336, 2162688, 2150400, 2025330, 1908315, 1804683, 1687628, 1545032, 1399931, 1399931, 1399931, 1399931, 1399931, 1399931, 1399931, 1399931, 1399931, 1399931, 1399931, 1492906, 1495225, 1496559, 1496559, 1497059, 1497198, 1497523, 1498139, 1498139, 1498139, 1498304, 1498572, 1498792, 1499023, 1499023, 1499023])\n",
      "\n",
      "\n",
      "f([2359296, 2119680, 2119680, 2119680, 2119680, 2119680, 1997010, 1837027, 1741280, 1643617, 1519699, 1386902, 1386902, 1386902, 1386902, 1386902, 1386902, 1386902, 1386902, 1386902, 1386902, 1386902, 1505756, 1506967, 1507395, 1507481, 1507694, 1507694, 1507927, 1508302, 1508302, 1508302, 1508302, 1508302, 1508302, 1508302, 1508464, 1508464])\n",
      "\n",
      "\n",
      "f([2359296, 2095104, 2095104, 2095104, 2095104, 2095104, 1971214, 1821885, 1722031, 1595254, 1484294, 1361764, 1361764, 1361764, 1361764, 1361764, 1361764, 1361764, 1361764, 1361764, 1361764, 1361764, 1474165, 1475489, 1475821, 1476112, 1476226, 1476226, 1476226, 1476374, 1476374, 1476374, 1476374, 1476374, 1476374, 1476374, 1476374, 1476374])\n",
      "\n",
      "\n",
      "f([2359296, 2122752, 2122752, 2122752, 2122752, 2122752, 1989744, 1828334, 1737524, 1630026, 1501830, 1404911, 1404911, 1404911, 1404911, 1404911, 1404911, 1404911, 1404911, 1404911, 1404911, 1404911, 1528381, 1534522, 1535981, 1536158, 1536512, 1536512, 1537169, 1538680, 1538680, 1538680, 1538680, 1539239, 1539239, 1539239, 1539239, 1539239])\n",
      "\n",
      "\n",
      "f([2359296, 2168832, 2153472, 2107392, 2107392, 2107392, 1985922, 1839865, 1738668, 1646943, 1561870, 1488605, 1448815, 1445685, 1445249, 1436121, 1435762, 1435762, 1431903, 1431807, 1431807, 1431164, 1538048, 1545442, 1546162, 1546930, 1547160, 1547485, 1547727, 1548144, 1548144, 1548144, 1548740, 1549043, 1549342, 1549342, 1549342, 1549342])\n",
      "\n",
      "\n",
      "f([2359296, 2217984, 2214912, 2184192, 2147328, 2144256, 2015591, 1907142, 1795158, 1685116, 1572834, 1481287, 1434455, 1424183, 1421056, 1415957, 1414226, 1414226, 1414226, 1414226, 1414226, 1414226, 1485103, 1490914, 1491506, 1492066, 1492345, 1492672, 1492947, 1493541, 1493689, 1493689, 1493689, 1493842, 1494008, 1494008, 1494008, 1494008])\n",
      "\n",
      "\n",
      "f([2359296, 2217984, 2199552, 2116608, 2116608, 2116608, 1979395, 1810346, 1719224, 1615815, 1501816, 1406113, 1406113, 1406113, 1406113, 1406113, 1406113, 1406113, 1406113, 1406113, 1406113, 1406113, 1533493, 1533769, 1533973, 1533973, 1533973, 1533973, 1534161, 1534775, 1534775, 1534775, 1534775, 1534999, 1534999, 1534999, 1534999, 1534999])\n",
      "\n",
      "\n",
      "f([2359296, 2254848, 2233344, 2168832, 2125824, 2104320, 1980433, 1831544, 1739674, 1627735, 1515853, 1413699, 1413699, 1413699, 1413699, 1413699, 1413699, 1413699, 1413699, 1413699, 1413699, 1413699, 1532994, 1534080, 1534446, 1534584, 1534584, 1534584, 1534676, 1535227, 1535227, 1535227, 1535227, 1535227, 1535227, 1535227, 1535227, 1535227])\n",
      "\n",
      "\n",
      "f([2359296, 2276352, 2254848, 2141184, 2079744, 2079744, 1965129, 1858981, 1751156, 1635786, 1512667, 1399327, 1399327, 1399327, 1399327, 1399327, 1399327, 1399327, 1399327, 1399327, 1399327, 1399327, 1489470, 1493590, 1493846, 1493968, 1494176, 1494313, 1494635, 1494882, 1494882, 1494882, 1494882, 1494882, 1494971, 1494971, 1494971, 1494971])\n",
      "\n",
      "\n",
      "f([2359296, 2254848, 2233344, 2128896, 2073600, 2073600, 1953239, 1830209, 1732890, 1618282, 1495097, 1393869, 1393869, 1393869, 1393869, 1393869, 1393869, 1393869, 1393869, 1393869, 1393869, 1393869, 1477068, 1480905, 1481263, 1481263, 1481396, 1481605, 1481773, 1482845, 1482845, 1482845, 1482845, 1482845, 1482845, 1482845, 1482845, 1482845])\n",
      "\n",
      "\n",
      "f([2359296, 2267136, 2214912, 2024448, 2024448, 2024448, 1908864, 1776116, 1683366, 1564956, 1460230, 1360293, 1360293, 1360293, 1360293, 1360293, 1360293, 1360293, 1360293, 1360293, 1360293, 1360293, 1467333, 1469379, 1469451, 1469451, 1469597, 1469679, 1469930, 1469996, 1470215, 1470314, 1470314, 1470314, 1470314, 1470314, 1470314, 1470314])\n",
      "\n",
      "\n",
      "f([2359296, 2251776, 2217984, 1953792, 1953792, 1953792, 1843925, 1751985, 1682222, 1602613, 1524626, 1452509, 1431246, 1427277, 1427277, 1426680, 1426680, 1426680, 1426680, 1426306, 1426306, 1426306, 1538999, 1550841, 1551962, 1551962, 1552478, 1552478, 1552660, 1553227, 1554000, 1554000, 1554000, 1554000, 1554000, 1554000, 1554000, 1554000])\n",
      "\n",
      "\n",
      "f([589824, 557568, 542976, 482304, 482304, 482304, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 420048, 432990, 435507, 436009, 436081, 436081, 436123, 437775, 437792, 438023, 438023, 438023, 438094, 438094, 438094, 438148, 438148])\n",
      "\n",
      "\n",
      "#Module 4 in progress....\n",
      "origin\n",
      "0\n",
      "{'precision': 0.8224719101123595,\n",
      " 'recall': 0.8433179723502304,\n",
      " 'f1-score': 0.832764505119454,\n",
      " 'support': 434.0}\n",
      "Start Positive CI sparse\n",
      "1\n",
      "{'precision': 0.8290993071593533,\n",
      " 'recall': 0.8271889400921659,\n",
      " 'f1-score': 0.8281430219146482,\n",
      " 'support': 434.0}\n",
      "2\n",
      "{'precision': 0.8448687350835322,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8300117233294255,\n",
      " 'support': 434.0}\n",
      "3\n",
      "{'precision': 0.8388625592417062,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8271028037383177,\n",
      " 'support': 434.0}\n",
      "4\n",
      "{'precision': 0.8388625592417062,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8271028037383177,\n",
      " 'support': 434.0}\n",
      "5\n",
      "{'precision': 0.8372641509433962,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.8275058275058275,\n",
      " 'support': 434.0}\n",
      "Start Positive CI after sparse\n",
      "6\n",
      "{'precision': 0.8525798525798526,\n",
      " 'recall': 0.7995391705069125,\n",
      " 'f1-score': 0.8252080856123662,\n",
      " 'support': 434.0}\n",
      "7\n",
      "{'precision': 0.8368794326241135,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8261376896149357,\n",
      " 'support': 434.0}\n",
      "8\n",
      "{'precision': 0.8467153284671532,\n",
      " 'recall': 0.8018433179723502,\n",
      " 'f1-score': 0.8236686390532545,\n",
      " 'support': 434.0}\n",
      "9\n",
      "{'precision': 0.8236658932714617,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.8208092485549133,\n",
      " 'support': 434.0}\n",
      "10\n",
      "{'precision': 0.8301886792452831,\n",
      " 'recall': 0.8110599078341014,\n",
      " 'f1-score': 0.8205128205128206,\n",
      " 'support': 434.0}\n",
      "11\n",
      "{'precision': 0.827906976744186,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.8240740740740741,\n",
      " 'support': 434.0}\n",
      "12\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "13\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "14\n",
      "{'precision': 0.8313817330210773,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.8246225319396051,\n",
      " 'support': 434.0}\n",
      "15\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "16\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "17\n",
      "{'precision': 0.8305882352941176,\n",
      " 'recall': 0.8133640552995391,\n",
      " 'f1-score': 0.8218859138533178,\n",
      " 'support': 434.0}\n",
      "18\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "19\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "20\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "21\n",
      "{'precision': 0.8309859154929577,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.8232558139534883,\n",
      " 'support': 434.0}\n",
      "Start Negative TI\n",
      "22\n",
      "{'precision': 0.6201022146507666,\n",
      " 'recall': 0.8387096774193549,\n",
      " 'f1-score': 0.713026444662096,\n",
      " 'support': 434.0}\n",
      "23\n",
      "{'precision': 0.6685393258426966,\n",
      " 'recall': 0.8225806451612904,\n",
      " 'f1-score': 0.737603305785124,\n",
      " 'support': 434.0}\n",
      "24\n",
      "{'precision': 0.6704331450094162,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7378238341968911,\n",
      " 'support': 434.0}\n",
      "25\n",
      "{'precision': 0.6735849056603773,\n",
      " 'recall': 0.8225806451612904,\n",
      " 'f1-score': 0.7406639004149377,\n",
      " 'support': 434.0}\n",
      "26\n",
      "{'precision': 0.6801541425818882,\n",
      " 'recall': 0.8133640552995391,\n",
      " 'f1-score': 0.7408184679958028,\n",
      " 'support': 434.0}\n",
      "27\n",
      "{'precision': 0.6704545454545454,\n",
      " 'recall': 0.815668202764977,\n",
      " 'f1-score': 0.735966735966736,\n",
      " 'support': 434.0}\n",
      "28\n",
      "{'precision': 0.6537753222836096,\n",
      " 'recall': 0.8179723502304147,\n",
      " 'f1-score': 0.7267144319344935,\n",
      " 'support': 434.0}\n",
      "29\n",
      "{'precision': 0.6544117647058824,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7280163599182005,\n",
      " 'support': 434.0}\n",
      "30\n",
      "{'precision': 0.6580406654343808,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7302564102564103,\n",
      " 'support': 434.0}\n",
      "31\n",
      "{'precision': 0.65625,\n",
      " 'recall': 0.8225806451612904,\n",
      " 'f1-score': 0.7300613496932514,\n",
      " 'support': 434.0}\n",
      "32\n",
      "{'precision': 0.6544117647058824,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.7280163599182005,\n",
      " 'support': 434.0}\n",
      "33\n",
      "{'precision': 0.6491862567811935,\n",
      " 'recall': 0.8271889400921659,\n",
      " 'f1-score': 0.7274569402228978,\n",
      " 'support': 434.0}\n",
      "34\n",
      "{'precision': 0.6451612903225806,\n",
      " 'recall': 0.8294930875576036,\n",
      " 'f1-score': 0.7258064516129032,\n",
      " 'support': 434.0}\n",
      "35\n",
      "{'precision': 0.6417112299465241,\n",
      " 'recall': 0.8294930875576036,\n",
      " 'f1-score': 0.7236180904522613,\n",
      " 'support': 434.0}\n",
      "36\n",
      "{'precision': 0.6428571428571429,\n",
      " 'recall': 0.8294930875576036,\n",
      " 'f1-score': 0.7243460764587526,\n",
      " 'support': 434.0}\n",
      "37\n",
      "{'precision': 0.6399286987522281,\n",
      " 'recall': 0.8271889400921659,\n",
      " 'f1-score': 0.721608040201005,\n",
      " 'support': 434.0}\n",
      "f([2359296, 2260224, 2158080, 1983744, 1983744, 1983744, 1918455, 1829594, 1736452, 1628240, 1533814, 1475180, 1463232, 1460262, 1459910, 1459224, 1458397, 1458219, 1457884, 1456546, 1456143, 1455294, 1526710, 1527035, 1527077, 1527100, 1527124, 1527125, 1527154, 1527166, 1527204, 1527204, 1527204, 1527215, 1527215, 1527215, 1527215, 1527215])\n",
      "\n",
      "\n",
      "f([2359296, 2085888, 2085888, 2085888, 2085888, 2085888, 1979965, 1889134, 1753297, 1640363, 1523465, 1458540, 1442853, 1436515, 1435384, 1435037, 1434337, 1434078, 1433447, 1433336, 1433247, 1430873, 1538153, 1538424, 1538424, 1538531, 1538604, 1538618, 1538642, 1538692, 1538692, 1538731, 1538731, 1538731, 1538731, 1538731, 1538731, 1538731])\n",
      "\n",
      "\n",
      "f([2359296, 2068224, 2068224, 2068224, 2068224, 2068224, 1968964, 1857058, 1784819, 1692013, 1621579, 1550560, 1540661, 1539404, 1539013, 1538388, 1537007, 1536737, 1536622, 1536622, 1536538, 1533977, 1640167, 1640872, 1640951, 1640995, 1641067, 1641067, 1641128, 1641160, 1641163, 1641163, 1641163, 1641163, 1641163, 1641203, 1641203, 1641260])\n",
      "\n",
      "\n",
      "f([2359296, 2113536, 2113536, 2113536, 2113536, 2113536, 2028554, 1906483, 1770423, 1656714, 1547480, 1492389, 1485439, 1484213, 1483820, 1483603, 1483339, 1483276, 1483044, 1482888, 1482814, 1481707, 1569253, 1570630, 1570791, 1570843, 1570926, 1570926, 1571006, 1571096, 1571096, 1571096, 1571141, 1571190, 1571228, 1571228, 1571278, 1571278])\n",
      "\n",
      "\n",
      "f([2359296, 2130432, 1966080, 1966080, 1966080, 1966080, 1886190, 1805936, 1688066, 1598663, 1496786, 1430247, 1419666, 1416261, 1415376, 1415376, 1415376, 1415376, 1415376, 1415376, 1415376, 1415376, 1503578, 1505313, 1505599, 1505669, 1505744, 1505817, 1505951, 1505951, 1506051, 1506051, 1506051, 1506051, 1506051, 1506051, 1506063, 1506108])\n",
      "\n",
      "\n",
      "f([2359296, 2115840, 2115840, 2115840, 2115840, 2115840, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2085596, 2095302, 2095696, 2095782, 2095840, 2095840, 2095840, 2096000, 2096013, 2096013, 2096013, 2096013, 2096115, 2096115, 2096115, 2096172, 2096197])\n",
      "\n",
      "\n",
      "f([2359296, 2098176, 2098176, 2098176, 2098176, 2098176, 2019532, 1917800, 1820618, 1714577, 1628865, 1565272, 1559791, 1558743, 1557388, 1557074, 1556927, 1556894, 1556867, 1556793, 1556708, 1549382, 1632878, 1636498, 1636893, 1637184, 1637458, 1637511, 1638042, 1638116, 1638221, 1638221, 1638257, 1638390, 1638390, 1638415, 1638484, 1638505])\n",
      "\n",
      "\n",
      "f([2359296, 2099712, 2099712, 2099712, 2099712, 2099712, 1999773, 1874320, 1761491, 1651630, 1563038, 1515989, 1507406, 1506028, 1504727, 1504340, 1503558, 1503183, 1503142, 1502330, 1502072, 1491026, 1598635, 1603629, 1604036, 1604528, 1604627, 1604627, 1605214, 1605229, 1605229, 1605229, 1605229, 1605308, 1605308, 1605344, 1605442, 1605442])\n",
      "\n",
      "\n",
      "f([2359296, 2127360, 1867008, 1867008, 1867008, 1867008, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1851033, 1875254, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336, 1875336])\n",
      "\n",
      "\n",
      "f([2359296, 2141184, 1865472, 1865472, 1865472, 1865472, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1855099, 1893371, 1893463, 1893463, 1893463, 1893463, 1893463, 1893463, 1893463, 1893463, 1893463, 1893463, 1893491, 1893491, 1893491, 1893491, 1893491])\n",
      "\n",
      "\n",
      "f([2359296, 2111232, 2111232, 2111232, 2111232, 2111232, 2013097, 1923655, 1834848, 1745484, 1632636, 1543177, 1485994, 1483826, 1483020, 1483011, 1482874, 1482699, 1482544, 1481142, 1480920, 1470751, 1571672, 1574621, 1574736, 1574825, 1574937, 1574985, 1575081, 1575081, 1575101, 1575101, 1575143, 1575259, 1575294, 1575294, 1575367, 1575407])\n",
      "\n",
      "\n",
      "f([2359296, 2088192, 2088192, 2088192, 2088192, 2088192, 2007567, 1906531, 1812189, 1719745, 1587185, 1511825, 1491989, 1488209, 1487686, 1487349, 1486975, 1486935, 1486931, 1486076, 1485758, 1476205, 1552339, 1555835, 1556071, 1556188, 1556335, 1556364, 1556410, 1556410, 1556439, 1556439, 1556516, 1556631, 1556631, 1556631, 1556686, 1556686])\n",
      "\n",
      "\n",
      "f([2359296, 2300928, 2264064, 2208768, 2178048, 2162688, 2037013, 1918374, 1814826, 1697449, 1551874, 1422727, 1351077, 1351077, 1351077, 1351077, 1351077, 1351077, 1351077, 1351077, 1351077, 1351077, 1441662, 1443921, 1444391, 1445562, 1445562, 1445562, 1446532, 1446735, 1446915, 1446915, 1446915, 1446915, 1447087, 1447087, 1447087, 1447087])\n",
      "\n",
      "\n",
      "f([2359296, 2221056, 2156544, 2095104, 2095104, 2095104, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1820020, 1835389, 1836347, 1836467, 1836641, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815, 1836815])\n",
      "\n",
      "\n",
      "f([2359296, 2165760, 2088960, 2088960, 2088960, 2088960, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1796075, 1811077, 1811544, 1811544, 1811544, 1811544, 1811544, 1811544, 1811544, 1811544, 1811544, 1811544, 1811720, 1811720, 1811720, 1811720, 1811720])\n",
      "\n",
      "\n",
      "f([2359296, 2181120, 2135040, 2098176, 2098176, 2098176, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1783424, 1799993, 1801024, 1801303, 1801539, 1801539, 1801539, 1801850, 1801850, 1801850, 1801850, 1801850, 1801850, 1801850, 1801850, 1801850, 1801850])\n",
      "\n",
      "\n",
      "f([2359296, 2211840, 2196480, 2184192, 2144256, 2122752, 2000714, 1843274, 1731586, 1624849, 1529476, 1410307, 1410307, 1410307, 1410307, 1410307, 1410307, 1410307, 1410307, 1410307, 1410307, 1410307, 1517797, 1523602, 1524121, 1524908, 1524987, 1525199, 1525513, 1525513, 1525640, 1525640, 1525640, 1525824, 1525824, 1525824, 1525824, 1525957])\n",
      "\n",
      "\n",
      "f([2359296, 2178048, 2168832, 2150400, 2135040, 2119680, 1992741, 1877725, 1766266, 1660626, 1561585, 1417688, 1377323, 1377323, 1377323, 1377323, 1377323, 1377323, 1377323, 1377323, 1377323, 1377323, 1457854, 1467495, 1469158, 1470490, 1470850, 1471471, 1477171, 1477171, 1477621, 1477785, 1478114, 1479699, 1479862, 1479862, 1480107, 1480535])\n",
      "\n",
      "\n",
      "f([2359296, 2144256, 2125824, 2104320, 2104320, 2104320, 1966984, 1797164, 1708842, 1607549, 1503740, 1390098, 1390098, 1390098, 1390098, 1390098, 1390098, 1390098, 1390098, 1390098, 1390098, 1390098, 1510208, 1512925, 1513162, 1513765, 1513765, 1513765, 1514708, 1514851, 1514851, 1514851, 1514851, 1515965, 1516366, 1516458, 1516458, 1516458])\n",
      "\n",
      "\n",
      "f([2359296, 2205696, 2150400, 2119680, 2119680, 2119680, 1995509, 1844894, 1752828, 1636704, 1525240, 1423193, 1380283, 1380283, 1380283, 1380283, 1380283, 1380283, 1380283, 1380283, 1380283, 1380283, 1490732, 1494727, 1494727, 1494989, 1494989, 1494989, 1496287, 1496504, 1496504, 1496504, 1496504, 1496787, 1496787, 1496952, 1497125, 1497125])\n",
      "\n",
      "\n",
      "f([2359296, 2227200, 2162688, 2122752, 2122752, 2122752, 2005804, 1899976, 1792060, 1680292, 1562601, 1437701, 1421655, 1414975, 1414975, 1414975, 1414975, 1414975, 1414975, 1414975, 1414975, 1414975, 1500766, 1513741, 1514454, 1515262, 1515540, 1515753, 1516644, 1516741, 1516741, 1516844, 1516844, 1517081, 1517081, 1517081, 1517196, 1517196])\n",
      "\n",
      "\n",
      "f([2359296, 2230272, 2156544, 2131968, 2082816, 2082816, 1961873, 1840370, 1750547, 1666052, 1542308, 1447932, 1420882, 1414997, 1414997, 1414997, 1414997, 1414997, 1414997, 1414997, 1414997, 1414997, 1496930, 1506371, 1506633, 1507127, 1507127, 1507127, 1507655, 1507655, 1507655, 1507655, 1507791, 1508081, 1508081, 1508081, 1508306, 1508306])\n",
      "\n",
      "\n",
      "f([2359296, 2171904, 2088960, 2088960, 2088960, 2088960, 1969621, 1829541, 1730602, 1616850, 1498286, 1387420, 1387420, 1387420, 1387420, 1387420, 1387420, 1387420, 1387420, 1387420, 1387420, 1387420, 1494894, 1497464, 1497464, 1498001, 1498132, 1498132, 1498343, 1498406, 1498406, 1498406, 1498406, 1498406, 1498548, 1498548, 1498548, 1498679])\n",
      "\n",
      "\n",
      "f([2359296, 2082816, 2082816, 2082816, 2082816, 2082816, 1965898, 1825944, 1739330, 1643807, 1543680, 1418693, 1385183, 1385183, 1385183, 1385183, 1385183, 1385183, 1385183, 1385183, 1385183, 1385183, 1494600, 1511335, 1511808, 1513197, 1513197, 1513269, 1513828, 1513828, 1514023, 1514160, 1514529, 1515546, 1515923, 1515923, 1516592, 1516694])\n",
      "\n",
      "\n",
      "f([589824, 503040, 503040, 503040, 503040, 503040, 477118, 461188, 447347, 426745, 404486, 386991, 378875, 377218, 377192, 377006, 377006, 377001, 377001, 376982, 376982, 376071, 400061, 404112, 405086, 405544, 405755, 406139, 406623, 407257, 407257, 407257, 407491, 408306, 408306, 408306, 408332, 408342])\n",
      "\n",
      "\n",
      "#Module 4 in progress....\n",
      "origin\n",
      "0\n",
      "{'precision': 0.8224719101123595,\n",
      " 'recall': 0.8433179723502304,\n",
      " 'f1-score': 0.832764505119454,\n",
      " 'support': 434.0}\n",
      "Start Positive CI sparse\n",
      "1\n",
      "{'precision': 0.8048780487804879,\n",
      " 'recall': 0.836405529953917,\n",
      " 'f1-score': 0.8203389830508475,\n",
      " 'support': 434.0}\n",
      "2\n",
      "{'precision': 0.8248847926267281,\n",
      " 'recall': 0.8248847926267281,\n",
      " 'f1-score': 0.8248847926267281,\n",
      " 'support': 434.0}\n",
      "3\n",
      "{'precision': 0.8225806451612904,\n",
      " 'recall': 0.8225806451612904,\n",
      " 'f1-score': 0.8225806451612904,\n",
      " 'support': 434.0}\n",
      "4\n",
      "{'precision': 0.8206896551724138,\n",
      " 'recall': 0.8225806451612904,\n",
      " 'f1-score': 0.8216340621403913,\n",
      " 'support': 434.0}\n",
      "5\n",
      "{'precision': 0.8240740740740741,\n",
      " 'recall': 0.8202764976958525,\n",
      " 'f1-score': 0.8221709006928406,\n",
      " 'support': 434.0}\n",
      "Start Positive CI after sparse\n",
      "6\n",
      "{'precision': 0.8127753303964758,\n",
      " 'recall': 0.8502304147465438,\n",
      " 'f1-score': 0.831081081081081,\n",
      " 'support': 434.0}\n",
      "7\n",
      "{'precision': 0.7505070993914807,\n",
      " 'recall': 0.8525345622119815,\n",
      " 'f1-score': 0.7982740021574972,\n",
      " 'support': 434.0}\n",
      "8\n",
      "{'precision': 0.7612244897959184,\n",
      " 'recall': 0.8594470046082949,\n",
      " 'f1-score': 0.8073593073593074,\n",
      " 'support': 434.0}\n",
      "9\n",
      "{'precision': 0.735812133072407,\n",
      " 'recall': 0.8663594470046083,\n",
      " 'f1-score': 0.7957671957671958,\n",
      " 'support': 434.0}\n",
      "10\n",
      "{'precision': 0.7362204724409449,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7940552016985138,\n",
      " 'support': 434.0}\n",
      "11\n",
      "{'precision': 0.73046875,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7906976744186046,\n",
      " 'support': 434.0}\n",
      "12\n",
      "{'precision': 0.72568093385214,\n",
      " 'recall': 0.8594470046082949,\n",
      " 'f1-score': 0.7869198312236286,\n",
      " 'support': 434.0}\n",
      "13\n",
      "{'precision': 0.7276264591439688,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7890295358649788,\n",
      " 'support': 434.0}\n",
      "14\n",
      "{'precision': 0.7276264591439688,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7890295358649788,\n",
      " 'support': 434.0}\n",
      "15\n",
      "{'precision': 0.73046875,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7906976744186046,\n",
      " 'support': 434.0}\n",
      "16\n",
      "{'precision': 0.73046875,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7906976744186046,\n",
      " 'support': 434.0}\n",
      "17\n",
      "{'precision': 0.73046875,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7906976744186046,\n",
      " 'support': 434.0}\n",
      "18\n",
      "{'precision': 0.7290448343079922,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7898627243928193,\n",
      " 'support': 434.0}\n",
      "19\n",
      "{'precision': 0.7290448343079922,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7898627243928193,\n",
      " 'support': 434.0}\n",
      "20\n",
      "{'precision': 0.7290448343079922,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7898627243928193,\n",
      " 'support': 434.0}\n",
      "21\n",
      "{'precision': 0.7290448343079922,\n",
      " 'recall': 0.8617511520737328,\n",
      " 'f1-score': 0.7898627243928193,\n",
      " 'support': 434.0}\n",
      "Start Negative TI\n",
      "22\n",
      "{'precision': 0.5221354166666666,\n",
      " 'recall': 0.923963133640553,\n",
      " 'f1-score': 0.6672212978369384,\n",
      " 'support': 434.0}\n",
      "23\n",
      "{'precision': 0.5355704697986577,\n",
      " 'recall': 0.9193548387096774,\n",
      " 'f1-score': 0.6768447837150127,\n",
      " 'support': 434.0}\n",
      "24\n",
      "{'precision': 0.5236220472440944,\n",
      " 'recall': 0.9193548387096774,\n",
      " 'f1-score': 0.6672240802675584,\n",
      " 'support': 434.0}\n",
      "25\n",
      "{'precision': 0.5154241645244216,\n",
      " 'recall': 0.923963133640553,\n",
      " 'f1-score': 0.6617161716171618,\n",
      " 'support': 434.0}\n",
      "26\n",
      "{'precision': 0.5228758169934641,\n",
      " 'recall': 0.9216589861751152,\n",
      " 'f1-score': 0.6672226855713095,\n",
      " 'support': 434.0}\n",
      "27\n",
      "{'precision': 0.5154241645244216,\n",
      " 'recall': 0.923963133640553,\n",
      " 'f1-score': 0.6617161716171618,\n",
      " 'support': 434.0}\n",
      "28\n",
      "{'precision': 0.5167095115681234,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6633663366336634,\n",
      " 'support': 434.0}\n",
      "29\n",
      "{'precision': 0.5153846153846153,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6622734761120264,\n",
      " 'support': 434.0}\n",
      "30\n",
      "{'precision': 0.5140664961636828,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6611842105263157,\n",
      " 'support': 434.0}\n",
      "31\n",
      "{'precision': 0.5082174462705437,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.656326530612245,\n",
      " 'support': 434.0}\n",
      "32\n",
      "{'precision': 0.5031289111389237,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6520681265206814,\n",
      " 'support': 434.0}\n",
      "33\n",
      "{'precision': 0.5031289111389237,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6520681265206814,\n",
      " 'support': 434.0}\n",
      "34\n",
      "{'precision': 0.5037593984962406,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6525974025974026,\n",
      " 'support': 434.0}\n",
      "35\n",
      "{'precision': 0.5069356872635561,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6552567237163813,\n",
      " 'support': 434.0}\n",
      "36\n",
      "{'precision': 0.5069356872635561,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6552567237163813,\n",
      " 'support': 434.0}\n",
      "37\n",
      "{'precision': 0.5062972292191436,\n",
      " 'recall': 0.9262672811059908,\n",
      " 'f1-score': 0.6547231270358306,\n",
      " 'support': 434.0}\n",
      "f([2359296, 2259456, 2145792, 1989888, 1989888, 1989888, 1924377, 1835289, 1742178, 1633713, 1539119, 1480575, 1468393, 1465883, 1465194, 1464563, 1464289, 1463854, 1463734, 1463734, 1463657, 1463205, 1534538, 1534863, 1534863, 1534863, 1534893, 1534960, 1534960, 1534960, 1534960, 1535002, 1535002, 1535002, 1535002, 1535002, 1535002, 1535002])\n",
      "\n",
      "\n",
      "f([2359296, 2139648, 1853184, 1853184, 1853184, 1853184, 1758903, 1678140, 1557578, 1456391, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1351178, 1459280, 1459559, 1459636, 1459684, 1459684, 1459698, 1459726, 1459743, 1459743, 1459777, 1459777, 1459779, 1459779, 1459809, 1459809, 1459809])\n",
      "\n",
      "\n",
      "f([2359296, 2116608, 2116608, 2116608, 2116608, 2116608, 2014840, 1901143, 1825944, 1731330, 1637280, 1588417, 1578757, 1577650, 1577426, 1576486, 1576046, 1576021, 1575718, 1575574, 1575464, 1575010, 1680501, 1681381, 1681460, 1681507, 1681545, 1681575, 1681824, 1681824, 1681824, 1681824, 1681824, 1681824, 1681824, 1681824, 1681834, 1681834])\n",
      "\n",
      "\n",
      "f([2359296, 2177280, 1921536, 1921536, 1921536, 1921536, 1844177, 1733478, 1609315, 1505367, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1404924, 1493315, 1495226, 1495350, 1495481, 1495536, 1495572, 1495788, 1495814, 1495814, 1495962, 1495997, 1495997, 1495997, 1495997, 1495997, 1495997])\n",
      "\n",
      "\n",
      "f([2359296, 2180352, 2022912, 2022912, 2022912, 2022912, 1940702, 1857907, 1736936, 1642902, 1538353, 1468347, 1458842, 1455912, 1455534, 1454443, 1454353, 1454084, 1454035, 1453896, 1453824, 1453111, 1541187, 1543182, 1543426, 1543734, 1543791, 1543912, 1544134, 1544165, 1544165, 1544278, 1544278, 1544278, 1544314, 1544314, 1544314, 1544314])\n",
      "\n",
      "\n",
      "f([2359296, 2194176, 2054400, 2054400, 2054400, 2054400, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2023548, 2035549, 2035728, 2035728, 2035728, 2035728, 2035728, 2035728, 2035760, 2035760, 2035818, 2035883, 2035883, 2035883, 2035883, 2035883, 2035883])\n",
      "\n",
      "\n",
      "f([2359296, 2183424, 2034432, 2034432, 2034432, 2034432, 1958286, 1858635, 1765415, 1662895, 1580567, 1518893, 1512267, 1510669, 1510324, 1506937, 1506852, 1506744, 1506384, 1506224, 1506224, 1505642, 1589271, 1594329, 1594754, 1595076, 1595167, 1595455, 1595881, 1595881, 1596025, 1596244, 1596273, 1596298, 1596338, 1596465, 1596465, 1596529])\n",
      "\n",
      "\n",
      "f([2359296, 2232576, 2115840, 2115840, 2115840, 2115840, 2015187, 1889093, 1773652, 1663807, 1571434, 1524309, 1516408, 1511566, 1510342, 1507621, 1507051, 1506954, 1506846, 1505965, 1505955, 1505457, 1611944, 1619142, 1619955, 1620173, 1620463, 1620669, 1621002, 1621070, 1621330, 1621640, 1621802, 1621847, 1621887, 1622004, 1622004, 1622015])\n",
      "\n",
      "\n",
      "f([2359296, 2255616, 2137344, 1761024, 1761024, 1761024, 1676861, 1589807, 1505600, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1408239, 1511315, 1514900, 1515317, 1515521, 1515691, 1515770, 1515825, 1515825, 1515905, 1515947, 1516065, 1516109, 1516109, 1516420, 1516420, 1516420])\n",
      "\n",
      "\n",
      "f([2359296, 2253312, 2145792, 1753344, 1753344, 1753344, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1742965, 1789683, 1789830, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789931, 1789996, 1789996])\n",
      "\n",
      "\n",
      "f([2359296, 2207232, 2079744, 2079744, 2079744, 2079744, 1983176, 1894836, 1805245, 1715855, 1640276, 1507726, 1459004, 1456568, 1455691, 1451844, 1451566, 1451493, 1450580, 1449458, 1449458, 1449007, 1549847, 1553529, 1553794, 1553998, 1554030, 1554078, 1554169, 1554238, 1554426, 1554523, 1554788, 1554860, 1554925, 1555042, 1555073, 1555101])\n",
      "\n",
      "\n",
      "f([2359296, 2158848, 2031360, 2031360, 2031360, 2031360, 1952968, 1854325, 1763148, 1674272, 1547262, 1477386, 1458844, 1448458, 1447853, 1445446, 1445249, 1445225, 1445056, 1444447, 1444447, 1444346, 1519785, 1524115, 1524839, 1525404, 1525523, 1525553, 1525750, 1525782, 1525822, 1525940, 1526342, 1526439, 1526479, 1526809, 1526935, 1527011])\n",
      "\n",
      "\n",
      "f([2359296, 2328576, 2264064, 2233344, 2171904, 2135040, 2011113, 1897330, 1791470, 1676459, 1532547, 1413443, 1413443, 1413443, 1413443, 1413443, 1413443, 1413443, 1413443, 1413443, 1413443, 1413443, 1503926, 1506209, 1506880, 1507126, 1507126, 1507654, 1508778, 1508778, 1508778, 1509338, 1509550, 1510162, 1510262, 1510495, 1510612, 1510612])\n",
      "\n",
      "\n",
      "f([2359296, 2276352, 2184192, 2098176, 2098176, 2098176, 1976919, 1820701, 1724326, 1628615, 1527548, 1410240, 1410240, 1410240, 1410240, 1410240, 1410240, 1410240, 1410240, 1410240, 1410240, 1410240, 1518938, 1521396, 1521772, 1521946, 1522248, 1522467, 1522766, 1522766, 1522909, 1522909, 1523034, 1523194, 1523194, 1523194, 1523790, 1523790])\n",
      "\n",
      "\n",
      "f([2359296, 2187264, 2088960, 2088960, 2088960, 2088960, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1797820, 1813493, 1814839, 1814987, 1815132, 1815132, 1815330, 1815865, 1815865, 1816010, 1816010, 1816010, 1816010, 1816010, 1816010, 1816010, 1816010])\n",
      "\n",
      "\n",
      "f([2359296, 2230272, 2165760, 2027520, 2027520, 2027520, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1725626, 1746854, 1748311, 1748867, 1748867, 1748867, 1749010, 1749864, 1749864, 1749864, 1750151, 1750415, 1750415, 1750547, 1750547, 1750547, 1750547])\n",
      "\n",
      "\n",
      "f([2359296, 2297856, 2257920, 2128896, 2098176, 2098176, 1978207, 1811121, 1707636, 1603435, 1495796, 1379169, 1379169, 1379169, 1379169, 1379169, 1379169, 1379169, 1379169, 1379169, 1379169, 1379169, 1492923, 1497306, 1497847, 1498324, 1498457, 1498457, 1498619, 1498619, 1498849, 1499172, 1499172, 1499172, 1499172, 1499172, 1499172, 1499172])\n",
      "\n",
      "\n",
      "f([2359296, 2276352, 2239488, 2122752, 2122752, 2122752, 1996481, 1876212, 1767321, 1662571, 1560341, 1422290, 1375125, 1375125, 1375125, 1375125, 1375125, 1375125, 1375125, 1375125, 1375125, 1375125, 1452264, 1463792, 1464493, 1465478, 1465897, 1466501, 1466908, 1467055, 1467241, 1467241, 1467241, 1467241, 1467241, 1467389, 1467389, 1467389])\n",
      "\n",
      "\n",
      "f([2359296, 2300928, 2273280, 2141184, 2116608, 2116608, 1978604, 1809825, 1720122, 1616882, 1503779, 1408724, 1408724, 1408724, 1408724, 1408724, 1408724, 1408724, 1408724, 1408724, 1408724, 1408724, 1534257, 1541203, 1542488, 1542488, 1542488, 1542912, 1543390, 1543390, 1543672, 1544738, 1544888, 1544888, 1544888, 1544888, 1544888, 1544888])\n",
      "\n",
      "\n",
      "f([2359296, 2325504, 2282496, 2042880, 2042880, 2042880, 1922662, 1780572, 1690393, 1577659, 1468793, 1370178, 1370178, 1370178, 1370178, 1370178, 1370178, 1370178, 1370178, 1370178, 1370178, 1370178, 1483196, 1490271, 1491296, 1491591, 1491591, 1491914, 1492089, 1492089, 1492089, 1492289, 1492289, 1492601, 1492601, 1492723, 1492723, 1492819])\n",
      "\n",
      "\n",
      "f([2359296, 2319360, 2273280, 2052096, 2052096, 2052096, 1939314, 1832963, 1725076, 1617710, 1496592, 1380763, 1380763, 1380763, 1380763, 1380763, 1380763, 1380763, 1380763, 1380763, 1380763, 1380763, 1469913, 1479162, 1481305, 1481484, 1481484, 1482093, 1482603, 1482603, 1483006, 1483006, 1483006, 1483067, 1483067, 1483523, 1483523, 1483523])\n",
      "\n",
      "\n",
      "f([2359296, 2300928, 2254848, 2024448, 2024448, 2024448, 1906428, 1791627, 1707106, 1623864, 1509199, 1410898, 1410898, 1410898, 1410898, 1410898, 1410898, 1410898, 1410898, 1410898, 1410898, 1410898, 1502853, 1510742, 1513590, 1513863, 1513978, 1514372, 1515121, 1515121, 1515194, 1515442, 1515442, 1515818, 1515818, 1515935, 1515935, 1516101])\n",
      "\n",
      "\n",
      "f([2359296, 2254848, 2227200, 1963008, 1963008, 1963008, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1691487, 1714381, 1717030, 1719008, 1719353, 1719353, 1719353, 1719353, 1719353, 1719353, 1719517, 1719654, 1719654, 1719654, 1719801, 1719801, 1720119])\n",
      "\n",
      "\n",
      "f([2359296, 2199552, 2159616, 1935360, 1935360, 1935360, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1662738, 1691651, 1694623, 1696980, 1697437, 1697437, 1697437, 1697437, 1697437, 1697437, 1697437, 1697558, 1697558, 1697558, 1697558, 1697558, 1697558])\n",
      "\n",
      "\n",
      "f([589824, 523008, 523008, 523008, 523008, 523008, 495956, 479461, 465222, 444148, 422217, 404713, 396221, 392743, 392743, 392338, 392226, 392226, 392060, 391998, 391998, 391998, 414215, 417587, 422104, 422490, 422709, 422760, 423192, 423487, 423803, 424538, 424591, 424624, 424788, 424807, 425001, 425044])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in range(3):\n",
    "    i = 9\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "\n",
    "    positive_samples1 = sampling_class(\n",
    "        train_dataloader, i, 20, num_labels, True, 4, device=device\n",
    "    )\n",
    "    positive_samples2 = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "\n",
    "    module1 = copy.deepcopy(model)\n",
    "    w = WeightRemoverBert(model, p=0.9)\n",
    "    ci1 = ConcernIdentificationBert(model, p=0.5)\n",
    "    ti1 = TanglingIdentification(model, p=0.6)\n",
    "\n",
    "    ff1 = [\n",
    "        [\n",
    "            torch.sum(\n",
    "                model.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "            ).item()\n",
    "        ]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    ff2 = [\n",
    "        [torch.sum(model.bert.encoder.layer[num].output.dense.weight != 0).item()]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    pooler = [torch.sum(model.bert.pooler.dense.weight != 0).item()]\n",
    "    print(\"origin\")\n",
    "    j = 0\n",
    "    print(j)\n",
    "    results = test_f1(model, test_dataloader, model_config, False)\n",
    "    pp(results[\"details\"][f\"{i}\"])\n",
    "\n",
    "    print(\"Start Positive CI sparse\")\n",
    "\n",
    "    for batch in positive_samples1:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = w.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Positive CI after sparse\")\n",
    "\n",
    "    for batch in positive_samples2:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = ci1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Negative TI\")\n",
    "\n",
    "    for batch in negative_samples:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t = ti1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff1[num]})\")\n",
    "        print(\"\\n\")\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff2[num]})\")\n",
    "        print(\"\\n\")\n",
    "    print(f\"f({pooler})\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3eac6-f791-4fba-ba45-dcf94af4545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    i = 10\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "\n",
    "    positive_samples1 = sampling_class(\n",
    "        train_dataloader, i, 20, num_labels, True, 4, device=device\n",
    "    )\n",
    "    positive_samples2 = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "\n",
    "    module1 = copy.deepcopy(model)\n",
    "    w = WeightRemoverBert(model, p=0.9)\n",
    "    ci1 = ConcernIdentificationBert(model, p=0.5)\n",
    "    ti1 = TanglingIdentification(model, p=0.6)\n",
    "\n",
    "    ff1 = [\n",
    "        [\n",
    "            torch.sum(\n",
    "                model.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "            ).item()\n",
    "        ]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    ff2 = [\n",
    "        [torch.sum(model.bert.encoder.layer[num].output.dense.weight != 0).item()]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    pooler = [torch.sum(model.bert.pooler.dense.weight != 0).item()]\n",
    "    print(\"origin\")\n",
    "    j = 0\n",
    "    print(j)\n",
    "    results = test_f1(model, test_dataloader, model_config, False)\n",
    "    pp(results[\"details\"][f\"{i}\"])\n",
    "\n",
    "    print(\"Start Positive CI sparse\")\n",
    "\n",
    "    for batch in positive_samples1:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = w.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Positive CI after sparse\")\n",
    "\n",
    "    for batch in positive_samples2:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = ci1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Negative TI\")\n",
    "\n",
    "    for batch in negative_samples:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t = ti1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff1[num]})\")\n",
    "        print(\"\\n\")\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff2[num]})\")\n",
    "        print(\"\\n\")\n",
    "    print(f\"f({pooler})\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1f89c-8128-4c91-b9a1-acd19dfc9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    i = 11\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "\n",
    "    positive_samples1 = sampling_class(\n",
    "        train_dataloader, i, 20, num_labels, True, 4, device=device\n",
    "    )\n",
    "    positive_samples2 = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "\n",
    "    module1 = copy.deepcopy(model)\n",
    "    w = WeightRemoverBert(model, p=0.9)\n",
    "    ci1 = ConcernIdentificationBert(model, p=0.5)\n",
    "    ti1 = TanglingIdentification(model, p=0.6)\n",
    "\n",
    "    ff1 = [\n",
    "        [\n",
    "            torch.sum(\n",
    "                model.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "            ).item()\n",
    "        ]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    ff2 = [\n",
    "        [torch.sum(model.bert.encoder.layer[num].output.dense.weight != 0).item()]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    pooler = [torch.sum(model.bert.pooler.dense.weight != 0).item()]\n",
    "    print(\"origin\")\n",
    "    j = 0\n",
    "    print(j)\n",
    "    results = test_f1(model, test_dataloader, model_config, False)\n",
    "    pp(results[\"details\"][f\"{i}\"])\n",
    "\n",
    "    print(\"Start Positive CI sparse\")\n",
    "\n",
    "    for batch in positive_samples1:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = w.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Positive CI after sparse\")\n",
    "\n",
    "    for batch in positive_samples2:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = ci1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Negative TI\")\n",
    "\n",
    "    for batch in negative_samples:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t = ti1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff1[num]})\")\n",
    "        print(\"\\n\")\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff2[num]})\")\n",
    "        print(\"\\n\")\n",
    "    print(f\"f({pooler})\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c06992-c72b-4849-9498-15bfc87d37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    i = 12\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "\n",
    "    positive_samples1 = sampling_class(\n",
    "        train_dataloader, i, 20, num_labels, True, 4, device=device\n",
    "    )\n",
    "    positive_samples2 = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "\n",
    "    module1 = copy.deepcopy(model)\n",
    "    w = WeightRemoverBert(model, p=0.9)\n",
    "    ci1 = ConcernIdentificationBert(model, p=0.5)\n",
    "    ti1 = TanglingIdentification(model, p=0.6)\n",
    "\n",
    "    ff1 = [\n",
    "        [\n",
    "            torch.sum(\n",
    "                model.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "            ).item()\n",
    "        ]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    ff2 = [\n",
    "        [torch.sum(model.bert.encoder.layer[num].output.dense.weight != 0).item()]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    pooler = [torch.sum(model.bert.pooler.dense.weight != 0).item()]\n",
    "    print(\"origin\")\n",
    "    j = 0\n",
    "    print(j)\n",
    "    results = test_f1(model, test_dataloader, model_config, False)\n",
    "    pp(results[\"details\"][f\"{i}\"])\n",
    "\n",
    "    print(\"Start Positive CI sparse\")\n",
    "\n",
    "    for batch in positive_samples1:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = w.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Positive CI after sparse\")\n",
    "\n",
    "    for batch in positive_samples2:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = ci1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Negative TI\")\n",
    "\n",
    "    for batch in negative_samples:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t = ti1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff1[num]})\")\n",
    "        print(\"\\n\")\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff2[num]})\")\n",
    "        print(\"\\n\")\n",
    "    print(f\"f({pooler})\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd9e33-a82d-476e-a4d9-b82fa5c7a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(3):\n",
    "    i = 4\n",
    "    print(\"#Module \" + str(i) + \" in progress....\")\n",
    "    num_samples = 64\n",
    "\n",
    "    positive_samples1 = sampling_class(\n",
    "        train_dataloader, i, 20, num_labels, True, 4, device=device\n",
    "    )\n",
    "    positive_samples2 = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, True, 4, device=device\n",
    "    )\n",
    "    negative_samples = sampling_class(\n",
    "        train_dataloader, i, num_samples, num_labels, False, 4, device=device\n",
    "    )\n",
    "\n",
    "    module1 = copy.deepcopy(model)\n",
    "    w = WeightRemoverBert(model, p=0.9)\n",
    "    ci1 = ConcernIdentificationBert(model, p=0.5)\n",
    "    ti1 = TanglingIdentification(model, p=0.6)\n",
    "\n",
    "    ff1 = [\n",
    "        [\n",
    "            torch.sum(\n",
    "                model.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "            ).item()\n",
    "        ]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    ff2 = [\n",
    "        [torch.sum(model.bert.encoder.layer[num].output.dense.weight != 0).item()]\n",
    "        for num in range(config.num_hidden_layers)\n",
    "    ]\n",
    "    pooler = [torch.sum(model.bert.pooler.dense.weight != 0).item()]\n",
    "    print(\"origin\")\n",
    "    j = 0\n",
    "    print(j)\n",
    "    results = test_f1(model, test_dataloader, model_config, False)\n",
    "    pp(results[\"details\"][f\"{i}\"])\n",
    "\n",
    "    print(\"Start Positive CI sparse\")\n",
    "\n",
    "    for batch in positive_samples1:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = w.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Positive CI after sparse\")\n",
    "\n",
    "    for batch in positive_samples2:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t1 = ci1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    print(\"Start Negative TI\")\n",
    "\n",
    "    for batch in negative_samples:\n",
    "        input_ids, attn_mask, _, total_sampled = batch\n",
    "        with torch.no_grad():\n",
    "            t = ti1.propagate(module1, input_ids)\n",
    "        for num in range(config.num_hidden_layers):\n",
    "            ff1[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].intermediate.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "            ff2[num].append(\n",
    "                torch.sum(\n",
    "                    module1.bert.encoder.layer[num].output.dense.weight != 0\n",
    "                ).item()\n",
    "            )\n",
    "        pooler.append(torch.sum(module1.bert.pooler.dense.weight != 0).item())\n",
    "\n",
    "        j+=1\n",
    "        print(j)\n",
    "        \n",
    "        results = test_f1(module1, test_dataloader, model_config, False)\n",
    "        pp(results[\"details\"][f\"{i}\"])        \n",
    "\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff1[num]})\")\n",
    "        print(\"\\n\")\n",
    "    for num in range(config.num_hidden_layers):\n",
    "        print(f\"f({ff2[num]})\")\n",
    "        print(\"\\n\")\n",
    "    print(f\"f({pooler})\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
