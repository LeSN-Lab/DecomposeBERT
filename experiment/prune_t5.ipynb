{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574f39c9-c15e-4352-8bfd-e1acdc0ffb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "from utils.model_utils.evaluate import evaluate_model\n",
    "from utils.model_utils.load_model import load_model\n",
    "from utils.model_utils.model_config import ModelConfig\n",
    "from utils.dataset_utils.load_dataset import load_data\n",
    "from utils.decompose_utils.weight_remover import WeightRemoverBert\n",
    "from utils.decompose_utils.concern_identification import ConcernIdentificationBert\n",
    "from utils.decompose_utils.tangling_identification import TanglingIdentification\n",
    "from transformers import AutoConfig\n",
    "from utils.model_utils.save_module import save_module\n",
    "from datetime import datetime\n",
    "from utils.decompose_utils.concern_modularization import ConcernModularizationBert\n",
    "from utils.decompose_utils.sampling import sampling_class\n",
    "from utils.dataset_utils.load_dataset import (\n",
    "    convert_dataset_labels_to_binary,\n",
    "    extract_and_convert_dataloader,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03cc17d8-3184-441c-89c8-a662ca7237e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codet5-base-multi-sum\"\n",
    "task_type = \"seq2seq\"\n",
    "architectures = \"T5\"\n",
    "dataset_name = \"Go\"\n",
    "num_labels = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5268e5-e6bd-4ae5-9f0d-cf0084077d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e56880-d19b-405d-9806-9a45d1255fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "model_config = ModelConfig(\n",
    "    model_name=model_name,\n",
    "    task_type=task_type,\n",
    "    dataset_name=dataset_name,\n",
    "    checkpoint=checkpoint,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127f9b5f-e5fc-40eb-89dc-c17539242553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /home/Minwoo/LESN/Decompose/DecomposeTransformer/Models/Configs/seq2seq/Salesforce/codet5-base-multi-sum exists.\n",
      "Loading the model.\n",
      "The model Salesforce/codet5-base-multi-sum is loaded.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, checkpoint = load_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fc3394-407e-47a6-b03e-9c77e6e6dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab3ab86-0f62-4fe7-93fb-55d2a6ff6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset Go\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Minwoo/.conda/envs/DecomposeTransformer/lib/python3.8/site-packages/datasets/load.py:1486: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching is completed.\n",
      "The dataset Go is loaded\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = load_data(\n",
    "        model_config, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b208b19-c462-42e3-b1f7-1be371f1f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0c9f0-b1ae-4a34-a52f-92321447fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7839d18-cf67-493d-ba3c-9490a63d623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd86c4f-874b-4fe2-817c-3fd056e6f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf24844-b417-4e3d-bc19-b2dfb31fc967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba7e05-dd2b-4a32-b8e6-a2b95ea5cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = None\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model_config = ModelConfig(\n",
    "    _model_name=model_name,\n",
    "    _model_type=model_type,\n",
    "    _data=data,\n",
    "    _transformer_config=config,\n",
    "    _checkpoint_name=checkpoint_name,\n",
    "    _device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58025822-7134-4cb5-8c3b-76e75c018217",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Salesforce/codet5-base-multi-sum\"\n",
    "model_type = \"pretrained\"\n",
    "data = \"OSDG\"\n",
    "num_labels = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed643bb-8258-4eee-9f9f-c8e7ddd8391b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7839636-e7f9-4d80-b6bd-0154c596768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d1988-7134-4b02-bc6b-0fc60a3672a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf97b4-cb20-4c17-9abe-125118474319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl_line(jsonl_line):\n",
    "    \"\"\"\n",
    "    Process a single line of JSONL data.\n",
    "\n",
    "    Args:\n",
    "    jsonl_line (str): A single line from a JSONL file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the processed data.\n",
    "    \"\"\"\n",
    "    data = json.loads(jsonl_line)\n",
    "    \n",
    "    # Perform some operation on the data\n",
    "    # Example: Extracting relevant information and transforming it\n",
    "    processed_data = {\n",
    "        \"repository\": data.get(\"repo\"),\n",
    "        \"file_path\": data.get(\"path\"),\n",
    "        \"function_name\": data.get(\"func_name\"),\n",
    "        \"code_snippet\": data.get(\"code\"),\n",
    "        \"language\": data.get(\"language\"),\n",
    "        \"documentation\": data.get(\"docstring\"),\n",
    "        \"url\": data.get(\"url\")\n",
    "    }\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955d551-ee4f-4d85-abdd-c1c281657905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl_file(file_path):\n",
    "    \"\"\"\n",
    "    Process an entire JSONL file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the JSONL file.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries with the processed data from each line.\n",
    "    \"\"\"\n",
    "    processed_data_list = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            processed_data = process_jsonl_line(line)\n",
    "            processed_data_list.append(processed_data)\n",
    "    \n",
    "    return processed_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864ca17-f2e4-4f48-b051-e9ffb626a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Datasets/Codes/python/train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c28e2-8ee4-4425-8286-623b61733f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = process_jsonl_file(file_path)\n",
    "for item in processed_data:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff3aba-bffb-4f12-ac77-525cf0ea22a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'repository': 'smdabdoub/phylotoast', 'file_path': 'phylotoast/util.py', 'function_name': 'split_phylogeny', 'code_snippet': 'def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\\n    \"\"\"\\n    level = level+\"__\"\\n    result = p.split(level)\\n    return result[0]+level+result[1].split(\";\")[0]', 'language': 'python', 'documentation': 'Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.', 'url': 'https://github.com/smdabdoub/phylotoast/blob/0b74ef171e6a84761710548501dfac71285a58a3/phylotoast/util.py#L159-L177'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04650ab-58a5-4300-88c4-9cee302853d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[\"code_snippet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ae706-d3a4-46f4-9fd5-1ce925c4e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"repository\": data.get(\"repo\"),\n",
    "\"file_path\": data.get(\"path\"),\n",
    "\"function_name\": data.get(\"func_name\"),\n",
    "\"code_snippet\": data.get(\"code\"),\n",
    "\"language\": data.get(\"language\"),\n",
    "\"documentation\": data.get(\"docstring\"),\n",
    "\"url\": data.get(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457bd05-81d5-4950-be80-ee6e959f048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, tokenizer, text):\n",
    "    model = model.to(device)\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generated_ids = model.generate(input_ids, max_length=20)\n",
    "    print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2aac15-76d6-424b-979a-dacda1f28ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''def split_phylogeny(p, level=\"s\"):\n",
    "    \"\"\"\n",
    "    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n",
    "\n",
    "    :type p: str\n",
    "    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n",
    "\n",
    "    :type level: str\n",
    "    :param level: The different level of identification are kingdom (k), phylum (p),\n",
    "                  class (c),order (o), family (f), genus (g) and species (s). If level is\n",
    "                  not provided, the default level of identification is species.\n",
    "\n",
    "    :rtype: str\n",
    "    :return: A QIIME-formatted taxonomy string up to the classification given\n",
    "            by param level.\n",
    "    \"\"\"\n",
    "    level = level+\"__\"\n",
    "    result = p.split(level)\n",
    "    return result[0]+level+result[1].split(\";\")[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a55ea-088f-4d34-9128-8db498631e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4dd83e-4490-41ac-85d9-87c72c1208c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''def split_phylogeny(p, level=\"s\"):\n",
    "    level = level+\"__\"\n",
    "    result = p.split(level)\n",
    "    return result[0]+level+result[1].split(\";\")[0]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04673f7b-e0e8-47c0-a761-8d239c9959f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6d9e9-922b-4e2c-90d3-abddcc1d0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "class WeightRemover:\n",
    "    def __init__(self, model, device=\"cuda:0\", p=0.8):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.p = p\n",
    "        self.results = {\"layer\": [], \"input\": [], \"output\": []}\n",
    "\n",
    "    def hook(self, layer, input, output):\n",
    "        self.results[\"layer\"].append(layer)\n",
    "        self.results[\"input\"].append(input[0].to('cpu'))\n",
    "        self.results[\"output\"].append(output[0].to('cpu'))\n",
    "\n",
    "    def register_hooks(self):\n",
    "        handle_list = []\n",
    "        for layer in self.model.modules():\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                handle = layer.register_forward_hook(self.hook)\n",
    "                handle_list.append(handle)\n",
    "        return handle_list\n",
    "\n",
    "    def remove_hooks(self, handle_list):\n",
    "        for handle in handle_list:\n",
    "            handle.remove()\n",
    "\n",
    "    def remove_weights(self, layer):\n",
    "        current_weight = layer.weight.clone()\n",
    "        if layer.bias is not None:\n",
    "            current_bias = layer.bias.clone()\n",
    "        else:\n",
    "            current_bias = None\n",
    "\n",
    "        mean = torch.mean(current_weight, dim=1, keepdim=True)\n",
    "        std = torch.std(current_weight, dim=1, keepdim=True)\n",
    "        z_scores = (current_weight - mean) / std\n",
    "\n",
    "        lower_z, upper_z = norm.ppf(0.45), norm.ppf(0.55)\n",
    "        mask = torch.logical_and(z_scores >= lower_z, z_scores < upper_z)\n",
    "\n",
    "        current_weight[mask] = 0\n",
    "        all_zeros = ~mask.any(dim=1)\n",
    "        if current_bias is not None:\n",
    "            current_bias[all_zeros] = 0\n",
    "        self.set_parameters(layer, current_weight, current_bias)\n",
    "\n",
    "    def set_parameters(self, layer, weight, bias):\n",
    "        layer.weight.data = weight\n",
    "        if bias is not None:\n",
    "            layer.bias.data = bias\n",
    "\n",
    "    def process(self, input_tensor, decoder_input_ids):\n",
    "        self.results = {\"layer\": [], \"input\": [], \"output\": []}\n",
    "        handle_list = self.register_hooks()\n",
    "        output = self.model(input_ids=input_tensor.to(self.device), decoder_input_ids=decoder_input_ids.to(self.device))\n",
    "        self.remove_hooks(handle_list)\n",
    "        return output\n",
    "        \n",
    "    def apply_removal(self):\n",
    "        total_original_weights = 0\n",
    "        total_remaining_weights = 0\n",
    "\n",
    "        for idx, layer in enumerate(self.results[\"layer\"]):\n",
    "            current_weight = layer.weight\n",
    "            original_non_zero_weights = torch.sum(current_weight != 0).item()\n",
    "            total_original_weights += original_non_zero_weights\n",
    "\n",
    "            if torch.sum(current_weight != 0) > torch.numel(current_weight) * self.p:\n",
    "                self.results[\"output\"][idx] = self.results[\"output\"][idx].to(self.device)\n",
    "                self.remove_weights(layer)\n",
    "                self.results[\"output\"][idx] = self.results[\"output\"][idx].to('cpu')\n",
    "\n",
    "            remaining_non_zero_weights = torch.sum(layer.weight != 0).item()\n",
    "            total_remaining_weights += remaining_non_zero_weights\n",
    "\n",
    "            print(f\"Layer {idx} - Original non-zero weights: {original_non_zero_weights}, Remaining non-zero weights: {remaining_non_zero_weights}, Reduction: {original_non_zero_weights - remaining_non_zero_weights}\")\n",
    "\n",
    "        print(f\"Total original non-zero weights: {total_original_weights}\")\n",
    "        print(f\"Total remaining non-zero weights: {total_remaining_weights}\")\n",
    "        print(f\"Total reduction: {total_original_weights - total_remaining_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0b8dc-8aa8-42b0-9f15-ed193652213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcernIdentification:\n",
    "    def __init__(self, ref_model, model, device='cuda:0', p=0.7):\n",
    "        self.ref_model = ref_model.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.p = p\n",
    "        self.original_results = {\"layer\": [], \"input\": [], \"output\": []}\n",
    "        self.current_results = {\"layer\": [], \"input\": [], \"output\": []}\n",
    "\n",
    "    def original_hook(self, layer, input, output):\n",
    "        self.original_results[\"layer\"].append(layer)\n",
    "        self.original_results[\"input\"].append(input[0].to('cpu'))\n",
    "        self.original_results[\"output\"].append(output[0].to('cpu'))\n",
    "\n",
    "    def current_hook(self, layer, input, output):\n",
    "        self.current_results[\"layer\"].append(layer)\n",
    "        self.current_results[\"input\"].append(input[0].to('cpu'))\n",
    "        self.current_results[\"output\"].append(output[0].to('cpu'))\n",
    "\n",
    "    def register_hooks(self, model, hook):\n",
    "        handle_list = []\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                handle = layer.register_forward_hook(hook)\n",
    "                handle_list.append(handle)\n",
    "        return handle_list\n",
    "\n",
    "    def remove_hooks(self, handle_list):\n",
    "        for handle in handle_list:\n",
    "            handle.remove()\n",
    "\n",
    "    def prune(self, ref_model, model, original_output, output):\n",
    "        current_weight = model.weight.clone()\n",
    "        if model.bias is not None:\n",
    "            current_bias = model.bias.clone()\n",
    "        else:\n",
    "            current_bias = None\n",
    "        original_weight = ref_model.weight.clone()\n",
    "        \n",
    "        if ref_model.bias is not None:\n",
    "            original_bias = ref_model.bias.clone()\n",
    "        else:\n",
    "            original_bias = None\n",
    "        shape = current_weight.shape\n",
    "\n",
    "        output_loss = output - original_output\n",
    "        if len(output_loss.shape) > len(shape):\n",
    "            output_loss = output_loss[:, 0, :]\n",
    "            \n",
    "        positive_loss_mask = (\n",
    "            torch.all(output_loss > 0, dim=0).unsqueeze(1).expand(-1, shape[1])\n",
    "        )\n",
    "\n",
    "        original_weight_std = safe_std(original_weight, dim=1, keepdim=True)\n",
    "        current_weight_std = safe_std(\n",
    "            current_weight,\n",
    "            epsilon=original_weight_std,\n",
    "            unbiased=True,\n",
    "            dim=1,\n",
    "            keepdim=True,\n",
    "        )\n",
    "\n",
    "        padded_positive = torch.where(\n",
    "            current_weight > 0, current_weight, torch.tensor(float(\"nan\"))\n",
    "        )\n",
    "        padded_negative = torch.where(\n",
    "            current_weight < 0, current_weight, torch.tensor(float(\"nan\"))\n",
    "        )\n",
    "        positive_mean = torch.nanmean(padded_positive, dim=1, keepdim=True)\n",
    "        negative_mean = torch.nanmean(padded_negative, dim=1, keepdim=True)\n",
    "\n",
    "        positive_std = safe_std(\n",
    "            current_weight,\n",
    "            epsilon=current_weight_std,\n",
    "            unbiased=True,\n",
    "            dim=1,\n",
    "            keepdim=True,\n",
    "        )\n",
    "        negative_std = safe_std(\n",
    "            current_weight,\n",
    "            epsilon=current_weight_std,\n",
    "            unbiased=True,\n",
    "            dim=1,\n",
    "            keepdim=True,\n",
    "        )\n",
    "\n",
    "        positive_scores = (padded_positive - positive_mean) / positive_std\n",
    "        negative_scores = (padded_negative - negative_mean) / negative_std\n",
    "\n",
    "        positive_median = torch.nanmedian(padded_positive, dim=1, keepdim=True)\n",
    "        negative_median = torch.nanmedian(padded_negative, dim=1, keepdim=True)\n",
    "        lower_z, upper_z = norm.ppf(0.1), norm.ppf(0.3)\n",
    "\n",
    "        positive_remove_mask = torch.where(\n",
    "            positive_mean < positive_median.values,\n",
    "            positive_scores <= lower_z,\n",
    "            torch.logical_and(positive_scores >= lower_z, positive_scores < upper_z),\n",
    "        )\n",
    "\n",
    "        negative_remove_mask = torch.where(\n",
    "            negative_mean < negative_median.values,\n",
    "            torch.logical_and(negative_scores < -lower_z, negative_scores >= -upper_z),\n",
    "            negative_scores >= -upper_z,\n",
    "        )\n",
    "\n",
    "        remove_mask = torch.where(\n",
    "            ~positive_loss_mask, positive_remove_mask, negative_remove_mask\n",
    "        )\n",
    "\n",
    "        current_weight[remove_mask] = 0\n",
    "\n",
    "        all_zeros = ~remove_mask.any(dim=1)\n",
    "        if current_bias is not None:\n",
    "            current_bias[all_zeros] = 0\n",
    "        self.set_parameters(model, current_weight, current_bias)\n",
    "\n",
    "    def set_parameters(self, layer, weight, bias):\n",
    "        layer.weight.data = weight\n",
    "        if bias is not None:\n",
    "            layer.bias.data = bias\n",
    "\n",
    "    def process(self, input_tensor, decoder_input_ids):\n",
    "        self.original_results = {\"layer\": [], \"input\": [], \"output\": []}\n",
    "        self.current_results = {\"layer\": [], \"input\": [], \"output\": []}\n",
    "\n",
    "        handle_list = self.register_hooks(self.model, self.current_hook)\n",
    "        self.model(input_ids=input_tensor.to(self.device), decoder_input_ids=decoder_input_ids.to(self.device))\n",
    "        self.remove_hooks(handle_list)\n",
    "        handle_list = self.register_hooks(self.ref_model, self.original_hook)\n",
    "        self.ref_model(input_ids=input_tensor.to(self.device), decoder_input_ids=decoder_input_ids.to(self.device))\n",
    "        self.remove_hooks(handle_list)\n",
    "\n",
    "    def apply_prune(self):\n",
    "        total_original_weights = 0\n",
    "        total_remaining_weights = 0\n",
    "        \n",
    "        for idx, layer in enumerate(self.current_results[\"layer\"]):\n",
    "            current_weight = layer.weight\n",
    "            original_non_zero_weights = torch.sum(current_weight != 0).item()\n",
    "            total_original_weights += original_non_zero_weights\n",
    "            \n",
    "            if torch.sum(current_weight != 0) > torch.numel(current_weight) * self.p:\n",
    "                self.original_results[\"output\"][idx] = self.original_results[\"output\"][idx].to(self.device)\n",
    "                self.current_results[\"output\"][idx] = self.current_results[\"output\"][idx].to(self.device)\n",
    "                self.prune(self.original_results[\"layer\"][idx], layer, self.original_results[\"output\"][idx],\n",
    "                           self.current_results[\"output\"][idx])\n",
    "                self.original_results[\"output\"][idx] = self.original_results[\"output\"][idx].to('cpu')\n",
    "                self.current_results[\"output\"][idx] = self.current_results[\"output\"][idx].to('cpu')\n",
    "\n",
    "            remaining_non_zero_weights = torch.sum(layer.weight != 0).item()\n",
    "            total_remaining_weights += remaining_non_zero_weights\n",
    "\n",
    "            print(f\"Layer {idx} - Original non-zero weights: {original_non_zero_weights}, Remaining non-zero weights: {remaining_non_zero_weights}, Reduction: {original_non_zero_weights - remaining_non_zero_weights}\")\n",
    "            \n",
    "        \n",
    "        print(f\"Total original non-zero weights: {total_original_weights}\")\n",
    "        print(f\"Total remaining non-zero weights: {total_remaining_weights}\")\n",
    "        print(f\"Total reduction: {total_original_weights - total_remaining_weights}\")                \n",
    "\n",
    "\n",
    "def safe_std(tensor, epsilon=None, unbiased=False, dim=None, keepdim=True):\n",
    "    if tensor.numel():\n",
    "        return nanstd(tensor, dim=dim, unbiased=unbiased, keepdim=keepdim)\n",
    "    else:\n",
    "        return torch.tensor(epsilon, dtype=tensor.dtype)\n",
    "\n",
    "\n",
    "def nanstd(tensor, unbiased=False, dim=None, keepdim=True):\n",
    "    mask = torch.isnan(tensor)\n",
    "    n_obs = mask.logical_not().sum(dim=dim, keepdim=keepdim)\n",
    "    mean = torch.nanmean(tensor, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    centered = tensor - mean\n",
    "    centered = centered.masked_fill(mask, 0)\n",
    "    sum_sq = torch.sum(centered ** 2, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    unbiased_factor = torch.where(n_obs > 1, n_obs - 1, n_obs)\n",
    "    var = sum_sq / unbiased_factor\n",
    "\n",
    "    std = torch.sqrt(var)\n",
    "    if not keepdim:\n",
    "        std = std.squeeze(dim)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c91c4b-7e1b-4e2d-be23-8533dcb31db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_remover = WeightRemover(model, device, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c7775-f01a-4958-9a82-41b2fc667599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90117692-80d0-4612-bf19-c5c0341964bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "sequence_length = 10\n",
    "\n",
    "for idx in range(5):\n",
    "    print(f\"-------------{idx} of the ids-------------\")\n",
    "    random_input_ids = torch.tensor(np.random.randint(0, tokenizer.vocab_size, (batch_size, sequence_length)), dtype=torch.long)\n",
    "    random_decoder_input_ids = torch.tensor(np.random.randint(0, tokenizer.vocab_size, (batch_size, sequence_length)), dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        y_ = weight_remover.process(random_input_ids, random_decoder_input_ids)\n",
    "    weight_remover.apply_removal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525a396a-a3a5-458b-9aa4-f3a01807a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(ref_model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60fc3f-77fa-4f11-92fd-2e18e00350d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0c1b6-269a-4d91-9138-35ec8f7acd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_weight_data(data):\n",
    "    epochs = []\n",
    "    original_weights = []\n",
    "    remaining_weights = []\n",
    "    reductions = []\n",
    "\n",
    "    pattern = re.compile(r'Total original non-zero weights: (\\d+)\\s+Total remaining non-zero weights: (\\d+)\\s+Total reduction: (\\d+)')\n",
    "    matches = pattern.findall(data)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        epoch = i + 1\n",
    "        original = int(match[0])\n",
    "        remaining = int(match[1])\n",
    "        reduction = int(match[2])\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        original_weights.append(original)\n",
    "        remaining_weights.append(remaining)\n",
    "        reductions.append(reduction)\n",
    "\n",
    "    return epochs, original_weights, remaining_weights, reductions\n",
    "\n",
    "def plot_weight_changes(data):\n",
    "    epochs, original_weights, remaining_weights, reductions = parse_weight_data(data)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, original_weights, label='Original Weights', marker='o')\n",
    "    plt.plot(epochs, remaining_weights, label='Remaining Weights', marker='o')\n",
    "    plt.plot(epochs, reductions, label='Reduction', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Number of Weights')\n",
    "    plt.title('Weight Changes per Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edf4c2-2dd0-494a-8da3-5fc9202fd008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../Datasets/Codes/python/train.jsonl'\n",
    "valid_file = '../Datasets/Codes/python/valid.jsonl'\n",
    "test_file = '../Datasets/Codes/python/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd321e-b27b-4d4e-ae4f-14da3f6e2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_code_snippets(file_path):\n",
    "    code_snippets = []\n",
    "    tokens = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            code_snippets.append(data['code_tokens'])\n",
    "            tokens.append(data['docstring_tokens'])\n",
    "    return code_snippets, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f070a5a-f22a-4f4d-9760-970574cc60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snippets, train_tokens = load_code_snippets(train_file)\n",
    "valid_snippets, valid_tokens = load_code_snippets(valid_file)\n",
    "test_snippets, test_tokens = load_code_snippets(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8c951-6666-4761-8b16-13d8fc02e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cc3af-4edc-4655-b921-67a230a49b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_snippets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36139aa-eba8-42db-b1d7-94bf83a19c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = ConcernIdentification(ref_model, model, device='cuda:0', p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da984a6-6c89-480e-9c84-ae7695e52ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (text, tokens) in enumerate(zip(train_snippets, train_tokens)):\n",
    "    print(idx)\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(f\"-------------{idx} of the ids-------------\")\n",
    "    random_input_ids = input_ids\n",
    "    random_decoder_input_ids = generated_ids\n",
    "    with torch.no_grad():\n",
    "        y_ = ci.process(text, tokens)\n",
    "    ci.apply_prune()\n",
    "    if idx > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1586bc-f578-4ae9-95a3-33f1e669adeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c636cddd-ed49-44f7-adde-41dc8df144fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(ref_model, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf1f32-913d-4fc3-828e-97eda0f9f95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
